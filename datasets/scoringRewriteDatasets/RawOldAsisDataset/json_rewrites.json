{
  "3018q3zvoiqh6tkjkzarysii2vbarg": {
    "number_of_turns": 12,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-1",
        "original_question": "What's Rubio going to decide in the next few weeks?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-1",
        "answer": "running for president"
      },
      {
        "turn_num": 2,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-2",
        "original_question": "Does he feel confident about it?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-2",
        "answer": "yes"
      },
      {
        "turn_num": 3,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-3",
        "original_question": "What policy is he not in favor of?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-3",
        "answer": "immigration"
      },
      {
        "turn_num": 4,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-4",
        "original_question": "What type of people does he think are dangerous to the West?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-4",
        "answer": "Radicalized individuals"
      },
      {
        "turn_num": 5,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-5",
        "original_question": "Who are some likely competition to him?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-5",
        "answer": "Mitt Romney and Jeb Bush"
      },
      {
        "turn_num": 6,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-6",
        "original_question": "Does he think they have a lot of money and credibility?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-6",
        "answer": "yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-7",
        "original_question": "What was the title of his book?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-7",
        "answer": "American Dreams"
      },
      {
        "turn_num": 8,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-8",
        "original_question": "When was it released?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-8",
        "answer": "Tuesday"
      },
      {
        "turn_num": 9,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-9",
        "original_question": "Has he decided if he'd be a better president or senator?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-9",
        "answer": "not yet"
      },
      {
        "turn_num": 10,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-10",
        "original_question": "What happens the longer you wait?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-10",
        "answer": "the harder it becomes"
      },
      {
        "turn_num": 11,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-11",
        "original_question": "When's Romney considering making his own bid?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-11",
        "answer": "2016"
      },
      {
        "turn_num": 12,
        "turn_id": "3018q3zvoiqh6tkjkzarysii2vbarg-12",
        "original_question": "What type of resources are Bush and Romney trying to get?",
        "original_question_id": "3018q3zvoiqh6tkjkzarysii2vbarg-12",
        "answer": "big-money supporters"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What decision is Marco Rubio going to make over the next few weeks?",
        "answer": "running for president",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What's Rubio going to decide in the next few weeks?",
        "answer": "running for president",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What's Rubio going to decide in the next few weeks?",
        "answer": "running for president",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Marco Rubio going to decide in the next few weeks?",
        "answer": "running for president",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Does Marco Rubio feel confident about running for president?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Rubio feel confident about running for president?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Marco Rubio feel confident about running for president in the next few weeks?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Marco Rubio feel confident about running for president?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What policy does Marco Rubio not support?",
        "answer": "immigration",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What policy is Rubio not in favor of?",
        "answer": "immigration",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What policy is Marco Rubio not in favor of?",
        "answer": "immigration",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What policy is Marco Rubio not in favor of?",
        "answer": "immigration",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who does Marco Rubio feel poses a threat to the West?",
        "answer": "Radicalized individuals",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What type of people does Rubio think are dangerous to the West?",
        "answer": "Radicalized individuals",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What type of people does Marco Rubio think are dangerous to the West?",
        "answer": "Radicalized individuals",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of people does Marco Rubio think are dangerous to the West?",
        "answer": "Radicalized individuals",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who will probably be Marco Rubio's competition in the presidential race?",
        "answer": "Mitt Romney and Jeb Bush",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who are some likely competition to Rubio?",
        "answer": "Mitt Romney and Jeb Bush",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who are some likely competition to Marco Rubio in the race for president?",
        "answer": "Mitt Romney and Jeb Bush",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who are some people that could be a challenge to Marco Rubio?",
        "answer": "Mitt Romney and Jeb Bush",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Do Mitt Romney and Jeb Bush ahve a lot of money and credibility, according to Marco Rubio?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Rubio think Romney and Bush have a lot of money and credibility?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Mitt Romney think Jeb Bush have a lot of money and credibility?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Marco Rubio think Mitt Romney and Jeb Bush have a lot of money and credibility?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is the title of Marco Rubio's book?",
        "answer": "American Dreams",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the title of Rubio's book?",
        "answer": "American Dreams",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the title of Rubio's book?",
        "answer": "American Dreams",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the title of Marco Rubio's book?",
        "answer": "American Dreams",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "When was the book American Dreams released?",
        "answer": "Tuesday",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was Rubio's book 'American Dreams' released?",
        "answer": "Tuesday",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was American Dreams released?",
        "answer": "Tuesday",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was American Dreams released?",
        "answer": "Tuesday",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Has Marco Rubio decided whether he would be better served in the senate or the presidency?",
        "answer": "not yet",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Has Rubio decided if he'd be a better president or senator?",
        "answer": "not yet",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Has Marco Rubio decided if he'd be a better president or senator?",
        "answer": "not yet",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Has Marco Rubio decided whether he would be a better president or senator?",
        "answer": "not yet",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What happens the longer you wait to declare a presidential campaign?",
        "answer": "the harder it becomes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happens the longer you wait?",
        "answer": "the harder it becomes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happens the longer you wait for Mitt Romney to run for president?",
        "answer": "the harder it becomes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happens the longer you wait?",
        "answer": "the harder it becomes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "When is Mitt Romney considering running for president?",
        "answer": "2016",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When is Romney considering making his own bid?",
        "answer": "2016",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When's Mitt Romney considering making his own bid for president?",
        "answer": "2016",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When is Mitt Romney considering making his own bid?",
        "answer": "2016",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What resources are both Mitt Romney and Jeb Bush vying for?",
        "answer": "big-money supporters",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What type of resources are Bush and Romney trying to get?",
        "answer": "big-money supporters",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What type of resources are Jeb Bush and Mitt Romney trying to get?",
        "answer": "big-money supporters",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of resources are Jeb Bush and Mitt Romney trying to get?",
        "answer": "big-money supporters",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "30og32w0subzh8937xvwlr3znpknep": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-1",
        "original_question": "What is the title of the chapter?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-1",
        "answer": "The meeting of the gee eyes"
      },
      {
        "turn_num": 2,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-2",
        "original_question": "Who is badly injured?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-2",
        "answer": "Merwell"
      },
      {
        "turn_num": 3,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-3",
        "original_question": "Does Dave think he will recover?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-3",
        "answer": "yes"
      },
      {
        "turn_num": 4,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-4",
        "original_question": "What does he tell Merwell to do?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-4",
        "answer": "keep a civil tongue in your head"
      },
      {
        "turn_num": 5,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-5",
        "original_question": "How did Dave leave?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-5",
        "answer": "he left on an ice-boat"
      },
      {
        "turn_num": 6,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-6",
        "original_question": "What did he throw off the boat?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-6",
        "answer": "three skates"
      },
      {
        "turn_num": 7,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-7",
        "original_question": "What insult had Dave been called?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-7",
        "answer": "\"poorhouse rat\""
      },
      {
        "turn_num": 8,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-8",
        "original_question": "True or False: This made Dave very angry.",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-8",
        "answer": "true"
      },
      {
        "turn_num": 9,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-9",
        "original_question": "Was he able to keep his temper?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-9",
        "answer": "no"
      },
      {
        "turn_num": 10,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-10",
        "original_question": "Had he ever been able to?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-10",
        "answer": "no"
      },
      {
        "turn_num": 11,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-11",
        "original_question": "Who does Dave say he wants to find?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-11",
        "answer": "Henshaw and the others."
      },
      {
        "turn_num": 12,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-12",
        "original_question": "Did Poole want him to leave?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-12",
        "answer": "no"
      },
      {
        "turn_num": 13,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-13",
        "original_question": "What was Poole afraid Dave would do to him?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-13",
        "answer": "hit him"
      },
      {
        "turn_num": 14,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-14",
        "original_question": "Who did he threaten to tell if Dave did?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-14",
        "answer": "Doctor Clay"
      },
      {
        "turn_num": 15,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-15",
        "original_question": "Did Dave hit him?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-15",
        "answer": "no"
      },
      {
        "turn_num": 16,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-16",
        "original_question": "Where does Dave say Poole needs to bring Merwell?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-16",
        "answer": "the Hall"
      },
      {
        "turn_num": 17,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-17",
        "original_question": "What is Poole's first name?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-17",
        "answer": "Nat"
      },
      {
        "turn_num": 18,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-18",
        "original_question": "And Merwell's?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-18",
        "answer": "Link"
      },
      {
        "turn_num": 19,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-19",
        "original_question": "What is Dave's last name?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-19",
        "answer": "Porter"
      },
      {
        "turn_num": 20,
        "turn_id": "30og32w0subzh8937xvwlr3znpknep-20",
        "original_question": "Did Poole shout at him in a strong voice?",
        "original_question_id": "30og32w0subzh8937xvwlr3znpknep-20",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What title is given to the chapter?",
        "answer": "The meeting of the gee eyes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the title of the chapter?",
        "answer": "The meeting of the gee eyes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the title of the chapter?",
        "answer": "The meeting of the gee eyes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the chapter's title?",
        "answer": "The meeting of the gee eyes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who has sustained a serious injury?",
        "answer": "Merwell",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is badly injured?",
        "answer": "Merwell",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is badly injured in The meeting of the gee eyes?",
        "answer": "Merwell",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is badly injured?",
        "answer": "Merwell",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Does Dave think that Merwell will recover?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Dave think he will recover?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Dave think he will recover from the injury in The meeting of the gee eyes?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Dave think Merwell will recover?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What does Dave tell Merwell to do?",
        "answer": "keep a civil tongue in your head",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Dave tell Merwell to do?",
        "answer": "keep a civil tongue in your head",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Dave tell Merwell to do?",
        "answer": "keep a civil tongue in your head",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Dave tell Merwell to do?",
        "answer": "keep a civil tongue in your head",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What did Dave leave in?",
        "answer": "he left on an ice-boat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did Dave leave?",
        "answer": "he left on an ice-boat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Dave leave The meeting of the gee eyes?",
        "answer": "he left on an ice-boat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Dave do after leaving?",
        "answer": "he left on an ice-boat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What did Dave throw off the boat?",
        "answer": "three skates",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Dave throw off the boat?",
        "answer": "three skates",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Dave throw off the ice-boat?",
        "answer": "three skates",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Dave throw off the boat?",
        "answer": "three skates",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How had Dave been insulted?",
        "answer": "\"poorhouse rat\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What insult had Dave been called?",
        "answer": "\"poorhouse rat\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What insult had Dave been called?",
        "answer": "\"poorhouse rat\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What insult did Dave receive?",
        "answer": "\"poorhouse rat\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "True or False: Dave did not like being called a poorhouse rat",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "True or False: This made Dave very angry.",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "True or False: This made Dave very angry.",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "True or false: Dave was angry about what was being said?",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Was Dave able to remain calm after being called a poorhouse rat?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was he able to keep his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Dave able to keep his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Dave able to keep his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "In general, had Dave been able to control his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Had he ever been able to?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Had Dave ever been able to keep his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Had Dave ever been able to keep his temper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who does Dave say that he desires to locate?",
        "answer": "Henshaw and the others.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who does Dave say he wants to find?",
        "answer": "Henshaw and the others.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who does Dave say he wants to find in the book The meeting of the gee eyes?",
        "answer": "Henshaw and the others.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who does Dave say he wants to find?",
        "answer": "Henshaw and the others.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Nat Poole want Dave to leave?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Poole want him to leave?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Poole want Dave to leave the ice-boat?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Poole want Dave to leave?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What did Nat Poole fear Dave could do to him?",
        "answer": "hit him",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Poole afraid Dave would do to him?",
        "answer": "hit him",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Poole afraid Dave would do to him?",
        "answer": "hit him",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Poole fear Dave would do to him?",
        "answer": "hit him",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "If Dave hit him, who did Poole to threaten to tell?",
        "answer": "Doctor Clay",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did Poole threaten to tell if Dave hit him?",
        "answer": "Doctor Clay",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who did Poole threaten to tell if Dave did?",
        "answer": "Doctor Clay",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Poole threaten to tell if Dave did?",
        "answer": "Doctor Clay",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Did Dave end up hitting Nat Poole?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Dave hit him?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Dave hit Clay?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Dave hit Clay?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "According to Dave, where does Nat Poole need to bring Merwell?",
        "answer": "the Hall",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where does Dave say Poole needs to bring Merwell?",
        "answer": "the Hall",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where does Dave say Poole needs to bring Merwell?",
        "answer": "the Hall",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does Dave say Poole needs to bring Merwell?",
        "answer": "the Hall",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What is the first name of Poole?",
        "answer": "Nat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Poole's first name?",
        "answer": "Nat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Dave Poole's first name?",
        "answer": "Nat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Poole's first name?",
        "answer": "Nat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What is the first name of Merwell?",
        "answer": "Link",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Merwell's first name?",
        "answer": "Link",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And Merwell's?",
        "answer": "Link",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Merwell's first name?",
        "answer": "Link",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What is the last name of Dave?",
        "answer": "Porter",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Dave's last name?",
        "answer": "Porter",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Dave's last name?",
        "answer": "Porter",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Dave's last name?",
        "answer": "Porter",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Did Poole shout loudly at Dave?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Poole shout at him in a strong voice?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Poole shout at him in a strong voice?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Poole shout at Merwell in a strong voice?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "31lvtdxbl7ay2cbnhqzh76ytw9drlm": {
    "number_of_turns": 19,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-1",
        "original_question": "Who felt there was a silence before the storm?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-1",
        "answer": "Even Saton"
      },
      {
        "turn_num": 2,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-2",
        "original_question": "Who stood in the centre of the little group?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-2",
        "answer": "Saton and Lois"
      },
      {
        "turn_num": 3,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-3",
        "original_question": "Whose elbow was on the mantelpiece?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-3",
        "answer": "Rochester"
      },
      {
        "turn_num": 4,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-4",
        "original_question": "Who looked anxiously at them?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-4",
        "answer": "Mary"
      },
      {
        "turn_num": 5,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-5",
        "original_question": "And who stood on the side and held his peace?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-5",
        "answer": "Vandermere"
      },
      {
        "turn_num": 6,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-6",
        "original_question": "What was beginning to slowly ebb away from Saton?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-6",
        "answer": "courage"
      },
      {
        "turn_num": 7,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-7",
        "original_question": "Whose hand was he holding?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-7",
        "answer": "Lois"
      },
      {
        "turn_num": 8,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-8",
        "original_question": "Had she consented to be his wife?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-8",
        "answer": "yes"
      },
      {
        "turn_num": 9,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-9",
        "original_question": "Who had shrunk back, terrified?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-9",
        "answer": "Pauline"
      },
      {
        "turn_num": 10,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-10",
        "original_question": "Where did Saton say he could take Lois if she couldn't be married there?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-10",
        "answer": "the Comtesse"
      },
      {
        "turn_num": 11,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-11",
        "original_question": "What is the title of the chapter?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-11",
        "answer": "THE CHARLATAN UNMASKED"
      },
      {
        "turn_num": 12,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-12",
        "original_question": "When Saton turned toward Rochester was he defiant or scared?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-12",
        "answer": "defiant"
      },
      {
        "turn_num": 13,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-13",
        "original_question": "Who was forbidden to enter the house (Rochester's)?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-13",
        "answer": "Saton"
      },
      {
        "turn_num": 14,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-14",
        "original_question": "What else was he not supposed to do?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-14",
        "answer": "hold any communication with Rochester's ward"
      },
      {
        "turn_num": 15,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-15",
        "original_question": "What is Lois today?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-15",
        "answer": "she is her own mistress"
      },
      {
        "turn_num": 16,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-16",
        "original_question": "Do they still have to have Rochestere's approval?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-16",
        "answer": "no"
      },
      {
        "turn_num": 17,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-17",
        "original_question": "Was Saton still holding Lois' hand when he turned to Rochester again?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-17",
        "answer": "yes"
      },
      {
        "turn_num": 18,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-18",
        "original_question": "What number chapter is this?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-18",
        "answer": "XXXVI"
      },
      {
        "turn_num": 19,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-19",
        "original_question": "Did Saton obey 2 conditions set?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytw9drlm-19",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who sensed a silence before the storm?",
        "answer": "Even Saton",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who felt there was a silence before the storm?",
        "answer": "Even Saton",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who felt there was a silence before the storm?",
        "answer": "Even Saton",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who felt there was a silence before the storm?",
        "answer": "Even Saton",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who was in the center of the group?",
        "answer": "Saton and Lois",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who stood in the centre of the little group?",
        "answer": "Saton and Lois",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who stood in the centre of the little group?",
        "answer": "Saton and Lois",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who stood in the centre of the group?",
        "answer": "Saton and Lois",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Whose elbow was perched on the mantelpiece?",
        "answer": "Rochester",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Whose elbow was on the mantelpiece?",
        "answer": "Rochester",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Whose elbow was on the mantelpiece of the little group?",
        "answer": "Rochester",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose elbow was on the mantelpiece?",
        "answer": "Rochester",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who looked anxiously at Saton and Lois?",
        "answer": "Mary",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who looked anxiously at them?",
        "answer": "Mary",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who looked anxiously at Saton and Lois?",
        "answer": "Mary",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who looked anxiously at Saton and Lois?",
        "answer": "Mary",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who held his peace off to the side?",
        "answer": "Vandermere",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who stood on the side and held his peace?",
        "answer": "Vandermere",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And who stood on the side and held Saton's peace?",
        "answer": "Vandermere",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who sat on the side and held his peace?",
        "answer": "Vandermere",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What was inching away from Saton?",
        "answer": "courage",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was beginning to slowly ebb away from Saton?",
        "answer": "courage",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was beginning to slowly ebb away from Saton?",
        "answer": "courage",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was beginning to slowly ebb away from Saton?",
        "answer": "courage",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Whose hand was Saton holding?",
        "answer": "Lois",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Whose hand was Saton holding?",
        "answer": "Lois",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Whose hand was Saton Vandermere holding?",
        "answer": "Lois",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose hand was Vandermere holding?",
        "answer": "Lois",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Had Lois consented to marry Saton?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Had Lois consented to be Saton's wife?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Had Lois consented to be Saton's wife?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Had Lois consented to be Saton's wife?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who shrunk back in terror?",
        "answer": "Pauline",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who had shrunk back, terrified?",
        "answer": "Pauline",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who had shrunk back, terrified, to be Saton's wife?",
        "answer": "Pauline",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who had a fearful relapse?",
        "answer": "Pauline",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Where did Saton say he and Lois would go if they could not marry in their present location?",
        "answer": "the Comtesse",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Saton say he could take Lois if she couldn't be married there?",
        "answer": "the Comtesse",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Saton say he could take Lois if she couldn't be married there?",
        "answer": "the Comtesse",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Saton say he could take Lois if she couldn't be married there?",
        "answer": "the Comtesse",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What is the chapter called?",
        "answer": "THE CHARLATAN UNMASKED",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the title of the chapter?",
        "answer": "THE CHARLATAN UNMASKED",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the title of the chapter about Saton?",
        "answer": "THE CHARLATAN UNMASKED",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the chapter's title?",
        "answer": "THE CHARLATAN UNMASKED",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Saton turn towards Rochester in a defiant or frightened manner?",
        "answer": "defiant",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When Saton turned toward Rochester was he defiant or scared?",
        "answer": "defiant",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When Saton turned toward Rochester was he defiant or scared?",
        "answer": "defiant",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Saton defiant or scared when he turned toward Rochester?",
        "answer": "defiant",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Who did Rochester forbid from entering his home?",
        "answer": "Saton",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was forbidden to enter the house?",
        "answer": "Saton",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was forbidden to enter the house (Rochester's)?",
        "answer": "Saton",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was forbidden to enter the house (Rochester's)?",
        "answer": "Saton",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What was Saton not allowed to do in addition to entering Rochester's home?",
        "answer": "hold any communication with Rochester's ward",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else was he not supposed to do?",
        "answer": "hold any communication with Rochester's ward",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else was Saton not supposed to do besides turn toward Rochester?",
        "answer": "hold any communication with Rochester's ward",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Saton not supposed to do besides enter the house?",
        "answer": "hold any communication with Rochester's ward",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How does Saton describe Lois presently?",
        "answer": "she is her own mistress",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Lois today?",
        "answer": "she is her own mistress",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Lois today?",
        "answer": "she is her own mistress",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Lois's current status?",
        "answer": "she is her own mistress",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Is Rochester's approval needed for Lois and Saton to marry?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Do they still have to have Rochestere's approval?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Do Saton and Rochester still have to have Rochestere's approval?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do Saton and Lois still have to have Rochestere's approval?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Did Saton still hold Lois's hand as he turned to Rochester a second time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Saton still holding Lois' hand when he turned to Rochester again?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Saton still holding Lois' hand when he turned to Rochester again?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Saton holding Lois' hand when he turned to Rochester?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What is the number of this chapter?",
        "answer": "XXXVI",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What number chapter is this?",
        "answer": "XXXVI",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What number chapter is the chapter about the charlatine unmasked?",
        "answer": "XXXVI",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What chapter is the chapter called?",
        "answer": "XXXVI",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Did Saton obey the two conditions given to him?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Saton obey 2 conditions set?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Saton obey 2 conditions set by the XXXVI. Countess?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Saton obey the two conditions?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "31lvtdxbl7ay2cbnhqzh76ytxkxrlt": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-1",
        "original_question": "Who was in a temper?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-1",
        "answer": "Dan"
      },
      {
        "turn_num": 2,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-2",
        "original_question": "What kind of temper?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-2",
        "answer": "fine temper"
      },
      {
        "turn_num": 3,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-3",
        "original_question": "Who was restraining him?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-3",
        "answer": "Seth,"
      },
      {
        "turn_num": 4,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-4",
        "original_question": "How did he turn to his partner?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-4",
        "answer": "savagely"
      },
      {
        "turn_num": 5,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-5",
        "original_question": "How did seth reply to him?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-5",
        "answer": "soothingly."
      },
      {
        "turn_num": 6,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-6",
        "original_question": "What happened to Seth's house?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-6",
        "answer": ",burned down"
      },
      {
        "turn_num": 7,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-7",
        "original_question": "How much did the man get paid?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-7",
        "answer": "fifteen cents"
      },
      {
        "turn_num": 8,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-8",
        "original_question": "Was the man a runaway criminal?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-8",
        "answer": "yes"
      },
      {
        "turn_num": 9,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-9",
        "original_question": "Why didn't the man raise his hands?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-9",
        "answer": "he felt bad"
      },
      {
        "turn_num": 10,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-10",
        "original_question": "What was his name?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-10",
        "answer": "Jip Collins"
      },
      {
        "turn_num": 11,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-11",
        "original_question": "What does dan think is a shame?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-11",
        "answer": "letting him go"
      },
      {
        "turn_num": 12,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-12",
        "original_question": "True or False: Seth and Dan are partners.",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-12",
        "answer": "true"
      },
      {
        "turn_num": 13,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-13",
        "original_question": "True or False: Seth thinks Dan should have beaten up Jip.",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-13",
        "answer": "false"
      },
      {
        "turn_num": 14,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-14",
        "original_question": "What kind of name does Seth want dan to avoid?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-14",
        "answer": "a bruiser."
      },
      {
        "turn_num": 15,
        "turn_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-15",
        "original_question": "Is Dan okay with having that name?",
        "original_question_id": "31lvtdxbl7ay2cbnhqzh76ytxkxrlt-15",
        "answer": "yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Whose temper was flared?",
        "answer": "Dan",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was in a temper?",
        "answer": "Dan",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was in a temper?",
        "answer": "Dan",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was in a temper?",
        "answer": "Dan",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What kind of temper did Dan have?",
        "answer": "fine temper",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of temper was Dan in?",
        "answer": "fine temper",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of temper was Dan in?",
        "answer": "fine temper",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of temper did Dan have?",
        "answer": "fine temper",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who was Dan being restrained by?",
        "answer": "Seth,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was restraining Dan in his fine temper?",
        "answer": "Seth,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was restraining Dan?",
        "answer": "Seth,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was restraining Dan?",
        "answer": "Seth,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "How did Dan turn to Seth?",
        "answer": "savagely",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did he turn to his partner?",
        "answer": "savagely",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Dan turn to his partner?",
        "answer": "savagely",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Dan do to Seth?",
        "answer": "savagely",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "How did Seth reply to Dan?",
        "answer": "soothingly.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did Seth reply to him?",
        "answer": "soothingly.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did seth reply to Dan?",
        "answer": "soothingly.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Seth do to respond to Dan?",
        "answer": "soothingly.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What was the fate of Seth's house?",
        "answer": ",burned down",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened to Seth's house?",
        "answer": ",burned down",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happened to Seth's house?",
        "answer": ",burned down",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened to Seth's house?",
        "answer": ",burned down",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How much money did Seth give the man?",
        "answer": "fifteen cents",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How much did the man get paid?",
        "answer": "fifteen cents",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How much did the man get paid for being burned down?",
        "answer": "fifteen cents",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How much was Seth's partner paid?",
        "answer": "fifteen cents",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Did Seth pay a runaway criminal?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the man a runaway criminal?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the man who ran away from Seth a runaway criminal?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the man a runaway criminal?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Why didn't Jip become violent with the men?",
        "answer": "he felt bad",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why didn't the man raise his hands?",
        "answer": "he felt bad",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why didn't the man raise his hands?",
        "answer": "he felt bad",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why didn't the man raise his hands?",
        "answer": "he felt bad",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who are Dan and Seth talking about?",
        "answer": "Jip Collins",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was his name?",
        "answer": "Jip Collins",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the man's name?",
        "answer": "Jip Collins",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the runaway criminal?",
        "answer": "Jip Collins",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What is a shame according to Dan?",
        "answer": "letting him go",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Dan think is a shame?",
        "answer": "letting him go",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Dan think is a shame?",
        "answer": "letting him go",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Dan think is a shame?",
        "answer": "letting him go",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Is it true or false that Seth and Dan are partners?",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "True or False: Seth and Dan are partners.",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "True or False: Seth and Dan are partners.",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "True or false: Seth and Dan are partners.",
        "answer": "true",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Is it true or false that Seth believes Dan should've beaten up Jip?",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "True or False: Seth thinks Dan should have beaten up Jip.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "True or False: Seth thinks Dan should have beaten up Jip Collins?",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "True or false: Seth thinks that Dan should have beaten up Jip Collins?",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What nickname does Seth want Dan to avoid?",
        "answer": "a bruiser.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of name does Seth want dan to avoid?",
        "answer": "a bruiser.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of name does Seth want dan to avoid?",
        "answer": "a bruiser.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What name does Seth want Dan to avoid?",
        "answer": "a bruiser.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Is Dan ok with being called a bruiser?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is Dan okay with having that name?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is Dan okay with having the name 'Brucer'?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is Dan okay with having the name bruiser?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "32ktq2v7rdfc4uxmnl0agydort19mq": {
    "number_of_turns": 14,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-1",
        "original_question": "What is the city of focus?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-1",
        "answer": "New Haven"
      },
      {
        "turn_num": 2,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-2",
        "original_question": "What was it's population in 2012?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-2",
        "answer": "130,741"
      },
      {
        "turn_num": 3,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-3",
        "original_question": "When was that census held?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-3",
        "answer": "1 July 2012"
      },
      {
        "turn_num": 4,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-4",
        "original_question": "In what year did the Puritans winter there?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-4",
        "answer": "1637"
      },
      {
        "turn_num": 5,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-5",
        "original_question": "Who was the reverend who led them?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-5",
        "answer": "John Davenport"
      },
      {
        "turn_num": 6,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-6",
        "original_question": "Which merchant traveled him?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-6",
        "answer": "Theophilus Eaton"
      },
      {
        "turn_num": 7,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-7",
        "original_question": "How did the travel?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-7",
        "answer": "sailed"
      },
      {
        "turn_num": 8,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-8",
        "original_question": "What was the settlers hope?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-8",
        "answer": "to establish a theological community"
      },
      {
        "turn_num": 9,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-9",
        "original_question": "What two tribes were at war?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-9",
        "answer": "Quinnipiacs and Pequots"
      },
      {
        "turn_num": 10,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-10",
        "original_question": "Which of them sold out for protection?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-10",
        "answer": "Pequots"
      },
      {
        "turn_num": 11,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-11",
        "original_question": "Which sound is the city located on?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-11",
        "answer": "Long Island Sound"
      },
      {
        "turn_num": 12,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-12",
        "original_question": "On which harbor?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-12",
        "answer": "New Haven Harbor"
      },
      {
        "turn_num": 13,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-13",
        "original_question": "In terms population, where does the city rank?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-13",
        "answer": "second-largest city in Connecticut"
      },
      {
        "turn_num": 14,
        "turn_id": "32ktq2v7rdfc4uxmnl0agydort19mq-14",
        "original_question": "Which city is first?",
        "original_question_id": "32ktq2v7rdfc4uxmnl0agydort19mq-14",
        "answer": "Bridgeport"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What city is the article about?",
        "answer": "New Haven",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the city of focus?",
        "answer": "New Haven",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the city of focus?",
        "answer": "New Haven",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the city of focus?",
        "answer": "New Haven",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was the population of New Haven in 2012?",
        "answer": "130,741",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the population of New Haven in 2012?",
        "answer": "130,741",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was New Haven's population in 2012?",
        "answer": "130,741",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was New Haven's population in 2012?",
        "answer": "130,741",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "When was the 2012 census held?",
        "answer": "1 July 2012",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the census held in New Haven?",
        "answer": "1 July 2012",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the 2012 census held in New Haven?",
        "answer": "1 July 2012",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the 2012 census take place?",
        "answer": "1 July 2012",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "In what year did Puritans winter in New Haven?",
        "answer": "1637",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In what year did the Puritans winter there?",
        "answer": "1637",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In what year did the Puritans winter in New Haven?",
        "answer": "1637",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "In what year did the Puritans winter in New Haven?",
        "answer": "1637",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What reverend led the Puritans to New Haven?",
        "answer": "John Davenport",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was the reverend who led the Puritans in New Haven?",
        "answer": "John Davenport",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was the reverend who led the Puritans to New Haven?",
        "answer": "John Davenport",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the reverend that led the Puritans to New Haven?",
        "answer": "John Davenport",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Which merchant travelled with John Davenport?",
        "answer": "Theophilus Eaton",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which merchant traveled with him?",
        "answer": "Theophilus Eaton",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which merchant traveled John Davenport to New Haven?",
        "answer": "Theophilus Eaton",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which merchant traveled John Davenport?",
        "answer": "Theophilus Eaton",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How did Theophilus Eaton and the reverend travel?",
        "answer": "sailed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did the Puritans travel to New Haven?",
        "answer": "sailed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Theophilus Eaton travel to New Haven?",
        "answer": "sailed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did Theophilus Eaton travel to New Haven?",
        "answer": "sailed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was the wish of Puritan settlers?",
        "answer": "to establish a theological community",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the settlers hope?",
        "answer": "to establish a theological community",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the settlers hope in New Haven?",
        "answer": "to establish a theological community",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the settlers hope for New Haven?",
        "answer": "to establish a theological community",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What Indian tribes were at war?",
        "answer": "Quinnipiacs and Pequots",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What two tribes were at war?",
        "answer": "Quinnipiacs and Pequots",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What two tribes were at war in New Haven?",
        "answer": "Quinnipiacs and Pequots",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What two tribes were in a war?",
        "answer": "Quinnipiacs and Pequots",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Which tribe sold their land to the Puritans?",
        "answer": "Pequots",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which of them sold out for protection?",
        "answer": "Pequots",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which of the Quinnipiacs and Pequots sold out for protection?",
        "answer": "Pequots",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which tribe sold out for protection?",
        "answer": "Pequots",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What sound is New Haven located on?",
        "answer": "Long Island Sound",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which sound is the city located on?",
        "answer": "Long Island Sound",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which sound is New Haven located on?",
        "answer": "Long Island Sound",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What sound is New Haven located on?",
        "answer": "Long Island Sound",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What harbor is New Haven located on?",
        "answer": "New Haven Harbor",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "On which harbor is the city located?",
        "answer": "New Haven Harbor",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "On which harbor was New Haven located?",
        "answer": "New Haven Harbor",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What harbor is New Haven on?",
        "answer": "New Haven Harbor",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What is New Haven's rank in population in the state of Connecticut?",
        "answer": "second-largest city in Connecticut",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In terms of population, where does the city of New Haven rank?",
        "answer": "second-largest city in Connecticut",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In terms of population, where does New Haven rank?",
        "answer": "second-largest city in Connecticut",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is New Haven's rank in population?",
        "answer": "second-largest city in Connecticut",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What city has the largest population in Connecticut?",
        "answer": "Bridgeport",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which city is the largest in Connecticut?",
        "answer": "Bridgeport",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which city is first in terms of population?",
        "answer": "Bridgeport",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which city is the first in terms of population in Connecticut?",
        "answer": "Bridgeport",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "32z9zlut1lktj30hyd3flj0h4btoho": {
    "number_of_turns": 11,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-1",
        "original_question": "did Sean like golden coins?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-1",
        "answer": "No."
      },
      {
        "turn_num": 2,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-2",
        "original_question": "why?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-2",
        "answer": "They were too yellow."
      },
      {
        "turn_num": 3,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-3",
        "original_question": "what color things did he use to cover his treasure?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-3",
        "answer": "Green."
      },
      {
        "turn_num": 4,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-4",
        "original_question": "did he cover his cave?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-4",
        "answer": "No."
      },
      {
        "turn_num": 5,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-5",
        "original_question": "did he put stuff all over his cave?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-5",
        "answer": "Yes."
      },
      {
        "turn_num": 6,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-6",
        "original_question": "like what?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-6",
        "answer": "Grass, clovers, leaves, and vines."
      },
      {
        "turn_num": 7,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-7",
        "original_question": "who was worried?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-7",
        "answer": "The other dragons."
      },
      {
        "turn_num": 8,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-8",
        "original_question": "were they afraid he might get hurt?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-8",
        "answer": "Yes."
      },
      {
        "turn_num": 9,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-9",
        "original_question": "did Sean agree?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-9",
        "answer": "No."
      },
      {
        "turn_num": 10,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-10",
        "original_question": "who did Sean visit?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-10",
        "answer": "Zarah."
      },
      {
        "turn_num": 11,
        "turn_id": "32z9zlut1lktj30hyd3flj0h4btoho-11",
        "original_question": "what did he see when visiting Zarah?",
        "original_question_id": "32z9zlut1lktj30hyd3flj0h4btoho-11",
        "answer": "A piece of green in a golden necklace."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Did golden coins make Sean happy?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did Sean like golden coins?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Sean like golden coins?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sean like golden coins?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Why wasn't Sean a fan of gold coins?",
        "answer": "They were too yellow.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did Sean like golden coins?",
        "answer": "They were too yellow.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "why did Sean like golden coins?",
        "answer": "They were too yellow.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Sean like golden coins?",
        "answer": "They were too yellow.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What color would Sean use to cover up his treasure?",
        "answer": "Green.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "color things Sean used to cover his treasure",
        "answer": "Green.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what color things did Sean use to cover his treasure?",
        "answer": "Green.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What color of things did Sean use to cover his treasure?",
        "answer": "Green.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Was the thing Sean would cover up his cave?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did Sean cover his cave?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Sean cover his cave with green things?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sean cover his cave with green things?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did Sean place items all throughout his cave?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Sean put stuff all over his cave?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Sean put stuff all over his cave?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sean cover his cave with stuff?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What kinds of things would Sean put all over his cave?",
        "answer": "Grass, clovers, leaves, and vines.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Sean put all over his cave?",
        "answer": "Grass, clovers, leaves, and vines.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Sean like gold coins?",
        "answer": "Grass, clovers, leaves, and vines.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Sean put all over his cave?",
        "answer": "Grass, clovers, leaves, and vines.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Who felt worried about Sean?",
        "answer": "The other dragons.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who was worried?",
        "answer": "The other dragons.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who was worried about the golden coins?",
        "answer": "The other dragons.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was worried?",
        "answer": "The other dragons.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Were the other dragons fearful that Sean was going to get hurt?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "were they afraid he might get hurt?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "were the other dragons afraid Sean might get hurt?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the dragons fear that Sean might get hurt?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Did Sean agree with his friends that he could get hurt?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did Sean agree?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Sean agree with the other dragons?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sean agree with the dragons' fears?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What dragon did Sean go to visit?",
        "answer": "Zarah.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who did Sean visit?",
        "answer": "Zarah.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who did Sean visit?",
        "answer": "Zarah.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Sean visit?",
        "answer": "Zarah.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What did Sean notice during his visit to Zarah?",
        "answer": "A piece of green in a golden necklace.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what did Sean see when visiting Zarah?",
        "answer": "A piece of green in a golden necklace.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did Sean see when visiting Zarah?",
        "answer": "A piece of green in a golden necklace.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Sean see when he visited Zarah?",
        "answer": "A piece of green in a golden necklace.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "336kav9kyqs1yr11lf9606shu0wy2a": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-1",
        "original_question": "How old is Catherine?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-1",
        "answer": "54"
      },
      {
        "turn_num": 2,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-2",
        "original_question": "where does she live?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-2",
        "answer": "Sweden"
      },
      {
        "turn_num": 3,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-3",
        "original_question": "Who is Tom?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-3",
        "answer": "a dog"
      },
      {
        "turn_num": 4,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-4",
        "original_question": "Who provides care for its people?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-4",
        "answer": "the government"
      },
      {
        "turn_num": 5,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-5",
        "original_question": "is it expensive?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-5",
        "answer": "Yes"
      },
      {
        "turn_num": 6,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-6",
        "original_question": "How do they afford it?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-6",
        "answer": "owners offer health and even life _ for their dog."
      },
      {
        "turn_num": 7,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-7",
        "original_question": "Are there taxes on most things in Sweden?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-7",
        "answer": "Yes"
      },
      {
        "turn_num": 8,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-8",
        "original_question": "even dogs?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-8",
        "answer": "Yes"
      },
      {
        "turn_num": 9,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-9",
        "original_question": "how much?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-9",
        "answer": "500 Swedish kronor"
      },
      {
        "turn_num": 10,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-10",
        "original_question": "what is that money used for?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-10",
        "answer": "dog hospitals and sometimes medical treatment"
      },
      {
        "turn_num": 11,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-11",
        "original_question": "What happens if a dog is hit by a car?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-11",
        "answer": "the owner, has to pay for any damage done to the car,"
      },
      {
        "turn_num": 12,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-12",
        "original_question": "but what if the dog is killed?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-12",
        "answer": "even if your dog has been killed in the accident."
      },
      {
        "turn_num": 13,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-13",
        "original_question": "What does Catherine do for work?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-13",
        "answer": "secretary"
      },
      {
        "turn_num": 14,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-14",
        "original_question": "Does Tom bother most people?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-14",
        "answer": "No"
      },
      {
        "turn_num": 15,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-15",
        "original_question": "What does she consider Tom as?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-15",
        "answer": "fourth child"
      },
      {
        "turn_num": 16,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-16",
        "original_question": "is treatment for a dog expensive?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-16",
        "answer": "Yes"
      },
      {
        "turn_num": 17,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-17",
        "original_question": "Does Tom buy his own food?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-17",
        "answer": "No"
      },
      {
        "turn_num": 18,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-18",
        "original_question": "HOw many children does she have besides tom?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-18",
        "answer": "Three"
      },
      {
        "turn_num": 19,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-19",
        "original_question": "Does he go to the office with her?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-19",
        "answer": "Yes"
      },
      {
        "turn_num": 20,
        "turn_id": "336kav9kyqs1yr11lf9606shu0wy2a-20",
        "original_question": "how about shopping?",
        "original_question_id": "336kav9kyqs1yr11lf9606shu0wy2a-20",
        "answer": "Yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is Catherine Green's age?",
        "answer": "54",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How old is Catherine?",
        "answer": "54",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How old is Catherine?",
        "answer": "54",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Catherine's age?",
        "answer": "54",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Where does Catherine Green live?",
        "answer": "Sweden",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where does Catherine live?",
        "answer": "Sweden",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where does Catherine live?",
        "answer": "Sweden",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does Catherine live?",
        "answer": "Sweden",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What kind of animal is Tom?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Tom?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is Tom?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Tom's role?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who provides care for the people of Sweden?",
        "answer": "the government",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who provides care for its people?",
        "answer": "the government",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who provides care for Sweden's people?",
        "answer": "the government",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is responsible for the care of Sweden's people?",
        "answer": "the government",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Is the level of care Swedes receive from their government costly?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is it expensive?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is the government's care for its people expensive?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is the government's care expensive?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "How does the Swedish government afford to care for its people?",
        "answer": "owners offer health and even life _ for their dog.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How do they afford it?",
        "answer": "owners offer health and even life _ for their dog.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How do the Swedish people afford the health care they receive?",
        "answer": "owners offer health and even life _ for their dog.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How can Sweden's people afford to pay for care?",
        "answer": "owners offer health and even life _ for their dog.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Are most things taxed in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Are there taxes on most things in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Are there taxes on most things in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is there a tax on most things in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "In Sweden, does owning a dog mean more taxes?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Are there taxes on dogs in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "even dogs in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Sweden tax dogs?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "At what rate are dogs taxed in Sweden?",
        "answer": "500 Swedish kronor",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the tax rate on dogs in Sweden?",
        "answer": "500 Swedish kronor",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How much does Sweden cost to have a dog?",
        "answer": "500 Swedish kronor",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How much does Sweden tax dogs?",
        "answer": "500 Swedish kronor",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What does the dog tax in Sweden go towards?",
        "answer": "dog hospitals and sometimes medical treatment",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is the money used for in Sweden?",
        "answer": "dog hospitals and sometimes medical treatment",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is the 500 Swedish kronor used for?",
        "answer": "dog hospitals and sometimes medical treatment",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the purpose of the 500 Swedish kronor?",
        "answer": "dog hospitals and sometimes medical treatment",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "If a dog is hit by a car in Sweden, what are the consequences for the owner?",
        "answer": "the owner, has to pay for any damage done to the car,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happens if a dog is hit by a car?",
        "answer": "the owner, has to pay for any damage done to the car,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happens if a dog is hit by a car?",
        "answer": "the owner, has to pay for any damage done to the car,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happens if a dog is hit by a car?",
        "answer": "the owner, has to pay for any damage done to the car,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What if a car kills a dog in Sweden?",
        "answer": "even if your dog has been killed in the accident.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happens if a dog is killed by a car in Sweden?",
        "answer": "even if your dog has been killed in the accident.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "but what if the dog is killed?",
        "answer": "even if your dog has been killed in the accident.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happens if the dog is killed?",
        "answer": "even if your dog has been killed in the accident.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What is Catherine Green's job?",
        "answer": "secretary",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Catherine do for work?",
        "answer": "secretary",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Catherine do for work?",
        "answer": "secretary",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Catherine do for a living?",
        "answer": "secretary",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Do people find Catherine Green's dog annoying?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Tom bother most people?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Tom bother most people in Sweden?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Tom make people feel bad?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What does Catherine Green consider Tom to be?",
        "answer": "fourth child",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Catherine consider Tom as?",
        "answer": "fourth child",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Catherine consider Tom as?",
        "answer": "fourth child",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Catherine consider Tom to be?",
        "answer": "fourth child",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Is caring for a dog expensive in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is treatment for a dog expensive?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is treatment for a dog expensive in Sweden?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is treatment for a dog expensive?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Does Tom the dog pay for his own food?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Tom buy his own food?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Tom buy his own food?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Tom buy his own food?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "How many kids does Catherine Green have apart from Tom?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many children does Catherine have besides Tom?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many children does Catherine have besides Tom?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many children does Catherine have besides Tom?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Does Tom go to the office with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Tom go to the office with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Tom go to the office with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Tom go to the office with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Does Tom go shopping with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Catherine take Tom with her when she goes shopping?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How about shopping with Catherine?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Catherine go shopping with Tom?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "33fbrbdw6ozzh32l540id6d1c6t8cp": {
    "number_of_turns": 16,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-1",
        "original_question": "What does BFF stand for?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-1",
        "answer": "Best Friends Forever"
      },
      {
        "turn_num": 2,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-2",
        "original_question": "Who is Eddie's BFF?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-2",
        "answer": "Milo"
      },
      {
        "turn_num": 3,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-3",
        "original_question": "How old is he?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-3",
        "answer": "six"
      },
      {
        "turn_num": 4,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-4",
        "original_question": "How are they special?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-4",
        "answer": "they're dogs"
      },
      {
        "turn_num": 5,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-5",
        "original_question": "Were they new BFFs?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-5",
        "answer": "no"
      },
      {
        "turn_num": 6,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-6",
        "original_question": "Did one of them start to lose their hearing?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-6",
        "answer": "no"
      },
      {
        "turn_num": 7,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-7",
        "original_question": "What did he lose?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-7",
        "answer": "his eyesight."
      },
      {
        "turn_num": 8,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-8",
        "original_question": "How could they tell this was happening?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-8",
        "answer": "he was walking into walls"
      },
      {
        "turn_num": 9,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-9",
        "original_question": "Did he bump anything else?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-9",
        "answer": "dustbins"
      },
      {
        "turn_num": 10,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-10",
        "original_question": "Did his best buddy decide to let him figure things out himself?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-10",
        "answer": "no"
      },
      {
        "turn_num": 11,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-11",
        "original_question": "What did he do instead?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-11",
        "answer": "became his guide dog"
      },
      {
        "turn_num": 12,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-12",
        "original_question": "Who owned the two?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-12",
        "answer": "Angie Baker"
      },
      {
        "turn_num": 13,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-13",
        "original_question": "What did she realize?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-13",
        "answer": "how much Eddie depended on his friend"
      },
      {
        "turn_num": 14,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-14",
        "original_question": "Does she take them for runs?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-14",
        "answer": "no"
      },
      {
        "turn_num": 15,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-15",
        "original_question": "What would they take instead?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-15",
        "answer": "walks"
      },
      {
        "turn_num": 16,
        "turn_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-16",
        "original_question": "Did their human mom get them only a few years ago?",
        "original_question_id": "33fbrbdw6ozzh32l540id6d1c6t8cp-16",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What does BFF mean?",
        "answer": "Best Friends Forever",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does BFF stand for?",
        "answer": "Best Friends Forever",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does BFF stand for?",
        "answer": "Best Friends Forever",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is BFF short for?",
        "answer": "Best Friends Forever",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What's the name of Eddie's BFF?",
        "answer": "Milo",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Eddie's BFF?",
        "answer": "Milo",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is Eddie's BFF?",
        "answer": "Milo",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Eddie's BFF called?",
        "answer": "Milo",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "How old is Eddie's BFF?",
        "answer": "six",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How old is Milo?",
        "answer": "six",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How old is Eddie's BFF?",
        "answer": "six",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How old is Milo?",
        "answer": "six",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What's special about Milo and Eddie?",
        "answer": "they're dogs",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How are Eddie and Milo special?",
        "answer": "they're dogs",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How are Eddie and Milo special?",
        "answer": "they're dogs",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How are Eddie and Milo special?",
        "answer": "they're dogs",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did Milo and Eddie just become BFFs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Were Eddie and Milo new BFFs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Were Eddie and Milo new BFFs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Were Eddie and Milo new BFFs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did Milo or Eddie start to lose their hearing?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did one of them start to lose their hearing?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did one of Eddie's BFFs start to lose their hearing?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did one of Eddie and Milo lose their hearing?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What sense was Eddie losing?",
        "answer": "his eyesight.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he lose?",
        "answer": "his eyesight.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Eddie lose?",
        "answer": "his eyesight.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Milo lose?",
        "answer": "his eyesight.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was a symptom of Eddie losing his eyesight?",
        "answer": "he was walking into walls",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How could they tell this was happening?",
        "answer": "he was walking into walls",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How could they tell Eddie's BFF was losing his eyesight?",
        "answer": "he was walking into walls",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How could the group tell that Eddie had lost his sight?",
        "answer": "he was walking into walls",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What else did Eddie bump into besides walls?",
        "answer": "dustbins",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he bump anything else?",
        "answer": "dustbins",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Eddie bump anything else?",
        "answer": "dustbins",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Eddie bump into anything else?",
        "answer": "dustbins",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did Eddie's BFF decide to let him bump around on his own?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did his best buddy decide to let him figure things out himself?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Eddie's best buddy decide to let him figure things out himself?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Eddie's best friend decide to let him figure things out himself?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What did Milo do to help Eddie?",
        "answer": "became his guide dog",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he do instead?",
        "answer": "became his guide dog",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Eddie do instead of letting his best buddy figure things out himself?",
        "answer": "became his guide dog",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Eddie do instead of letting Milo figure things out himself?",
        "answer": "became his guide dog",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Who owned Eddie and Milo?",
        "answer": "Angie Baker",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who owned the two?",
        "answer": "Angie Baker",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who owned the two BFFs?",
        "answer": "Angie Baker",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who owned Eddie and Milo?",
        "answer": "Angie Baker",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What did Angie Baker realize?",
        "answer": "how much Eddie depended on his friend",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Angie Baker realize?",
        "answer": "how much Eddie depended on his friend",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Angie Baker realize?",
        "answer": "how much Eddie depended on his friend",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Angie Baker realize?",
        "answer": "how much Eddie depended on his friend",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Does Angie Baker go on runs with her dogs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Angie take Eddie and Milo for runs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Angie Baker take Eddie's best friend for runs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Angie Baker take Eddie and Milo on runs?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Instead of runs, what does Angie Baker do with her dogs?",
        "answer": "walks",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What would they take instead?",
        "answer": "walks",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What would Eddie and Angie take instead of Eddie's best friend?",
        "answer": "walks",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What would Angie take Eddie and Milo for a run instead?",
        "answer": "walks",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Did Milo and Eddie's mom just recently acquire them?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did their human mom get them only a few years ago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Eddie's best friend Angie Baker get them only a few years ago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Angie get Eddie and Milo only a few years ago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "34bbwhlwhab1k7k3vhca2pei8rowil": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-1",
        "original_question": "Who is a reformed rider?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-1",
        "answer": "David Millar"
      },
      {
        "turn_num": 2,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-2",
        "original_question": "Who has tainted legacies?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-2",
        "answer": "Lance Armstrong"
      },
      {
        "turn_num": 3,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-3",
        "original_question": "What did they win?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-3",
        "answer": "Tour de France champions"
      },
      {
        "turn_num": 4,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-4",
        "original_question": "Who else?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-4",
        "answer": "Jan Ulrich"
      },
      {
        "turn_num": 5,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-5",
        "original_question": "What do they cast?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-5",
        "answer": "dark shadow"
      },
      {
        "turn_num": 6,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-6",
        "original_question": "Where was David sitting in the past?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-6",
        "answer": "Biarritz restaurant"
      },
      {
        "turn_num": 7,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-7",
        "original_question": "How long ago?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-7",
        "answer": "nine years"
      },
      {
        "turn_num": 8,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-8",
        "original_question": "Where was the restaurant?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-8",
        "answer": "west France"
      },
      {
        "turn_num": 9,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-9",
        "original_question": "Who was he having dinner with?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-9",
        "answer": "Dave Brailsford"
      },
      {
        "turn_num": 10,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-10",
        "original_question": "Who is that now?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-10",
        "answer": "performance director at British Cycling and Team Sky"
      },
      {
        "turn_num": 11,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-11",
        "original_question": "Who arrived there?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-11",
        "answer": "French police"
      },
      {
        "turn_num": 12,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-12",
        "original_question": "What did they do?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-12",
        "answer": "arrest the cyclist,"
      },
      {
        "turn_num": 13,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-13",
        "original_question": "Why?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-13",
        "answer": "outing him as a drugs cheat"
      },
      {
        "turn_num": 14,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-14",
        "original_question": "Did they ban him?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-14",
        "answer": "Yes"
      },
      {
        "turn_num": 15,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-15",
        "original_question": "For how long?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-15",
        "answer": "two years"
      },
      {
        "turn_num": 16,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-16",
        "original_question": "Did he return?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-16",
        "answer": "Yes"
      },
      {
        "turn_num": 17,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-17",
        "original_question": "Where was he born?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-17",
        "answer": "Malta"
      },
      {
        "turn_num": 18,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-18",
        "original_question": "Where does he live?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-18",
        "answer": "England and Hong Kong"
      },
      {
        "turn_num": 19,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-19",
        "original_question": "What is his ethnicity?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-19",
        "answer": "Scot"
      },
      {
        "turn_num": 20,
        "turn_id": "34bbwhlwhab1k7k3vhca2pei8rowil-20",
        "original_question": "Did he turn into a doper?",
        "original_question_id": "34bbwhlwhab1k7k3vhca2pei8rowil-20",
        "answer": "Yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who identifies as a reformed rider?",
        "answer": "David Millar",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is a reformed rider?",
        "answer": "David Millar",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is a reformed rider?",
        "answer": "David Millar",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is a reformed rider?",
        "answer": "David Millar",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What cyclist's legacies are tainted?",
        "answer": "Lance Armstrong",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who has tainted legacies?",
        "answer": "Lance Armstrong",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who has tainted legacies of David Millar?",
        "answer": "Lance Armstrong",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who has tainted legacies?",
        "answer": "Lance Armstrong",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What did Lance Armstrong win?",
        "answer": "Tour de France champions",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did David Millar and Lance Armstrong win?",
        "answer": "Tour de France champions",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Lance Armstrong and David Millar win?",
        "answer": "Tour de France champions",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lance Armstrong win?",
        "answer": "Tour de France champions",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What cyclist's legacies are tainted, along with Lance Armstrong?",
        "answer": "Jan Ulrich",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who else has tainted legacies?",
        "answer": "Jan Ulrich",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who else besides Lance Armstrong and David Millar has tainted legacies?",
        "answer": "Jan Ulrich",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Lance Armstrong win besides the Tour de France?",
        "answer": "Jan Ulrich",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What do the tarnished reputations of certain cyclists create on the sport?",
        "answer": "dark shadow",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What do they cast?",
        "answer": "dark shadow",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What do the Tour de France champions cast?",
        "answer": "dark shadow",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What do the Tour de France champions cast?",
        "answer": "dark shadow",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Where does David Millar remember once sitting?",
        "answer": "Biarritz restaurant",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was David sitting in the past?",
        "answer": "Biarritz restaurant",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was David Millar sitting in the past?",
        "answer": "Biarritz restaurant",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was David Millar sitting in the past?",
        "answer": "Biarritz restaurant",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How long ago was David Millar in the Biarritz restaurant?",
        "answer": "nine years",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long ago was David sitting in Biarritz restaurant?",
        "answer": "nine years",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long ago was David Millar sitting in the Biarritz restaurant?",
        "answer": "nine years",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long ago was David Millar sitting in the Biarritz restaurant?",
        "answer": "nine years",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Where was the Biarritz restaurant?",
        "answer": "west France",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was the restaurant?",
        "answer": "west France",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was the Biarritz restaurant?",
        "answer": "west France",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was the Biarritz restaurant?",
        "answer": "west France",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who was David Millar's dining partner in Biarritz?",
        "answer": "Dave Brailsford",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was David Millar having dinner with at Biarritz restaurant in west France nine years ago?",
        "answer": "Dave Brailsford",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was David Millar having dinner with at the Biarritz restaurant?",
        "answer": "Dave Brailsford",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was David Millar having dinner with?",
        "answer": "Dave Brailsford",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who is Dave Brailsford?",
        "answer": "performance director at British Cycling and Team Sky",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Dave Brailsford now?",
        "answer": "performance director at British Cycling and Team Sky",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is Dave Brailsford now?",
        "answer": "performance director at British Cycling and Team Sky",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is Dave Brailsford now?",
        "answer": "performance director at British Cycling and Team Sky",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who arrived at the Biarritz restaurant?",
        "answer": "French police",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who arrived at the restaurant?",
        "answer": "French police",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who arrived at the Biarritz restaurant?",
        "answer": "French police",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who arrived at the Biarritz restaurant?",
        "answer": "French police",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What did the French police do to David Millar?",
        "answer": "arrest the cyclist,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the French police do to David Millar?",
        "answer": "arrest the cyclist,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the French police do?",
        "answer": "arrest the cyclist,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the French police do at Biarritz?",
        "answer": "arrest the cyclist,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Why did French police arrest David Millar?",
        "answer": "outing him as a drugs cheat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did the French police arrest the cyclist?",
        "answer": "outing him as a drugs cheat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did the French police arrest the cyclist, David Millar?",
        "answer": "outing him as a drugs cheat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did the French police arrest the cyclist?",
        "answer": "outing him as a drugs cheat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Did David Millar receive a ban from cycling?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they ban David Millar?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the French police ban David Millar?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the police ban David Millar?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How long was David Millar banned from cycling?",
        "answer": "two years",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long was David Millar banned for?",
        "answer": "two years",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "For how long did the French police ban David Millar?",
        "answer": "two years",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long did the French police ban David Millar?",
        "answer": "two years",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Did David Millar return to the sport of cycling?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did David Millar return to professional cycling after his ban?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did David Millar return to the US after being banned from cycling for two years?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Dave Brailsford return to Biarritz?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Where was David Millar born?",
        "answer": "Malta",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was David Millar born?",
        "answer": "Malta",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was David Millar born?",
        "answer": "Malta",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was David Millar born?",
        "answer": "Malta",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Where does David Millar live?",
        "answer": "England and Hong Kong",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where does David Millar live?",
        "answer": "England and Hong Kong",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where does David Millar live?",
        "answer": "England and Hong Kong",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does David Millar live?",
        "answer": "England and Hong Kong",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What is David Millar's ethnicity?",
        "answer": "Scot",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is David Millar's ethnicity?",
        "answer": "Scot",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is David Millar's ethnicity?",
        "answer": "Scot",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is David Millar's ethnicity?",
        "answer": "Scot",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Did David Millar turn to drugs?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did David Millar turn into a doper?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did David Millar turn into a doper?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did David Millar become a doper?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "35gmh2sv3ehhzt9f8cv90g34dzqeow": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-1",
        "original_question": "who played a prank?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-1",
        "answer": "Tom"
      },
      {
        "turn_num": 2,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-2",
        "original_question": "on who?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-2",
        "answer": "William"
      },
      {
        "turn_num": 3,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-3",
        "original_question": "who asks him about it?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-3",
        "answer": "Sam"
      },
      {
        "turn_num": 4,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-4",
        "original_question": "what did Sam ask?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-4",
        "answer": "What did you do to him?"
      },
      {
        "turn_num": 5,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-5",
        "original_question": "What did Tom do?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-5",
        "answer": "Put an advertisement of pills on his back"
      },
      {
        "turn_num": 6,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-6",
        "original_question": "what else?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-6",
        "answer": "and some other ads in his text books"
      },
      {
        "turn_num": 7,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-7",
        "original_question": "did William notice?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-7",
        "answer": "yes"
      },
      {
        "turn_num": 8,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-8",
        "original_question": "did he look angry when he was approaching?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-8",
        "answer": "yes"
      },
      {
        "turn_num": 9,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-9",
        "original_question": "what was he doing?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-9",
        "answer": "approaching with long strides"
      },
      {
        "turn_num": 10,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-10",
        "original_question": "and what else?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-10",
        "answer": "he was shaking his other fist wrathfully"
      },
      {
        "turn_num": 11,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-11",
        "original_question": "did he know who did it?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-11",
        "answer": "yes"
      },
      {
        "turn_num": 12,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-12",
        "original_question": "why couldn't he speak well?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-12",
        "answer": "he was so full of wrath he could not speak"
      },
      {
        "turn_num": 13,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-13",
        "original_question": "what did he say?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-13",
        "answer": "Tom Rover, you've--er--insulted me!"
      },
      {
        "turn_num": 14,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-14",
        "original_question": "what else?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-14",
        "answer": "\"You've humiliated me before the whole class! I'll--I'll----\""
      },
      {
        "turn_num": 15,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-15",
        "original_question": "what was Tom's reply?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-15",
        "answer": "Take a cough drop and clear your throat Billy,"
      },
      {
        "turn_num": 16,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-16",
        "original_question": "what was the advertisement for?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-16",
        "answer": "Gumley's Red Pills"
      },
      {
        "turn_num": 17,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-17",
        "original_question": "what did the advertisement say about them?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-17",
        "answer": "\"'Gumley's Red Pills for Red-Blooded People,'\""
      },
      {
        "turn_num": 18,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-18",
        "original_question": "what happened to the guy in Rottenberg?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-18",
        "answer": "he was flat on his back with half a dozen fatal diseases"
      },
      {
        "turn_num": 19,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-19",
        "original_question": "How long was he to live?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-19",
        "answer": "three days,"
      },
      {
        "turn_num": 20,
        "turn_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-20",
        "original_question": "did the Gumley's pills save him?",
        "original_question_id": "35gmh2sv3ehhzt9f8cv90g34dzqeow-20",
        "answer": "yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who pulled the prank on William?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who played a prank?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who played a prank?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who played a prank?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who did Tom play a prank on?",
        "answer": "William",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who played a prank?",
        "answer": "William",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "on who played a prank?",
        "answer": "William",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Tom pranking on?",
        "answer": "William",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who asks Tom about the prank he's pulled?",
        "answer": "Sam",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who asks Tom about the prank?",
        "answer": "Sam",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who asks Tom about the prank?",
        "answer": "Sam",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who asks William about the prank?",
        "answer": "Sam",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What was Sam's question?",
        "answer": "What did you do to him?",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what did Sam ask?",
        "answer": "What did you do to him?",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did Sam ask about the prank?",
        "answer": "What did you do to him?",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Sam ask William?",
        "answer": "What did you do to him?",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What exactly was the prank Tom pulled on William?",
        "answer": "Put an advertisement of pills on his back",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Tom do?",
        "answer": "Put an advertisement of pills on his back",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Tom do?",
        "answer": "Put an advertisement of pills on his back",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Tom do to William?",
        "answer": "Put an advertisement of pills on his back",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What was the second part of Tom's prank?",
        "answer": "and some other ads in his text books",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what else did Tom do?",
        "answer": "and some other ads in his text books",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else did Tom do besides put an advertisement of pills on his back?",
        "answer": "and some other ads in his text books",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Tom do to William, in addition to placing an advertisement for pills on his back?",
        "answer": "and some other ads in his text books",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Did William notice what Tom had done to him?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did William notice?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did William notice the prank?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did William notice the pill prank?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Did William look angry as he approached?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did William look angry when he was approaching?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Tom look angry when he was approaching Tom?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did William look angry when Tom approached?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "How was William Walking?",
        "answer": "approaching with long strides",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what was he doing?",
        "answer": "approaching with long strides",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was Tom doing when he was approaching Tom?",
        "answer": "approaching with long strides",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Tom doing when William approached him?",
        "answer": "approaching with long strides",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What was WIlliam doing with his fist?",
        "answer": "he was shaking his other fist wrathfully",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "and what else?",
        "answer": "he was shaking his other fist wrathfully",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and what else did William notice about the pranks besides the pill ad?",
        "answer": "he was shaking his other fist wrathfully",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Tom doing, in addition to putting pills on his back?",
        "answer": "he was shaking his other fist wrathfully",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Did William know who had pulled the prank on him?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did William know who played the prank?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did William know who did the prank?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did William know who did the prank?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Why did William have trouble expressing himself?",
        "answer": "he was so full of wrath he could not speak",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "why couldn't he speak well?",
        "answer": "he was so full of wrath he could not speak",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "why couldn't Tom speak well?",
        "answer": "he was so full of wrath he could not speak",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why couldn't William speak well?",
        "answer": "he was so full of wrath he could not speak",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What did William say to Tom?",
        "answer": "Tom Rover, you've--er--insulted me!",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what did he say?",
        "answer": "Tom Rover, you've--er--insulted me!",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did Tom say when he was so full of wrath he could not speak well?",
        "answer": "Tom Rover, you've--er--insulted me!",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Tom say?",
        "answer": "Tom Rover, you've--er--insulted me!",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "How did William continue his tirade against Tom?",
        "answer": "\"You've humiliated me before the whole class! I'll--I'll----\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what else did William say?",
        "answer": "\"You've humiliated me before the whole class! I'll--I'll----\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what else did Tom Rover say besides he was so full of wrathfully he could not speak well?",
        "answer": "\"You've humiliated me before the whole class! I'll--I'll----\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Tom Rover say, besides insulting William?",
        "answer": "\"You've humiliated me before the whole class! I'll--I'll----\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How did Tom reply to William?",
        "answer": "Take a cough drop and clear your throat Billy,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Tom Rover, you've--er--insulted me! You've humiliated me before the whole class! I'll--I'll----",
        "answer": "Take a cough drop and clear your throat Billy,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was Tom Rover's reply to the prankster?",
        "answer": "Take a cough drop and clear your throat Billy,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Tom Rover's reply to William?",
        "answer": "Take a cough drop and clear your throat Billy,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What was sign that Tom put on William's back advertising?",
        "answer": "Gumley's Red Pills",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the advertisement for?",
        "answer": "Gumley's Red Pills",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was the ad for in Tom's text books?",
        "answer": "Gumley's Red Pills",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the purpose of the advertisement?",
        "answer": "Gumley's Red Pills",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What did the poster say about Gumley's Red Pills?",
        "answer": "\"'Gumley's Red Pills for Red-Blooded People,'\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the advertisement say about Gumley's Red Pills?",
        "answer": "\"'Gumley's Red Pills for Red-Blooded People,'\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did the Gumley's Red Pills advertisement say about them?",
        "answer": "\"'Gumley's Red Pills for Red-Blooded People,'\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the advertisement say about Gumley's Red Pills?",
        "answer": "\"'Gumley's Red Pills for Red-Blooded People,'\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What did the man in Rottenberg go through?",
        "answer": "he was flat on his back with half a dozen fatal diseases",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened to the guy in Rottenberg?",
        "answer": "he was flat on his back with half a dozen fatal diseases",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what happened to the guy in Rottenberg?",
        "answer": "he was flat on his back with half a dozen fatal diseases",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened to the guy in Rottenberg?",
        "answer": "he was flat on his back with half a dozen fatal diseases",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "How long did the man in Rottenburg have to live?",
        "answer": "three days,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long was he to live?",
        "answer": "three days,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long was Tom Rover to live?",
        "answer": "three days,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long was Billy to live?",
        "answer": "three days,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Did Gumley's Red Pills save the man in Rottenberg?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did the Gumley's pills save him?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did the Gumley's pills save Tom Rover?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the Gumley's pills save Billy?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "35l9rvqfcoiow8keuzfokps6n55huz": {
    "number_of_turns": 14,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-1",
        "original_question": "what type of watercraft was it?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-1",
        "answer": "canoe"
      },
      {
        "turn_num": 2,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-2",
        "original_question": "where did it go?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-2",
        "answer": "the mouth of the river"
      },
      {
        "turn_num": 3,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-3",
        "original_question": "did it get there right away?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-3",
        "answer": "no"
      },
      {
        "turn_num": 4,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-4",
        "original_question": "when did it?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-4",
        "answer": "the third day"
      },
      {
        "turn_num": 5,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-5",
        "original_question": "was it morning?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-5",
        "answer": "no"
      },
      {
        "turn_num": 6,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-6",
        "original_question": "was it because it was far?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-6",
        "answer": "no"
      },
      {
        "turn_num": 7,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-7",
        "original_question": "then why?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-7",
        "answer": "the obstacles in the way"
      },
      {
        "turn_num": 8,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-8",
        "original_question": "what location were they coming close to?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-8",
        "answer": "the place where Gershom had left his wife and sister"
      },
      {
        "turn_num": 9,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-9",
        "original_question": "what was detected?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-9",
        "answer": "signs of an interest in the welfare of them"
      },
      {
        "turn_num": 10,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-10",
        "original_question": "and what else?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-10",
        "answer": "uneasiness"
      },
      {
        "turn_num": 11,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-11",
        "original_question": "was he comfortable leaving them alone?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-11",
        "answer": "no"
      },
      {
        "turn_num": 12,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-12",
        "original_question": "why?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-12",
        "answer": "the situation was exposed"
      },
      {
        "turn_num": 13,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-13",
        "original_question": "who is le Bourdon's companion?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-13",
        "answer": "Whiskey Centre"
      },
      {
        "turn_num": 14,
        "turn_id": "35l9rvqfcoiow8keuzfokps6n55huz-14",
        "original_question": "where is smoking coming from?",
        "original_question_id": "35l9rvqfcoiow8keuzfokps6n55huz-14",
        "answer": "the chiente"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is the watercraft present?",
        "answer": "canoe",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what type of watercraft was it?",
        "answer": "canoe",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what type of watercraft was it?",
        "answer": "canoe",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of watercraft was it?",
        "answer": "canoe",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Where did the canoe go?",
        "answer": "the mouth of the river",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where did the watercraft go?",
        "answer": "the mouth of the river",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where did the canoe go?",
        "answer": "the mouth of the river",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the canoe go?",
        "answer": "the mouth of the river",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Did the canoe get to the mouth of the river right away?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did it get there right away?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did the canoe get to the mouth of the river right away?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the canoe get to the mouth of the river right away?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "When did the canoe get to the mouth of the river?",
        "answer": "the third day",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when did the watercraft get to the mouth of the river?",
        "answer": "the third day",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when did the canoe get to the mouth of the river?",
        "answer": "the third day",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the canoe get to the mouth of the river?",
        "answer": "the third day",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did the canoe arrive in the morning?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "was it morning?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was the canoe morning?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was it morning?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did the distance create problems for the canoe?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "was it because it was far?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was the canoe because it was far?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the canoe far from the mouth of the river?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What was the problem with the canoe's path?",
        "answer": "the obstacles in the way",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "why did it take time to get to the mouth of the river?",
        "answer": "the obstacles in the way",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "then why was the canoe far from the mouth of the river?",
        "answer": "the obstacles in the way",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why was the canoe far from the river?",
        "answer": "the obstacles in the way",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What location was the group approaching?",
        "answer": "the place where Gershom had left his wife and sister",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what location were they coming close to?",
        "answer": "the place where Gershom had left his wife and sister",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what location were the canoes coming close to?",
        "answer": "the place where Gershom had left his wife and sister",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What location were the canoes coming close to?",
        "answer": "the place where Gershom had left his wife and sister",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What did Le Bourdon detect?",
        "answer": "signs of an interest in the welfare of them",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what was detected?",
        "answer": "signs of an interest in the welfare of them",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was detected by the canoe at the mouth of the river?",
        "answer": "signs of an interest in the welfare of them",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was detected?",
        "answer": "signs of an interest in the welfare of them",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What did Le Bourdon detect in his companion besides desire for welfare?",
        "answer": "uneasiness",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what else was detected?",
        "answer": "uneasiness",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and what else was detected in the canoe besides the signs of an interest in the welfare of Gershom and his wife and sister?",
        "answer": "uneasiness",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What else was detected besides signs of an interest in the welfare of Gershom's wife and sister?",
        "answer": "uneasiness",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Did Le Bourdon's companion think it was ok to leave his wife and sister alone?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Gershom comfortable leaving them alone?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was Gershom comfortable leaving his wife and sister alone?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Gershom comfortable leaving his wife and sister alone?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Why was Gershom against leaving alone his wife and sister?",
        "answer": "the situation was exposed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "why was he uncomfortable leaving them alone?",
        "answer": "the situation was exposed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "why was Gershom comfortable leaving his wife and sister alone?",
        "answer": "the situation was exposed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why was Gershom comfortable leaving his wife and sister alone?",
        "answer": "the situation was exposed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Who is le Bourdon traveling with?",
        "answer": "Whiskey Centre",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who is le Bourdon's companion?",
        "answer": "Whiskey Centre",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who is le Bourdon's companion?",
        "answer": "Whiskey Centre",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is Le Bourdon's companion?",
        "answer": "Whiskey Centre",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is the source of the smoke?",
        "answer": "the chiente",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where is smoking coming from?",
        "answer": "the chiente",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is smoking coming from in the scotch whisky centre?",
        "answer": "the chiente",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where is the smoke coming from?",
        "answer": "the chiente",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "35ldd5557a4wlqgdrirz67r42w6km6": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-1",
        "original_question": "Who was wanted to get somewhere?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-1",
        "answer": ". Uncle John"
      },
      {
        "turn_num": 2,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-2",
        "original_question": "Where did he want to go?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-2",
        "answer": "Rome and Venice,"
      },
      {
        "turn_num": 3,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-3",
        "original_question": "What did Mr. Watson do?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-3",
        "answer": "urged them to visit Syracus"
      },
      {
        "turn_num": 4,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-4",
        "original_question": "Why?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-4",
        "answer": "since they were not likely to return to Sicily again"
      },
      {
        "turn_num": 5,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-5",
        "original_question": "What did they canvas?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-5",
        "answer": "their future travels"
      },
      {
        "turn_num": 6,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-6",
        "original_question": "How?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-6",
        "answer": "considerable earnestness."
      },
      {
        "turn_num": 7,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-7",
        "original_question": "What was a few hours away from Taormina?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-7",
        "answer": "most famous of all the ancient historic capitals w"
      },
      {
        "turn_num": 8,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-8",
        "original_question": "How long did they decide to stay in Syracuse?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-8",
        "answer": "a week"
      },
      {
        "turn_num": 9,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-9",
        "original_question": "Who pleaded?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-9",
        "answer": "Kenneth"
      },
      {
        "turn_num": 10,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-10",
        "original_question": "What did he want?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-10",
        "answer": "one more day"
      },
      {
        "turn_num": 11,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-11",
        "original_question": "To do what?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-11",
        "answer": "to finish his picture of Etna"
      },
      {
        "turn_num": 12,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-12",
        "original_question": "Did Uncle John confess?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-12",
        "answer": "yes"
      },
      {
        "turn_num": 13,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-13",
        "original_question": "What was he feeling?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-13",
        "answer": "Uneasy"
      },
      {
        "turn_num": 14,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-14",
        "original_question": "Did Mr. Watson advise them?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-14",
        "answer": "yes"
      },
      {
        "turn_num": 15,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-15",
        "original_question": "Who might try to entrap them?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-15",
        "answer": "Il Duca"
      },
      {
        "turn_num": 16,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-16",
        "original_question": "Who visited them?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-16",
        "answer": "the duke"
      },
      {
        "turn_num": 17,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-17",
        "original_question": "When?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-17",
        "answer": "the enxt day"
      },
      {
        "turn_num": 18,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-18",
        "original_question": "What was he dressed in?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-18",
        "answer": "his usual faded velvet costume"
      },
      {
        "turn_num": 19,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-19",
        "original_question": "Who wore a gray gown?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-19",
        "answer": "Tato"
      },
      {
        "turn_num": 20,
        "turn_id": "35ldd5557a4wlqgdrirz67r42w6km6-20",
        "original_question": "Who caught her rapturously?",
        "original_question_id": "35ldd5557a4wlqgdrirz67r42w6km6-20",
        "answer": "patsy"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who had somwhere they wanted to go?",
        "answer": ". Uncle John",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was wanted to get somewhere?",
        "answer": ". Uncle John",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was wanted to get somewhere?",
        "answer": ". Uncle John",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was wanted to go somewhere?",
        "answer": ". Uncle John",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Where did Uncle John want to go?",
        "answer": "Rome and Venice,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Uncle John want to go?",
        "answer": "Rome and Venice,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Uncle John want to go?",
        "answer": "Rome and Venice,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Uncle John want to go?",
        "answer": "Rome and Venice,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What did Mr. Watson say to the group?",
        "answer": "urged them to visit Syracus",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Mr. Watson do?",
        "answer": "urged them to visit Syracus",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Mr. Watson do?",
        "answer": "urged them to visit Syracus",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Mr. Watson do?",
        "answer": "urged them to visit Syracus",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Why did Mr. Watson tell the group to visit Syracus?",
        "answer": "since they were not likely to return to Sicily again",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did Mr. Watson urge them to visit Syracuse?",
        "answer": "since they were not likely to return to Sicily again",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did Mr. Watson urge his family to visit Syracus?",
        "answer": "since they were not likely to return to Sicily again",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Mr. Watson urge Uncle John and his friends to go to Syracus?",
        "answer": "since they were not likely to return to Sicily again",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What subject did the group canvas?",
        "answer": "their future travels",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they canvas?",
        "answer": "their future travels",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Mr. Watson canvas?",
        "answer": "their future travels",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Uncle John and Mr. Watson canvas?",
        "answer": "their future travels",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "How did the group canvas their travels?",
        "answer": "considerable earnestness.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did they canvas their future travels?",
        "answer": "considerable earnestness.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Mr. Watson and his family canvas for their future travels?",
        "answer": "considerable earnestness.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did Uncle John and Mr. Watson canvas?",
        "answer": "considerable earnestness.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What was a few hour's distance from Taormina?",
        "answer": "most famous of all the ancient historic capitals w",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was a few hours away from Taormina?",
        "answer": "most famous of all the ancient historic capitals w",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was a few hours away from Taormina?",
        "answer": "most famous of all the ancient historic capitals w",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was a few hours away from Taormina?",
        "answer": "most famous of all the ancient historic capitals w",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How long did the group choose to stay in Syracus?",
        "answer": "a week",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long did they decide to stay in Syracuse?",
        "answer": "a week",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long did Mr. Watson decide to stay in Syracuse?",
        "answer": "a week",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long did Mr. Watson and Uncle John decide to stay in Syracuse?",
        "answer": "a week",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who was begging?",
        "answer": "Kenneth",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who pleaded?",
        "answer": "Kenneth",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who pleaded for them to visit Syracus?",
        "answer": "Kenneth",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who pleaded?",
        "answer": "Kenneth",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What was Kenneth pleading for?",
        "answer": "one more day",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Kenneth want?",
        "answer": "one more day",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Kenneth Watson want?",
        "answer": "one more day",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Kenneth Watson want?",
        "answer": "one more day",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What did Kenneth need one more day to do?",
        "answer": "to finish his picture of Etna",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "To do what?",
        "answer": "to finish his picture of Etna",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "To do what?",
        "answer": "to finish his picture of Etna",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Kenneth Watson want to do?",
        "answer": "to finish his picture of Etna",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Uncle John have a confession?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Uncle John confess?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Uncle John confess to the murder of his wife?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Uncle John confess?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "How was Uncle John feeling?",
        "answer": "Uneasy",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Uncle John feeling?",
        "answer": "Uneasy",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Uncle John feeling after finishing his picture of Etna?",
        "answer": "Uneasy",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Uncle John feeling?",
        "answer": "Uneasy",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Did Mr. Watson give the group advice?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Mr. Watson advise them?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Mr. Watson advise Uncle John and his wife to visit Sicily?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Mr. Watson advise the group?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Who may attempt to entrap the group?",
        "answer": "Il Duca",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who might try to entrap them?",
        "answer": "Il Duca",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who might try to entrap Uncle John and his wife?",
        "answer": "Il Duca",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who might try and entrap Uncle John and Kenneth?",
        "answer": "Il Duca",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Who did the group receive a visit from?",
        "answer": "the duke",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who visited them?",
        "answer": "the duke",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who visited Uncle John and Il Duca?",
        "answer": "the duke",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who visited Uncle John and Kenneth?",
        "answer": "the duke",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "When did the Duke visit the group?",
        "answer": "the enxt day",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did the duke visit them?",
        "answer": "the enxt day",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Mr. Watson visit the duke of etna?",
        "answer": "the enxt day",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Il Duca visit the group?",
        "answer": "the enxt day",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "How was the Duke dressed?",
        "answer": "his usual faded velvet costume",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was he dressed in?",
        "answer": "his usual faded velvet costume",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Uncle John Watson dressed in?",
        "answer": "his usual faded velvet costume",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Il Duca dressed in?",
        "answer": "his usual faded velvet costume",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Who was wearing a grey gown?",
        "answer": "Tato",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who wore a gray gown?",
        "answer": "Tato",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who wore a gray gown?",
        "answer": "Tato",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who wore a gray gown?",
        "answer": "Tato",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Who caught Tato rapturously?",
        "answer": "patsy",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who caught her rapturously?",
        "answer": "patsy",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who caught Tato rapturously?",
        "answer": "patsy",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who caught Tato in the rapturous?",
        "answer": "patsy",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "374tnbha8bviqa3mnqz7woqk90aqy3": {
    "number_of_turns": 11,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-1",
        "original_question": "who had 3 home runs ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-1",
        "answer": "Pablo Sandoval"
      },
      {
        "turn_num": 2,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-2",
        "original_question": "for what team ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-2",
        "answer": "San Francisco Giants"
      },
      {
        "turn_num": 3,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-3",
        "original_question": "was this the super bowl ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-3",
        "answer": "no"
      },
      {
        "turn_num": 4,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-4",
        "original_question": "what was it ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-4",
        "answer": "the World Series"
      },
      {
        "turn_num": 5,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-5",
        "original_question": "Barry Zito gave up what ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-5",
        "answer": "just one run"
      },
      {
        "turn_num": 6,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-6",
        "original_question": "who is Justin Verlander ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-6",
        "answer": "Tigers pitching ace"
      },
      {
        "turn_num": 7,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-7",
        "original_question": "what did Babe Ruth do ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-7",
        "answer": "Homered three times in one World Series game, twice"
      },
      {
        "turn_num": 8,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-8",
        "original_question": "who else did this ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-8",
        "answer": "Reggie Jackson"
      },
      {
        "turn_num": 9,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-9",
        "original_question": "who has been swinging the bat well",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-9",
        "answer": "Sandoval"
      },
      {
        "turn_num": 10,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-10",
        "original_question": "who said that ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-10",
        "answer": "His manager"
      },
      {
        "turn_num": 11,
        "turn_id": "374tnbha8bviqa3mnqz7woqk90aqy3-11",
        "original_question": "what happened in the 5th ?",
        "original_question_id": "374tnbha8bviqa3mnqz7woqk90aqy3-11",
        "answer": "Sandoval added another home run off Al Alburquerque"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who got three home runs?",
        "answer": "Pablo Sandoval",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who had 3 home runs ?",
        "answer": "Pablo Sandoval",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who had 3 home runs?",
        "answer": "Pablo Sandoval",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who had three home runs?",
        "answer": "Pablo Sandoval",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who was Pablo Sandoval playing for?",
        "answer": "San Francisco Giants",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who had 3 home runs ?",
        "answer": "San Francisco Giants",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "for what team?",
        "answer": "San Francisco Giants",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What team had Pablo Sandoval hit three home runs?",
        "answer": "San Francisco Giants",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Is the game being discussed the Super Bowl?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "was this the super bowl?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was the super bowl the San Francisco Giants' Super Bowl?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the Super Bowl the Super Bowl?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What game is being discussed?",
        "answer": "the World Series",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what was it?",
        "answer": "the World Series",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was the Super Bowl?",
        "answer": "the World Series",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the Super Bowl?",
        "answer": "the World Series",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What did Barry Zito give up?",
        "answer": "just one run",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Barry Zito gave up what?",
        "answer": "just one run",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Barry Zito gave up what?",
        "answer": "just one run",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Barry Zito give up?",
        "answer": "just one run",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What is Justin Verlander's position?",
        "answer": "Tigers pitching ace",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Justin Verlander",
        "answer": "Tigers pitching ace",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who is Justin Verlander?",
        "answer": "Tigers pitching ace",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Justin Verlander's role?",
        "answer": "Tigers pitching ace",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did Babe Ruth manage to do?",
        "answer": "Homered three times in one World Series game, twice",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Babe Ruth",
        "answer": "Homered three times in one World Series game, twice",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did Babe Ruth do?",
        "answer": "Homered three times in one World Series game, twice",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Babe Ruth do?",
        "answer": "Homered three times in one World Series game, twice",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Who else was able to achieve what Babe Ruth did?",
        "answer": "Reggie Jackson",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "players who hit three home runs in one World Series game",
        "answer": "Reggie Jackson",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who else did the home run in the World Series besides babe ruth?",
        "answer": "Reggie Jackson",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Babe Ruth homer twice in the World Series, in addition to Barry Zito?",
        "answer": "Reggie Jackson",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who's been doing well at swinging the bat?",
        "answer": "Sandoval",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who has been swinging the bat well",
        "answer": "Sandoval",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who has been swinging the bat well in the World Series?",
        "answer": "Sandoval",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who has been swinging the bat well?",
        "answer": "Sandoval",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who praised Pablo Sandoval's bat swinging skills?",
        "answer": "His manager",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who has been swinging the bat well",
        "answer": "His manager",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who said Pablo Sandoval has been swinging the bat well in the World Series?",
        "answer": "His manager",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who said that Pablo Sandoval had been swinging the bat well?",
        "answer": "His manager",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What took place in the 5th inning?",
        "answer": "Sandoval added another home run off Al Alburquerque",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what happened in the 5th inning?",
        "answer": "Sandoval added another home run off Al Alburquerque",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what happened in the 5th with Pablo Sandoval?",
        "answer": "Sandoval added another home run off Al Alburquerque",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened in the 5th game?",
        "answer": "Sandoval added another home run off Al Alburquerque",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "37m28k1j0qd08516cu1iw1wrtkcajw": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-1",
        "original_question": "What time of year was it?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-1",
        "answer": "not far past the middle of September"
      },
      {
        "turn_num": 2,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-2",
        "original_question": "how was the weather?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-2",
        "answer": "extremely cold"
      },
      {
        "turn_num": 3,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-3",
        "original_question": "What time did they reach the destination?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-3",
        "answer": "nearly one o'clock"
      },
      {
        "turn_num": 4,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-4",
        "original_question": "What did she need to do first?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-4",
        "answer": "enter the house"
      },
      {
        "turn_num": 5,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-5",
        "original_question": "and then?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-5",
        "answer": "introduce herself"
      },
      {
        "turn_num": 6,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-6",
        "original_question": "What was she doing there?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-6",
        "answer": "to teach her children"
      },
      {
        "turn_num": 7,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-7",
        "original_question": "Who's children?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-7",
        "answer": "Mrs. Bloomfield's"
      },
      {
        "turn_num": 8,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-8",
        "original_question": "How many?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-8",
        "answer": "four"
      },
      {
        "turn_num": 9,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-9",
        "original_question": "Which was known for speaking the truth?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-9",
        "answer": "Tom"
      },
      {
        "turn_num": 10,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-10",
        "original_question": "Does the story tell you who the youngest was?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-10",
        "answer": "Yes"
      },
      {
        "turn_num": 11,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-11",
        "original_question": "who?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-11",
        "answer": "Harriet"
      },
      {
        "turn_num": 12,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-12",
        "original_question": "Who is six?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-12",
        "answer": "Mary Ann"
      },
      {
        "turn_num": 13,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-13",
        "original_question": "and seven?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-13",
        "answer": "Tom"
      },
      {
        "turn_num": 14,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-14",
        "original_question": "Whos bed is going to be in her room?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-14",
        "answer": "Mary Ann"
      },
      {
        "turn_num": 15,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-15",
        "original_question": "How old was this teacher?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-15",
        "answer": "near nineteen"
      },
      {
        "turn_num": 16,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-16",
        "original_question": "What held her attention on the meal?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-16",
        "answer": "the toughness of the beefsteaks"
      },
      {
        "turn_num": 17,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-17",
        "original_question": "Where did they dine?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-17",
        "answer": "the dining-room"
      },
      {
        "turn_num": 18,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-18",
        "original_question": "Who dined together?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-18",
        "answer": "the girl, while Mrs. Bloomfield watched"
      },
      {
        "turn_num": 19,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-19",
        "original_question": "HAs Mrs. Bloomfield been putting in a lot of time teaching the children?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-19",
        "answer": "no"
      },
      {
        "turn_num": 20,
        "turn_id": "37m28k1j0qd08516cu1iw1wrtkcajw-20",
        "original_question": "Does she think they are unwilling to learn?",
        "original_question_id": "37m28k1j0qd08516cu1iw1wrtkcajw-20",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "WHen in the year was it?",
        "answer": "not far past the middle of September",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What time of year was it?",
        "answer": "not far past the middle of September",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What time of year was it?",
        "answer": "not far past the middle of September",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What time of year was it?",
        "answer": "not far past the middle of September",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was the weather like?",
        "answer": "extremely cold",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how was the weather?",
        "answer": "extremely cold",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How was the weather in the fall?",
        "answer": "extremely cold",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How was the weather?",
        "answer": "extremely cold",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "When did the group to where they were going?",
        "answer": "nearly one o'clock",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What time did they reach the destination?",
        "answer": "nearly one o'clock",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What time did the icebergs reach the destination?",
        "answer": "nearly one o'clock",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "At what time did the group reach their destination?",
        "answer": "nearly one o'clock",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What did the girl need to do first?",
        "answer": "enter the house",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did she need to do first?",
        "answer": "enter the house",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the girl need to do first to reach the destination?",
        "answer": "enter the house",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the girl need to do first?",
        "answer": "enter the house",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What did the girl need to do second",
        "answer": "introduce herself",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "and then?",
        "answer": "introduce herself",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and then?",
        "answer": "introduce herself",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the girl need to do before entering the house?",
        "answer": "introduce herself",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Why was the girl at the house?",
        "answer": "to teach her children",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was she doing there?",
        "answer": "to teach her children",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was she doing at the house when she arrived?",
        "answer": "to teach her children",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the girl doing in the house?",
        "answer": "to teach her children",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Whose children was the girl going to teach?",
        "answer": "Mrs. Bloomfield's",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who's children?",
        "answer": "Mrs. Bloomfield's",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who's children did she teach at the house?",
        "answer": "Mrs. Bloomfield's",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose children did the girl have to teach?",
        "answer": "Mrs. Bloomfield's",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How many hcildren did Mrs. Bloomfield have?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many children did Mrs. Bloomfield have?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many children did Mrs. Bloomfield have?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many children did Mrs. Bloomfield have?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Which of Mrs. Bloomfield's children was known for speaking the truth?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which was known for speaking the truth?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which was known for speaking the truth about Mrs. Bloomfield's children?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which person was known for speaking the truth?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Does the story give the name of the youngest Bloomfield child?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does the story tell you who the youngest was?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does the story tell you who the youngest of Mrs. Bloomfield's children was?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the story tell you who the youngest of Mrs. Bloomfield's children was?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who is the youngest of the Bloomfield kids?",
        "answer": "Harriet",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was the youngest?",
        "answer": "Harriet",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was the youngest Bloomfield's child?",
        "answer": "Harriet",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the youngest of Mrs. Bloomfield's children?",
        "answer": "Harriet",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Which Bloomfield child is six?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is six?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is six?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is six years old?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Which Bloomfield child is seven?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is seven?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and seven?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of Mary Ann's seven year old sister?",
        "answer": "Tom",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Whose bed will be in the teacher's room?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Whose bed is going to be in her room?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Whos bed is going to be in Mrs. Bloomfield's room?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose bed is going to be in Mary Ann's room?",
        "answer": "Mary Ann",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What was the age of the teacher?",
        "answer": "near nineteen",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How old was the teacher?",
        "answer": "near nineteen",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How old was the teacher who taught Mary Ann?",
        "answer": "near nineteen",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How old was the teacher?",
        "answer": "near nineteen",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What was the teacher's attention focused on during the meal?",
        "answer": "the toughness of the beefsteaks",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What held her attention on the meal?",
        "answer": "the toughness of the beefsteaks",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What held Mary Ann's attention on the meal?",
        "answer": "the toughness of the beefsteaks",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kept Mary Ann's attention on the meal?",
        "answer": "the toughness of the beefsteaks",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Where did the group eat?",
        "answer": "the dining-room",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did they dine?",
        "answer": "the dining-room",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the children of Mrs. Bloomfield dine?",
        "answer": "the dining-room",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Mary Ann and Harriet dine?",
        "answer": "the dining-room",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Who all was eating together?",
        "answer": "the girl, while Mrs. Bloomfield watched",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who dined together?",
        "answer": "the girl, while Mrs. Bloomfield watched",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who dined together in the dining room at Mrs. Bloomfield's?",
        "answer": "the girl, while Mrs. Bloomfield watched",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who ate together?",
        "answer": "the girl, while Mrs. Bloomfield watched",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Does Mrs. Bloomfield spend much time teaching the children?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Has Mrs. Bloomfield been putting in a lot of time teaching the children?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "HAs Mrs. Bloomfield been putting in a lot of time teaching the children?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Has Mrs. Bloomfield been putting in a lot of time teaching the children?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Does Mrs. Bloomfield feel as though the children are unwilling to learn?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does she think they are unwilling to learn?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Mrs. Bloomfield think the children are unwilling to learn?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Mrs. Bloomfield think that the children are unwilling to learn?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "39gaf6dqwr0d5co0x0m8ooeiklzv1g": {
    "number_of_turns": 13,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-1",
        "original_question": "who funds MedlinePlus?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-1",
        "answer": "the NLM"
      },
      {
        "turn_num": 2,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-2",
        "original_question": "what other site do they offer that has consumer health info?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-2",
        "answer": "MedlinePlus"
      },
      {
        "turn_num": 3,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-3",
        "original_question": "is there any other site that they offer?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-3",
        "answer": "yes"
      },
      {
        "turn_num": 4,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-4",
        "original_question": "name one",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-4",
        "answer": "PubMed Health"
      },
      {
        "turn_num": 5,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-5",
        "original_question": "what links patients EHR to related MedlinePlus info?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-5",
        "answer": "MedlinePlus Connect"
      },
      {
        "turn_num": 6,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-6",
        "original_question": "as of 2015 how many people around the world use MedlinePlus?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-6",
        "answer": "about 400 million people\\"
      },
      {
        "turn_num": 7,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-7",
        "original_question": "how many languages is it offered in?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-7",
        "answer": "Two"
      },
      {
        "turn_num": 8,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-8",
        "original_question": "what are the languages?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-8",
        "answer": "English and Spanish"
      },
      {
        "turn_num": 9,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-9",
        "original_question": "is MedlinePlus optimized for mobile?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-9",
        "answer": "yes"
      },
      {
        "turn_num": 10,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-10",
        "original_question": "what is the cost to end users?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-10",
        "answer": "it is free"
      },
      {
        "turn_num": 11,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-11",
        "original_question": "when was it introduced?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-11",
        "answer": "in October 1998"
      },
      {
        "turn_num": 12,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-12",
        "original_question": "by who?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-12",
        "answer": "The National Library of Medicine"
      },
      {
        "turn_num": 13,
        "turn_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-13",
        "original_question": "what service is it similar to?",
        "original_question_id": "39gaf6dqwr0d5co0x0m8ooeiklzv1g-13",
        "answer": "WebMD"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Where does MedlinePlus get its funding?",
        "answer": "the NLM",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who funds MedlinePlus?",
        "answer": "the NLM",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who funds MedlinePlus?",
        "answer": "the NLM",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the funder of MedlinePlus?",
        "answer": "the NLM",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What site does the NLM offer with consumer health info?",
        "answer": "MedlinePlus",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "other site that MedlinePlus offers",
        "answer": "MedlinePlus",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what other site do MedlinePlus offer that has consumer health info?",
        "answer": "MedlinePlus",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What other site does the NLM offer that has consumer health info?",
        "answer": "MedlinePlus",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Does the NLM offer sites other than MedlinePlus?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "other sites offered by MedlinePlus",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is there any other site that MedlinePlus offers besides consumer health info?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is there any other site that the NLM offers besides MedlinePlus?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What site was created to complement MedlinePlus?",
        "answer": "PubMed Health",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "name one",
        "answer": "PubMed Health",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "name one site that has consumer health info that MedlinePlus offers?",
        "answer": "PubMed Health",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is one of the sites that the NLM offers that has consumer health info?",
        "answer": "PubMed Health",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "How is a patient's EHR linked to their MedlinePlus information?",
        "answer": "MedlinePlus Connect",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what links patients EHR to related MedlinePlus info?",
        "answer": "MedlinePlus Connect",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what links patients EHR to related MedlinePlus info?",
        "answer": "MedlinePlus Connect",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What links patients' EHR to related MedlinePlus info?",
        "answer": "MedlinePlus Connect",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "In 2015, how many users did MedlinePlus have?",
        "answer": "about 400 million people\\",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "As of 2015, how many people around the world use MedlinePlus?",
        "answer": "about 400 million people\\",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "as of 2015 how many people around the world use MedlinePlus?",
        "answer": "about 400 million people\\",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many people use MedlinePlus as of 2015?",
        "answer": "about 400 million people\\",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How many languages is MedlinePlus offered in?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many languages is MedlinePlus offered in?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many languages is MedlinePlus offered in?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many languages is MedlinePlus available in?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What languages does MedlinePlus come in?",
        "answer": "English and Spanish",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what languages is MedlinePlus offered in?",
        "answer": "English and Spanish",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what are the languages that MedlinePlus is offered in?",
        "answer": "English and Spanish",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the languages that are available on MedlinePlus?",
        "answer": "English and Spanish",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Does MedlinePlus have a site optimized for mobile devices?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is MedlinePlus optimized for mobile?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is MedlinePlus optimized for mobile?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is MedlinePlus optimized for mobile?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "How much do users pay to access MedlinePlus?",
        "answer": "it is free",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the cost to end users of MedlinePlus?",
        "answer": "it is free",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is the cost to end users of MedlinePlus?",
        "answer": "it is free",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the cost of using MedlinePlus?",
        "answer": "it is free",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "When was MedlinePlus introduced?",
        "answer": "in October 1998",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was MedlinePlus introduced?",
        "answer": "in October 1998",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was MedlinePlus introduced?",
        "answer": "in October 1998",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was MedlinePlus introduced?",
        "answer": "in October 1998",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Who created MedlinePlus?",
        "answer": "The National Library of Medicine",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who introduced MedlinePlus?",
        "answer": "The National Library of Medicine",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "by who introduced MedlinePlus in October 1998?",
        "answer": "The National Library of Medicine",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who introduced MedlinePlus in October 1998?",
        "answer": "The National Library of Medicine",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What service is MedlinePlus similar to?",
        "answer": "WebMD",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what service is MedlinePlus similar to?",
        "answer": "WebMD",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what service is MedlinePlus similar to?",
        "answer": "WebMD",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What service is MedlinePlus similar to?",
        "answer": "WebMD",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "39gxdjn2otevgc8lwlvn3y1qxc9v89": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-1",
        "original_question": "How many friends were there?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-1",
        "answer": "Three"
      },
      {
        "turn_num": 2,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-2",
        "original_question": "Where did they live?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-2",
        "answer": "England."
      },
      {
        "turn_num": 3,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-3",
        "original_question": "Did they all live in different cities there?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-3",
        "answer": "No"
      },
      {
        "turn_num": 4,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-4",
        "original_question": "What did they do on their school break?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-4",
        "answer": "Went to a lake."
      },
      {
        "turn_num": 5,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-5",
        "original_question": "What did they do there?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-5",
        "answer": "Fished"
      },
      {
        "turn_num": 6,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-6",
        "original_question": "From the shore?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-6",
        "answer": "No"
      },
      {
        "turn_num": 7,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-7",
        "original_question": "What did they use?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-7",
        "answer": "A boat."
      },
      {
        "turn_num": 8,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-8",
        "original_question": "Did they search all day for a good spot to no avail?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-8",
        "answer": "No."
      },
      {
        "turn_num": 9,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-9",
        "original_question": "Was the spot they found a good spot to cast their lines?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-9",
        "answer": "Yes"
      },
      {
        "turn_num": 10,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-10",
        "original_question": "How come?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-10",
        "answer": "there were a lot of fish."
      },
      {
        "turn_num": 11,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-11",
        "original_question": "Who suggested making the spot easy to return to?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-11",
        "answer": "Bruce"
      },
      {
        "turn_num": 12,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-12",
        "original_question": "Who was in agreement?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-12",
        "answer": "Bruno"
      },
      {
        "turn_num": 13,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-13",
        "original_question": "Who was angry over the plan?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-13",
        "answer": "Dick"
      },
      {
        "turn_num": 14,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-14",
        "original_question": "What did he want to do instead?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-14",
        "answer": "Stay out."
      },
      {
        "turn_num": 15,
        "turn_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-15",
        "original_question": "Would this be a problem if it were a clear night?",
        "original_question_id": "39gxdjn2otevgc8lwlvn3y1qxc9v89-15",
        "answer": "No"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "How many friends does the story mention?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many friends were there?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many friends were there?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many friends were there?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Where did the three friends live?",
        "answer": "England.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did they live?",
        "answer": "England.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the three friends live?",
        "answer": "England.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the three friends live?",
        "answer": "England.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Did the three friends all live in different cities in England?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they all live in different cities there?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the Three of them all live in different cities in England?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the three friends live in different cities in England?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What did the three friends do on their school break?",
        "answer": "Went to a lake.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they do on their school break?",
        "answer": "Went to a lake.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the Three do on their school break?",
        "answer": "Went to a lake.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the three friends do during their school break?",
        "answer": "Went to a lake.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What did the three friends do on the lake?",
        "answer": "Fished",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they do there?",
        "answer": "Fished",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the boys do in England?",
        "answer": "Fished",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the group do at the lake?",
        "answer": "Fished",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did the friends stay on shore to fish?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "From the shore?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "From the shore of the Lake District?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the group do on the shore?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did the friends go fishing in?",
        "answer": "A boat.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they use?",
        "answer": "A boat.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the boys use to fish?",
        "answer": "A boat.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the group use to fish?",
        "answer": "A boat.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Were the friends not able to find a spot to fish all day?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they search all day for a good spot to no avail?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the boys search all day for a good spot to no avail?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the group search for a good spot all day?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Were the friends happy with the spot they chose to fish in?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the spot they found a good spot to cast their lines?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the spot they found a good spot to cast their lines?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the spot where the group found a good spot to cast their lines?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Why did the friends think they'd found a good spot for fishing?",
        "answer": "there were a lot of fish.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How come the spot they found was a good spot to cast their lines?",
        "answer": "there were a lot of fish.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How come they found a good spot to cast their lines?",
        "answer": "there were a lot of fish.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did the group come across a good spot to cast their lines?",
        "answer": "there were a lot of fish.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who suggested that they mark the spot so it would be easy to return to?",
        "answer": "Bruce",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who suggested making the spot easy to return to?",
        "answer": "Bruce",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who suggested making the spot easy to return to?",
        "answer": "Bruce",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who suggested making the spot easy to return to?",
        "answer": "Bruce",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Who agreed with Bruce?",
        "answer": "Bruno",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was in agreement?",
        "answer": "Bruno",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was in agreement with Bruce?",
        "answer": "Bruno",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was in agreement with Bruce?",
        "answer": "Bruno",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Who was angry at Bruce's plan?",
        "answer": "Dick",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was angry over the plan?",
        "answer": "Dick",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was angry over the plan to make the spot easy to return to?",
        "answer": "Dick",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was upset about the plan?",
        "answer": "Dick",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What did Dick want to do instead of mark the spot?",
        "answer": "Stay out.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Dick want to do instead?",
        "answer": "Stay out.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Dick want to do instead of fishing?",
        "answer": "Stay out.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Dick want to do instead of fishing?",
        "answer": "Stay out.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Would Bruce's idea pose a problem if the night was clear?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Would this be a problem if it were a clear night?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Would the fisheries be a problem if it were a clear night?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Would the fish be a problem if it was a clear night?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "39u1bhvtdlru2nyqf90cbz5ukfdt3v": {
    "number_of_turns": 13,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-1",
        "original_question": "Why is Lisa excited?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-1",
        "answer": "Saturday, Whiskers turns two"
      },
      {
        "turn_num": 2,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-2",
        "original_question": "Who is Whiskers?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-2",
        "answer": "a cat"
      },
      {
        "turn_num": 3,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-3",
        "original_question": "What does it look like?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-3",
        "answer": "black with a white spot"
      },
      {
        "turn_num": 4,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-4",
        "original_question": "Where did she rush on Friday?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-4",
        "answer": "the pet store"
      },
      {
        "turn_num": 5,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-5",
        "original_question": "For what?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-5",
        "answer": "to buy presents"
      },
      {
        "turn_num": 6,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-6",
        "original_question": "What did she get last year?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-6",
        "answer": "a play mouse"
      },
      {
        "turn_num": 7,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-7",
        "original_question": "anything else?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-7",
        "answer": "a blue feather"
      },
      {
        "turn_num": 8,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-8",
        "original_question": "What about this birthday?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-8",
        "answer": "a ball of yarn"
      },
      {
        "turn_num": 9,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-9",
        "original_question": "What else?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-9",
        "answer": "a bowl"
      },
      {
        "turn_num": 10,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-10",
        "original_question": "whats on the bowl?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-10",
        "answer": "a picture"
      },
      {
        "turn_num": 11,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-11",
        "original_question": "of what?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-11",
        "answer": "a black cat"
      },
      {
        "turn_num": 12,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-12",
        "original_question": "Does it match Whiskers?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-12",
        "answer": "definitely"
      },
      {
        "turn_num": 13,
        "turn_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-13",
        "original_question": "Where does this cat sleep?",
        "original_question_id": "39u1bhvtdlru2nyqf90cbz5ukfdt3v-13",
        "answer": "on her favorite chair"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What's got Lisa so excited?",
        "answer": "Saturday, Whiskers turns two",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why is Lisa excited?",
        "answer": "Saturday, Whiskers turns two",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why is Lisa excited?",
        "answer": "Saturday, Whiskers turns two",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why is Lisa excited?",
        "answer": "Saturday, Whiskers turns two",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What kind of animal is Whiskers?",
        "answer": "a cat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Whiskers?",
        "answer": "a cat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is Whiskers?",
        "answer": "a cat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Whiskers' role?",
        "answer": "a cat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What does Whiskers look like?",
        "answer": "black with a white spot",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Whiskers look like?",
        "answer": "black with a white spot",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Saturday, Whiskers look like?",
        "answer": "black with a white spot",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Whiskers's appearance?",
        "answer": "black with a white spot",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Where did Lisa rush to on Friday?",
        "answer": "the pet store",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Lisa rush on Friday?",
        "answer": "the pet store",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Lisa rush on Friday?",
        "answer": "the pet store",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Lisa rush on Friday?",
        "answer": "the pet store",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Why did Lisa go to the pet store?",
        "answer": "to buy presents",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "For what did Lisa rush to the pet store?",
        "answer": "to buy presents",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "For what?",
        "answer": "to buy presents",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lisa go to the pet store for?",
        "answer": "to buy presents",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What did Lisa get Whiskers last year, along with the blue feather?",
        "answer": "a play mouse",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Lisa get last year?",
        "answer": "a play mouse",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Lisa get last year?",
        "answer": "a play mouse",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lisa get last year?",
        "answer": "a play mouse",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did Lisa get Whiskers last year, along with the play mouse?",
        "answer": "a blue feather",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Lisa get last year?",
        "answer": "a blue feather",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "anything else Lisa got for her birthday besides a play mouse?",
        "answer": "a blue feather",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lisa get besides a mouse?",
        "answer": "a blue feather",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What is Lisa going to give Whiskers this year for his birthday with the bowl?",
        "answer": "a ball of yarn",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Lisa get for Whiskers' birthday this year?",
        "answer": "a ball of yarn",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What about Whiskers' birthday?",
        "answer": "a ball of yarn",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lisa get for her birthday?",
        "answer": "a ball of yarn",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What is Lisa going to give Whiskers this year for his birthday with the ball of yarn?",
        "answer": "a bowl",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else did Lisa get for Whiskers' birthday?",
        "answer": "a bowl",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else did Lisa get for her birthday besides a ball of yarn?",
        "answer": "a bowl",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lisa get for her birthday, besides a ball of yarn?",
        "answer": "a bowl",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What is on the bowl Lisa got Whiskers?",
        "answer": "a picture",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What's on the bowl?",
        "answer": "a picture",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "whats on the bowl of yarn for Whiskers?",
        "answer": "a picture",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is on the bowl?",
        "answer": "a picture",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What is the picture on the bowl of?",
        "answer": "a black cat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the picture on the bowl?",
        "answer": "a black cat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is on the bowl of yarn?",
        "answer": "a black cat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the picture on the bowl?",
        "answer": "a black cat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Does the cat on the bowl match Whiskers?",
        "answer": "definitely",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does it match Whiskers?",
        "answer": "definitely",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does the black cat match Whiskers?",
        "answer": "definitely",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the picture of the cat match Whiskers?",
        "answer": "definitely",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Where does Whiskers sleep?",
        "answer": "on her favorite chair",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where does this cat sleep?",
        "answer": "on her favorite chair",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where does this cat Whiskers sleep?",
        "answer": "on her favorite chair",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does the cat sleep?",
        "answer": "on her favorite chair",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3a0ex8zrn8ovm41x482h1zvlof2by0": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-1",
        "original_question": "When did Germany become a nation state?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-1",
        "answer": "In 1871"
      },
      {
        "turn_num": 2,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-2",
        "original_question": "What years were the German Revolution?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-2",
        "answer": "1918\u201319."
      },
      {
        "turn_num": 3,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-3",
        "original_question": "After that event, What was the Empire replaced by?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-3",
        "answer": "Weimar Republic."
      },
      {
        "turn_num": 4,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-4",
        "original_question": "When did the Nazi seizure of power happen?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-4",
        "answer": "In 1933"
      },
      {
        "turn_num": 5,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-5",
        "original_question": "Did this lead to the establishment of Nazi Germany?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-5",
        "answer": "Yes"
      },
      {
        "turn_num": 6,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-6",
        "original_question": "What was that built upon?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-6",
        "answer": "A dictatorship"
      },
      {
        "turn_num": 7,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-7",
        "original_question": "How many events did that cause?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-7",
        "answer": "Two."
      },
      {
        "turn_num": 8,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-8",
        "original_question": "What was the first one?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-8",
        "answer": "World War II"
      },
      {
        "turn_num": 9,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-9",
        "original_question": "And the second?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-9",
        "answer": "The Holocaust."
      },
      {
        "turn_num": 10,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-10",
        "original_question": "How many states were established after World War II?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-10",
        "answer": "Two"
      },
      {
        "turn_num": 11,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-11",
        "original_question": "What were they?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-11",
        "answer": "West Germany and East Germany."
      },
      {
        "turn_num": 12,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-12",
        "original_question": "Which one was democratic?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-12",
        "answer": "West Germany"
      },
      {
        "turn_num": 13,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-13",
        "original_question": "What year did communism end?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-13",
        "answer": "1989"
      },
      {
        "turn_num": 14,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-14",
        "original_question": "When was the country brought back together?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-14",
        "answer": "3 October 1990."
      },
      {
        "turn_num": 15,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-15",
        "original_question": "Since what time period have Germanic tribes been in parts of Germany?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-15",
        "answer": "Since classical antiquity."
      },
      {
        "turn_num": 16,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-16",
        "original_question": "Was there a recorded region before 100 AD?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-16",
        "answer": "Yes"
      },
      {
        "turn_num": 17,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-17",
        "original_question": "What was it called?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-17",
        "answer": "Germania"
      },
      {
        "turn_num": 18,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-18",
        "original_question": "Where did the tribes branch out to in the migration period?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-18",
        "answer": "Southward."
      },
      {
        "turn_num": 19,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-19",
        "original_question": "What is the official name of Germany?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-19",
        "answer": "Federal Republic of Germany,"
      },
      {
        "turn_num": 20,
        "turn_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-20",
        "original_question": "What is Germany's capital?",
        "original_question_id": "3a0ex8zrn8ovm41x482h1zvlof2by0-20",
        "answer": "Berlin"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "In what year did Germany become a nation state?",
        "answer": "In 1871",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Germany become a nation state?",
        "answer": "In 1871",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Germany become a nation state?",
        "answer": "In 1871",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Germany become a nation state?",
        "answer": "In 1871",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What were the years of the German Revolution?",
        "answer": "1918\u201319.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "ERROR",
        "answer": "1918\u201319.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What years was the German Revolution?",
        "answer": "1918\u201319.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What years were the German Revolution?",
        "answer": "1918\u201319.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What replaced the Empire after the German Revolution?",
        "answer": "Weimar Republic.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "After the German Revolution, what was the Empire replaced by?",
        "answer": "Weimar Republic.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "After the German Revolution, What was the German Empire replaced by?",
        "answer": "Weimar Republic.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "After the German Revolution, what was the Empire replaced by?",
        "answer": "Weimar Republic.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "When did Nazi seizure of power in Germany occur?",
        "answer": "In 1933",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did the Nazi seizure of power happen?",
        "answer": "In 1933",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did the Nazi seizure of power happen?",
        "answer": "In 1933",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Nazi seizure of power happen?",
        "answer": "In 1933",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did the seizure of power of the Nazis lead to the establishment of Nazi Germany?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did the Nazi seizure of power lead to the establishment of Nazi Germany?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the German Revolution lead to the establishment of Nazi Germany?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the Nazi seizure of power lead to the establishment of Nazi Germany?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What was Nazi Germany built upon?",
        "answer": "A dictatorship",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Nazi Germany built upon?",
        "answer": "A dictatorship",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the Weimar Republic built upon?",
        "answer": "A dictatorship",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the foundation of Nazi Germany?",
        "answer": "A dictatorship",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How many events did Nazi Germany lead to?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many events did the establishment of Nazi Germany cause?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many events did the German Revolution cause?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many events did the Nazi Germany dictatorship cause?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was the first event that Nazi Germany led to?",
        "answer": "World War II",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the first event caused by the establishment of Nazi Germany?",
        "answer": "World War II",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the first German Revolution?",
        "answer": "World War II",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the first event that led to the establishment of Nazi Germany?",
        "answer": "World War II",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What was the second event that Nazi Germany led to?",
        "answer": "The Holocaust.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the second event caused by the establishment of Nazi Germany?",
        "answer": "The Holocaust.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And the second World War II?",
        "answer": "The Holocaust.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the second World War?",
        "answer": "The Holocaust.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "In the wake of World War II, how many German states were founded?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many states were established after World War II?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many states were established after World War II?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many states were established after World War II?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What were the two German states established after World War II?",
        "answer": "West Germany and East Germany.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were the states established after World War II?",
        "answer": "West Germany and East Germany.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What were the German Republics?",
        "answer": "West Germany and East Germany.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What were the two states that were established after World War II?",
        "answer": "West Germany and East Germany.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Which post WWII German state was democratic?",
        "answer": "West Germany",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which one was democratic?",
        "answer": "West Germany",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which one was democratic?",
        "answer": "West Germany",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which German state was democratic?",
        "answer": "West Germany",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "When did communist rule in Germany come to an end?",
        "answer": "1989",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What year did communism end?",
        "answer": "1989",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What year did communism end in Germany?",
        "answer": "1989",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "In what year did communism end?",
        "answer": "1989",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "On what date were West and East Germany reunified?",
        "answer": "3 October 1990.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was Germany reunified?",
        "answer": "3 October 1990.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was Germany brought back together after the end of communism?",
        "answer": "3 October 1990.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was Germany brought back together?",
        "answer": "3 October 1990.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How long have there been Germanic tribes in Germany?",
        "answer": "Since classical antiquity.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Germanic tribes in Germany",
        "answer": "Since classical antiquity.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Since what time period have Germanic tribes been in parts of Germany?",
        "answer": "Since classical antiquity.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Since what time period have Germanic tribes been in parts of Germany?",
        "answer": "Since classical antiquity.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Were there any regions documented on German territory before 100 AD?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was there a recorded region before 100 AD?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was there a recorded region of Germanic tribes before 100 AD?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was there a recorded region in Germany before 100 AD?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What was pre-100 AD Germany called?",
        "answer": "Germania",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the region called before 100 AD?",
        "answer": "Germania",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the Germanic region called?",
        "answer": "Germania",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the name of the region that was recorded before 100 AD?",
        "answer": "Germania",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "During the migration period, where did Germanic tribes go?",
        "answer": "Southward.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did the Germanic tribes branch out to in the migration period?",
        "answer": "Southward.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the Germanic tribes branch out to in the migration period?",
        "answer": "Southward.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the Germanic tribes branch out to in the migration period?",
        "answer": "Southward.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What is Germany officially called?",
        "answer": "Federal Republic of Germany,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the official name of Germany?",
        "answer": "Federal Republic of Germany,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the official name of Germany?",
        "answer": "Federal Republic of Germany,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the official name of Germany?",
        "answer": "Federal Republic of Germany,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "What is the capital of Germany?",
        "answer": "Berlin",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the capital of Germany?",
        "answer": "Berlin",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Germany's capital?",
        "answer": "Berlin",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the capital of Germany?",
        "answer": "Berlin",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3befod78w6tb7ora6q4jzq284yi4md": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-1",
        "original_question": "What did the man throw?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-1",
        "answer": "His shoes."
      },
      {
        "turn_num": 2,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-2",
        "original_question": "What was his name?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-2",
        "answer": "Muntadhar al-Zaidi"
      },
      {
        "turn_num": 3,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-3",
        "original_question": "Was he arrested?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-3",
        "answer": "Yes."
      },
      {
        "turn_num": 4,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-4",
        "original_question": "Who did he throw his shoes at?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-4",
        "answer": "George W. Bush"
      },
      {
        "turn_num": 5,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-5",
        "original_question": "Is he still president?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-5",
        "answer": "No."
      },
      {
        "turn_num": 6,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-6",
        "original_question": "Has the al-Zaidi been released?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-6",
        "answer": "Yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-7",
        "original_question": "Where did he arrive?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-7",
        "answer": "Damascus"
      },
      {
        "turn_num": 8,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-8",
        "original_question": "Does he have any siblings?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-8",
        "answer": "Yes."
      },
      {
        "turn_num": 9,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-9",
        "original_question": "How many?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-9",
        "answer": "Unknown."
      },
      {
        "turn_num": 10,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-10",
        "original_question": "Did he need medical attention?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-10",
        "answer": "Yes."
      },
      {
        "turn_num": 11,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-11",
        "original_question": "Where did he get it?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-11",
        "answer": "Greece."
      },
      {
        "turn_num": 12,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-12",
        "original_question": "How did he get there?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-12",
        "answer": "In a private plane."
      },
      {
        "turn_num": 13,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-13",
        "original_question": "Why did he need it?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-13",
        "answer": "He was beaten."
      },
      {
        "turn_num": 14,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-14",
        "original_question": "With what?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-14",
        "answer": "With cables and pipes."
      },
      {
        "turn_num": 15,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-15",
        "original_question": "How long was his sentence?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-15",
        "answer": "One year."
      },
      {
        "turn_num": 16,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-16",
        "original_question": "Did he serve the whole year?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-16",
        "answer": "No."
      },
      {
        "turn_num": 17,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-17",
        "original_question": "Did he regret his choice?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-17",
        "answer": "No."
      },
      {
        "turn_num": 18,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-18",
        "original_question": "How many shoes did he throw?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-18",
        "answer": "Two."
      },
      {
        "turn_num": 19,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-19",
        "original_question": "Why was he released early?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-19",
        "answer": "For good behavior."
      },
      {
        "turn_num": 20,
        "turn_id": "3befod78w6tb7ora6q4jzq284yi4md-20",
        "original_question": "Does he consider himself a hero?",
        "original_question_id": "3befod78w6tb7ora6q4jzq284yi4md-20",
        "answer": "No."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What did Muntadhar al-Zaidi throw?",
        "answer": "His shoes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the man throw?",
        "answer": "His shoes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the man throw?",
        "answer": "His shoes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the man throw?",
        "answer": "His shoes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who threw a shoe at George Bush?",
        "answer": "Muntadhar al-Zaidi",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was his name?",
        "answer": "Muntadhar al-Zaidi",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the man's name?",
        "answer": "Muntadhar al-Zaidi",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the name of the man that threw the shoes?",
        "answer": "Muntadhar al-Zaidi",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Did Muntadhar al-Zaidi get arrested?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was he arrested?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Muntadhar al-Zaidi arrested?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Muntadhar al-Zaidi arrested?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who did Muntadhar al-Zaidi throw his shoe at?",
        "answer": "George W. Bush",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did the man throw his shoes at?",
        "answer": "George W. Bush",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who did Muntadhar al-Zaidi throw his shoes at?",
        "answer": "George W. Bush",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Muntadhar al-Zaidi throw his shoes at?",
        "answer": "George W. Bush",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Is George W. Bush still president?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is George W. Bush still president?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is George W. Bush still president?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is George W. Bush still president?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Has Muntadhar al-Zaidi been released from prison?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Has Muntadhar al-Zaidi been released?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Has Muntadhar al-Zaidi been released?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Has Muntadhar al-Zaidi been released?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Where did Muntadhar al-Zaidi arrive?",
        "answer": "Damascus",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did al-Zaidi arrive?",
        "answer": "Damascus",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Muntadhar al-Zaidi arrive?",
        "answer": "Damascus",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Muntadhar al-Zaidi arrive?",
        "answer": "Damascus",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Does Muntadhar al-Zaidi have any siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Muntadhar al-Zaidi have any siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Muntadhar al-Zaidi have any siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Muntadhar al-Zaidi have any siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "How many siblings does Muntadhar al-Zaidi have?",
        "answer": "Unknown.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many siblings does Muntadhar al-Zaidi have?",
        "answer": "Unknown.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many people did Muntadhar al-Zaidi have?",
        "answer": "Unknown.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many siblings does Muntadhar al-Zaidi have?",
        "answer": "Unknown.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did Muntadhar al-Zaidi require medical attention?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Muntadhar al-Zaidi need medical attention?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Muntadhar al-Zaidi need medical attention?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Muntadhar al-Zaidi need medical attention?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Where did Muntadhar al-Zaidi receive medical attention?",
        "answer": "Greece.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Muntadhar al-Zaidi receive medical attention?",
        "answer": "Greece.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Muntadhar al-Zaidi get medical attention?",
        "answer": "Greece.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Muntadhar al-Zaidi get medical attention?",
        "answer": "Greece.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "How did Muntadhar al-Zaidi get to Greece?",
        "answer": "In a private plane.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did Muntadhar al-Zaidi get to Greece?",
        "answer": "In a private plane.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Muntadhar al-Zaidi get to Greece?",
        "answer": "In a private plane.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did Muntadhar al-Zaidi get to Greece?",
        "answer": "In a private plane.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Why did Muntadhar al-Zaidi require medical care?",
        "answer": "He was beaten.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did Muntadhar al-Zaidi need medical attention?",
        "answer": "He was beaten.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did Muntadhar al-Zaidi need medical attention?",
        "answer": "He was beaten.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Muntadhar al-Zaidi need medical attention?",
        "answer": "He was beaten.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What was Muntadhar al-Zaidi beaten with?",
        "answer": "With cables and pipes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was he beaten with?",
        "answer": "With cables and pipes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "With what did Muntadhar al-Zaidi get beaten?",
        "answer": "With cables and pipes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Muntadhar al-Zaidi need to get medical attention with?",
        "answer": "With cables and pipes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How long was Muntadhar al-Zaidi's sentence?",
        "answer": "One year.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long was Muntadhar al-Zaidi's sentence?",
        "answer": "One year.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long was Muntadhar al-Zaidi's sentence?",
        "answer": "One year.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long was Muntadhar al-Zaidi sentenced to?",
        "answer": "One year.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Did Muntadhar al-Zaidi serve the whole year of his sentence?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Muntadhar al-Zaidi serve the whole year of his sentence?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Muntadhar al-Zaidi serve the whole year?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Muntadhar al-Zaidi serve the entire year?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Did Muntadhar al-Zaidi regret his actions?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Muntadhar al-Zaidi regret throwing his shoes at George W. Bush?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Muntadhar al-Zaidi regret his choice to serve the whole year?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Muntadhar al-Zaidi regret his choice?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "How many shoes did Muntadhar al-Zaidi throw at Bush?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many shoes did the man throw?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many shoes did Muntadhar al-Zaidi throw?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many shoes did Muntadhar al-Zaidi throw?",
        "answer": "Two.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Why was Muntadhar al-Zaidi released early?",
        "answer": "For good behavior.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why was Muntadhar al-Zaidi released early?",
        "answer": "For good behavior.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why was Muntadhar al-Zaidi released early?",
        "answer": "For good behavior.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why was Muntadhar al-Zaidi released early?",
        "answer": "For good behavior.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Does Muntadhar al-Zaidi see himself as a hero?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Muntadhar al-Zaidi consider himself a hero?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Muntadhar al-Zaidi consider himself a hero?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Muntadhar al-Zaidi consider himself a hero?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3bv8hq2zzw1okamzsb7tnxrm7kfa6p": {
    "number_of_turns": 12,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-1",
        "original_question": "Who was born in Mexico but not raised there?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-1",
        "answer": "Lupita Nyong'o"
      },
      {
        "turn_num": 2,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-2",
        "original_question": "What movie was her breakout role in?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-2",
        "answer": "\"12 Years A Slave."
      },
      {
        "turn_num": 3,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-3",
        "original_question": "Where was she raised?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-3",
        "answer": "Kenya"
      },
      {
        "turn_num": 4,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-4",
        "original_question": "What did her dad do?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-4",
        "answer": "He is a politician."
      },
      {
        "turn_num": 5,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-5",
        "original_question": "What else did he do?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-5",
        "answer": "He was a professor"
      },
      {
        "turn_num": 6,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-6",
        "original_question": "Of what?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-6",
        "answer": "Political science"
      },
      {
        "turn_num": 7,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-7",
        "original_question": "When did she return to Mexico?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-7",
        "answer": "At 16"
      },
      {
        "turn_num": 8,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-8",
        "original_question": "Why?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-8",
        "answer": "To learn Spanish"
      },
      {
        "turn_num": 9,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-9",
        "original_question": "What city did she grow up in?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-9",
        "answer": "Nairobi"
      },
      {
        "turn_num": 10,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-10",
        "original_question": "What was she nominated for?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-10",
        "answer": "An Oscar"
      },
      {
        "turn_num": 11,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-11",
        "original_question": "For what?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-11",
        "answer": "Best Actress in a Supporting Role."
      },
      {
        "turn_num": 12,
        "turn_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-12",
        "original_question": "What was her character's name?",
        "original_question_id": "3bv8hq2zzw1okamzsb7tnxrm7kfa6p-12",
        "answer": "Patsey"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who was born but not raised in Mexico?",
        "answer": "Lupita Nyong'o",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was born in Mexico but not raised there?",
        "answer": "Lupita Nyong'o",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was born in Mexico but not raised there?",
        "answer": "Lupita Nyong'o",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was born in Mexico but not raised in Mexico?",
        "answer": "Lupita Nyong'o",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What movie helped Lupita Nyong'o make a name for herself?",
        "answer": "\"12 Years A Slave.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Lupita Nyong'o breakout role movie",
        "answer": "\"12 Years A Slave.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What movie was Lupita Nyong'o breakout role in?",
        "answer": "\"12 Years A Slave.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What movie was Lupita Nyong'o's breakout role in?",
        "answer": "\"12 Years A Slave.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Where was Lupita Nyong'o raised?",
        "answer": "Kenya",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was Lupita Nyong'o raised?",
        "answer": "Kenya",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was Lupita Nyong'o raised?",
        "answer": "Kenya",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was Lupita Nyong'o raised?",
        "answer": "Kenya",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What did Lupita Nyong'o's dad do?",
        "answer": "He is a politician.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Lupita Nyong'o's dad do?",
        "answer": "He is a politician.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Lupita Nyong'o's dad do?",
        "answer": "He is a politician.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lupita Nyong'o's dad do?",
        "answer": "He is a politician.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "In addition to being a politician, what did Lupita Nyong'o's father do?",
        "answer": "He was a professor",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else did Lupita Nyong'o's dad do?",
        "answer": "He was a professor",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else did Lupita Nyong'o's dad do besides being a politician?",
        "answer": "He was a professor",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lupita Nyong'o's dad do besides being a politician?",
        "answer": "He was a professor",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What did Lupita Nyong'o's father teach?",
        "answer": "Political science",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Lupita Nyong'o's father teach?",
        "answer": "Political science",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Of what kind did Lupita Nyong'o's dad be?",
        "answer": "Political science",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Lupita Nyong'o's dad teach?",
        "answer": "Political science",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "When did Lupita Nyong'o return to Mexico?",
        "answer": "At 16",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Lupita Nyong'o return to Mexico?",
        "answer": "At 16",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Lupita Nyong'o return to Mexico?",
        "answer": "At 16",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Lupita Nyong'o return to Mexico?",
        "answer": "At 16",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Why did Lupita Nyong'o return to Mexico?",
        "answer": "To learn Spanish",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did Lupita Nyong'o return to Mexico?",
        "answer": "To learn Spanish",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did Lupita Nyong'o return to Mexico?",
        "answer": "To learn Spanish",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Lupita Nyong'o return to Mexico?",
        "answer": "To learn Spanish",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What city did Lupita Nyong'o grow up in?",
        "answer": "Nairobi",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What city did Lupita Nyong'o grow up in?",
        "answer": "Nairobi",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What city did Lupita Nyong'o grow up in?",
        "answer": "Nairobi",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What city did Lupita Nyong'o grow up in?",
        "answer": "Nairobi",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What awards has Lupita Nyong'o been nominated for?",
        "answer": "An Oscar",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Lupita Nyong'o nominated for?",
        "answer": "An Oscar",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Lupita Nyong'o nominated for?",
        "answer": "An Oscar",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Lupita Nyong'o nominated for?",
        "answer": "An Oscar",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Which Oscar was Lupita Nyong'o nominated for?",
        "answer": "Best Actress in a Supporting Role.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Lupita Nyong'o nominated for?",
        "answer": "Best Actress in a Supporting Role.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "For what was Lupita Nyong'o nominated for?",
        "answer": "Best Actress in a Supporting Role.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Lupita Nyong'o nominated for?",
        "answer": "Best Actress in a Supporting Role.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What was the name of Lupita Nyong'o's character in 12 Years a Slave?",
        "answer": "Patsey",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Lupita Nyong'o's character's name in the movie she was nominated for an Oscar?",
        "answer": "Patsey",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Lupita Nyong'o's character's name?",
        "answer": "Patsey",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Lupita Nyong'o's character called?",
        "answer": "Patsey",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3c2nj6jbkah7msxned0vjquaqh12nz": {
    "number_of_turns": 19,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-1",
        "original_question": "Who uses snakes to save lives?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-1",
        "answer": "Howie Choset"
      },
      {
        "turn_num": 2,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-2",
        "original_question": "How old is he?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-2",
        "answer": "37"
      },
      {
        "turn_num": 3,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-3",
        "original_question": "Does he teach?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-3",
        "answer": "Yes"
      },
      {
        "turn_num": 4,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-4",
        "original_question": "Where?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-4",
        "answer": "Carnegie Mellon"
      },
      {
        "turn_num": 5,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-5",
        "original_question": "What does he make?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-5",
        "answer": "robots"
      },
      {
        "turn_num": 6,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-6",
        "original_question": "Why?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-6",
        "answer": "To help victims"
      },
      {
        "turn_num": 7,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-7",
        "original_question": "What is Robotics Trends?/",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-7",
        "answer": "a company"
      },
      {
        "turn_num": 8,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-8",
        "original_question": "What does it do?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-8",
        "answer": "publishes an online industry magazine"
      },
      {
        "turn_num": 9,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-9",
        "original_question": "Where is it based?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-9",
        "answer": "Northboro"
      },
      {
        "turn_num": 10,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-10",
        "original_question": "Which state?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-10",
        "answer": "Massachusetts"
      },
      {
        "turn_num": 11,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-11",
        "original_question": "Does it have a president?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-11",
        "answer": "Yes"
      },
      {
        "turn_num": 12,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-12",
        "original_question": "Who is it?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-12",
        "answer": "Dan Kara"
      },
      {
        "turn_num": 13,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-13",
        "original_question": "What are the machines controlled by?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-13",
        "answer": "a joystick"
      },
      {
        "turn_num": 14,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-14",
        "original_question": "Do they move smoothly?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-14",
        "answer": "Yes"
      },
      {
        "turn_num": 15,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-15",
        "original_question": "What are servos?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-15",
        "answer": "small electric motors"
      },
      {
        "turn_num": 16,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-16",
        "original_question": "Who else uses them?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-16",
        "answer": "hobbyists"
      },
      {
        "turn_num": 17,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-17",
        "original_question": "What are the robots built from?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-17",
        "answer": "lightweight materials"
      },
      {
        "turn_num": 18,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-18",
        "original_question": "How big are they?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-18",
        "answer": "No"
      },
      {
        "turn_num": 19,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqh12nz-19",
        "original_question": "Do they know which way is up?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqh12nz-19",
        "answer": "Yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who draws inspiration from snakes to save lives?",
        "answer": "Howie Choset",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who uses snakes to save lives?",
        "answer": "Howie Choset",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who uses snakes to save lives?",
        "answer": "Howie Choset",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who uses snakes to save lives?",
        "answer": "Howie Choset",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "How old is Howie Choset?",
        "answer": "37",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How old is Howie Choset?",
        "answer": "37",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How old is Howie Choset?",
        "answer": "37",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How old is Howie Choset?",
        "answer": "37",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Does Howie Choset teach?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Howie Choset teach?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Howie Choset teach?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Howie Choset teach?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Where is Howie Choset a professor at?",
        "answer": "Carnegie Mellon",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where does Howie Choset teach?",
        "answer": "Carnegie Mellon",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where does Howie Choset teach?",
        "answer": "Carnegie Mellon",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does Howie Choset teach?",
        "answer": "Carnegie Mellon",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What does Howie Choset make?",
        "answer": "robots",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Howie Choset make?",
        "answer": "robots",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Howie Choset make?",
        "answer": "robots",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Howie Choset's invention?",
        "answer": "robots",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Why is Howie Choset making his snakelike robots?",
        "answer": "To help victims",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why does Howie Choset make robots?",
        "answer": "To help victims",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why does Howie Choset use snakes to save lives?",
        "answer": "To help victims",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why are robots made by Howie Choset?",
        "answer": "To help victims",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is the organization Robotics Trends?",
        "answer": "a company",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Robotics Trends?",
        "answer": "a company",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Robotics Trends?/ To help victims?",
        "answer": "a company",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the term Robotics Trends?",
        "answer": "a company",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What does Robotics Trends do?",
        "answer": "publishes an online industry magazine",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Robotics Trends do?",
        "answer": "publishes an online industry magazine",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Robotics Trends do?",
        "answer": "publishes an online industry magazine",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Robotics Trends do?",
        "answer": "publishes an online industry magazine",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Where is Robotics Trends based?",
        "answer": "Northboro",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where is Robotics Trends based?",
        "answer": "Northboro",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where is Robotics Trends based?",
        "answer": "Northboro",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where is Robotics Trends based?",
        "answer": "Northboro",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "In what state are the headquarters of Robotics Trends located?",
        "answer": "Massachusetts",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which state is Robotics Trends based in?",
        "answer": "Massachusetts",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which state is the headquarters of Robotics Trends?",
        "answer": "Massachusetts",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What state is Northboro in?",
        "answer": "Massachusetts",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Does Robotics Trends have a president?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does Robotics Trends have a president?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does the company Robotics Trends have a president?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Robotics Trends have a president?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Who is the president of Robotics Trends?",
        "answer": "Dan Kara",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is the president of Robotics Trends?",
        "answer": "Dan Kara",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is the company that publishes an online industry magazine?",
        "answer": "Dan Kara",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the president of Robotics Trends?",
        "answer": "Dan Kara",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What controls the machines of Robotics Trends?",
        "answer": "a joystick",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What are the machines controlled by?",
        "answer": "a joystick",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What are the robots Dan Kara controls?",
        "answer": "a joystick",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the robot controlled by?",
        "answer": "a joystick",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Does Robotics Trends have smooth running machines?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Do they move smoothly?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Do the robots move smoothly?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do robots move smoothly?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What does servo mean?",
        "answer": "small electric motors",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What are servos?",
        "answer": "small electric motors",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What are servos?",
        "answer": "small electric motors",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are servos?",
        "answer": "small electric motors",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Who uses servos, besides the industry?",
        "answer": "hobbyists",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who else uses servos?",
        "answer": "hobbyists",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who else uses servos besides Howie Choset?",
        "answer": "hobbyists",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who uses servos, in addition to Dan Kara?",
        "answer": "hobbyists",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What are the robots made out of?",
        "answer": "lightweight materials",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What are the robots built from?",
        "answer": "lightweight materials",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What are the robots built from?",
        "answer": "lightweight materials",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the robots made from?",
        "answer": "lightweight materials",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "How big are the robots?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How big are the robots?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How big are the robots?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How big are the robots?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Can the robots sense which way is up?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Do robots know which way is up?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Do robots know which way is up?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do robots know which way is up?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3c2nj6jbkah7msxned0vjquaqmkn2d": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-1",
        "original_question": "What type of leader is Mohammad Najafi?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-1",
        "answer": "an industry leader"
      },
      {
        "turn_num": 2,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-2",
        "original_question": "What does he supply?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-2",
        "answer": "auto parts"
      },
      {
        "turn_num": 3,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-3",
        "original_question": "What is one kind of part he supplies?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-3",
        "answer": "springs"
      },
      {
        "turn_num": 4,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-4",
        "original_question": "How many of those each year?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-4",
        "answer": "three million"
      },
      {
        "turn_num": 5,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-5",
        "original_question": "What is another part he supplies?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-5",
        "answer": "break-pads"
      },
      {
        "turn_num": 6,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-6",
        "original_question": "How many of those?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-6",
        "answer": "one million"
      },
      {
        "turn_num": 7,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-7",
        "original_question": "Who does he supply to?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-7",
        "answer": "Iranian auto makers"
      },
      {
        "turn_num": 8,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-8",
        "original_question": "How long ago did he start this?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-8",
        "answer": "A little over 26 years ago"
      },
      {
        "turn_num": 9,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-9",
        "original_question": "Is he successful?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-9",
        "answer": "yes"
      },
      {
        "turn_num": 10,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-10",
        "original_question": "What has that earned him?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-10",
        "answer": "seats"
      },
      {
        "turn_num": 11,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-11",
        "original_question": "To where?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-11",
        "answer": "Chamber of Commerce"
      },
      {
        "turn_num": 12,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-12",
        "original_question": "Where else?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-12",
        "answer": "the board of directors of the Iranian Auto Parts Manufacturers Association"
      },
      {
        "turn_num": 13,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-13",
        "original_question": "What had that done for him?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-13",
        "answer": "made him a leading voice"
      },
      {
        "turn_num": 14,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-14",
        "original_question": "In what?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-14",
        "answer": "Iran's efforts to re-energize its struggling economy"
      },
      {
        "turn_num": 15,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-15",
        "original_question": "What has he been paying attention to lately?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-15",
        "answer": "recent negotiations"
      },
      {
        "turn_num": 16,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-16",
        "original_question": "With whom?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-16",
        "answer": "world powers"
      },
      {
        "turn_num": 17,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-17",
        "original_question": "On what?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-17",
        "answer": "Iran's nuclear program"
      },
      {
        "turn_num": 18,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-18",
        "original_question": "Has he shared his thoughts with anyone about that?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-18",
        "answer": "yes"
      },
      {
        "turn_num": 19,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-19",
        "original_question": "With whom?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-19",
        "answer": "Reza Sayah"
      },
      {
        "turn_num": 20,
        "turn_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-20",
        "original_question": "What news outlet is he with?",
        "original_question_id": "3c2nj6jbkah7msxned0vjquaqmkn2d-20",
        "answer": "CNN"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What does Mohammad Reza Najafi lead?",
        "answer": "an industry leader",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What type of leader is Mohammad Najafi?",
        "answer": "an industry leader",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What type of leader is Mohammad Najafi?",
        "answer": "an industry leader",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Mohammad Najafi's role as leader?",
        "answer": "an industry leader",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What is Mohammad Reza Najafi a supplier of?",
        "answer": "auto parts",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Mohammad Najafi supply?",
        "answer": "auto parts",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Mohammad Najafi supply?",
        "answer": "auto parts",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Mohammad Najafi supply?",
        "answer": "auto parts",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What's one kind of auto part that Mohammad Reza Najafi supplies?",
        "answer": "springs",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "one kind of part Mohammad Najafi supplies",
        "answer": "springs",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is one kind of part Mohammad Najafi supplies?",
        "answer": "springs",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is one type of part that Mohammad Najafi supplies?",
        "answer": "springs",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "How many springs does Mohammad Reza Najafi supply each year?",
        "answer": "three million",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many springs does Mohammad Najafi supply each year?",
        "answer": "three million",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many springs does Mohammad Najafi supply each year?",
        "answer": "three million",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many springs does Mohammad Najafi supply each year?",
        "answer": "three million",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What's one kind of auto part, besides springs that Mohammad Reza Najafi supplies?",
        "answer": "break-pads",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What other parts does Mohammad Najafi supply?",
        "answer": "break-pads",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is another part Mohammad Najafi supplies besides springs?",
        "answer": "break-pads",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is another part that Mohammad Najafi supplies?",
        "answer": "break-pads",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "How many break-pads does Mohammad Reza Najafi supply each year?",
        "answer": "one million",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many break-pads does Mohammad Najafi supply each year?",
        "answer": "one million",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many of those do Mohammad Najafi supply?",
        "answer": "one million",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many of Mohammad Najafi's parts does he supply?",
        "answer": "one million",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Who purchases Mohammad Reza Najafi supply of auto parts?",
        "answer": "Iranian auto makers",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who does Mohammad Najafi supply to?",
        "answer": "Iranian auto makers",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who does Mohammad Najafi supply to?",
        "answer": "Iranian auto makers",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who does Mohammad Najafi supply to?",
        "answer": "Iranian auto makers",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How long has it been since Mohammad Reza Najafi began manufacturing auto parts?",
        "answer": "A little over 26 years ago",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long ago did Mohammad Najafi start his business?",
        "answer": "A little over 26 years ago",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long ago did Mohammad Najafi start the Iranian auto industry?",
        "answer": "A little over 26 years ago",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long ago did Mohammad Najafi start his business?",
        "answer": "A little over 26 years ago",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Is Mohammad Reza Najafi successful at what he does?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is Mohammad Najafi successful?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is Mohammad Najafi successful in his auto industry?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is Mohammad Najafi successful?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What has Mohammad Reza Najafi's success earned him?",
        "answer": "seats",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What has Mohammad Najafi earned?",
        "answer": "seats",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What has Mohammad Najafi's success earned him?",
        "answer": "seats",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What has Mohammad Najafi earned?",
        "answer": "seats",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Where does Mohammad Reza Najafi have a seat?",
        "answer": "Chamber of Commerce",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What has that earned him?",
        "answer": "Chamber of Commerce",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "To where did Mohammad Najafi start his business?",
        "answer": "Chamber of Commerce",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does Mohammad Najafi supply parts to?",
        "answer": "Chamber of Commerce",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Where, in addition to Iran's Chamber of Commerce does Mohammad Reza Najafi have a seat?",
        "answer": "the board of directors of the Iranian Auto Parts Manufacturers Association",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where else has Mohammad Najafi earned seats?",
        "answer": "the board of directors of the Iranian Auto Parts Manufacturers Association",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where else does Mohammad Najafi supply parts besides the Chamber of Commerce?",
        "answer": "the board of directors of the Iranian Auto Parts Manufacturers Association",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where else does Mohammad Najafi supply to the Chamber of Commerce?",
        "answer": "the board of directors of the Iranian Auto Parts Manufacturers Association",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What has Mohammad Reza Najafi's seats on important boards earned him?",
        "answer": "made him a leading voice",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What had that done for him?",
        "answer": "made him a leading voice",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What had the board of directors of the Iranian Auto Parts Manufacturers Association done for Mohammad Najafi?",
        "answer": "made him a leading voice",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Mohammad Najafi's board of directors do for him?",
        "answer": "made him a leading voice",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is Mohammad Reza Najafi a leading voice for?",
        "answer": "Iran's efforts to re-energize its struggling economy",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In what?",
        "answer": "Iran's efforts to re-energize its struggling economy",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In what?",
        "answer": "Iran's efforts to re-energize its struggling economy",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Mohammad Najafi a leading voice in?",
        "answer": "Iran's efforts to re-energize its struggling economy",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What has Mohammad Reza Najafi recently been paying close attention to?",
        "answer": "recent negotiations",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What has Mohammad Najafi been paying attention to lately?",
        "answer": "recent negotiations",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What has Mohammad Najafi been paying attention to lately?",
        "answer": "recent negotiations",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What has Mohammad Najafi been paying attention to lately?",
        "answer": "recent negotiations",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Who has Iran been recently negotiating with?",
        "answer": "world powers",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "recent negotiations",
        "answer": "world powers",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "With whom has Mohammad Najafi been paying attention to lately?",
        "answer": "world powers",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who has Mohammad Najafi been focusing on lately?",
        "answer": "world powers",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What is the subject of Iran's recent negotiations with world powers?",
        "answer": "Iran's nuclear program",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "recent negotiations",
        "answer": "Iran's nuclear program",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "On what?",
        "answer": "Iran's nuclear program",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Mohammad Najafi's latest focus?",
        "answer": "Iran's nuclear program",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Has Mohammad Reza Najafi shared his thoughts with anyone about Iran's nuclear negotiations?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Has Mohammad Najafi shared his thoughts about Iran's nuclear program with anyone?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Has Mohammad Najafi shared his thoughts with anyone about Iran's nuclear program?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Has Mohammad Najafi shared his thoughts with anyone regarding Iran's nuclear program?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What newscaster did Mohammad Reza Najafi speak with?",
        "answer": "Reza Sayah",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Has Mohammad Najafi shared his thoughts with anyone about Iran's nuclear program?",
        "answer": "Reza Sayah",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "With whom has Mohammad Najafi shared his thoughts about Iran's nuclear program?",
        "answer": "Reza Sayah",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who has Mohammad Najafi shared his thoughts with regarding Iran's nuclear program?",
        "answer": "Reza Sayah",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "What news outlet does Reza Sayah work for?",
        "answer": "CNN",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Reza Sayah",
        "answer": "CNN",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What news outlet is Mohammad Najafi with?",
        "answer": "CNN",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What news outlet is Reza Sayah with?",
        "answer": "CNN",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3ctoc39k37qip3385rpymau1s9p7jf": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-1",
        "original_question": "what is the highest location in Lawrence?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-1",
        "answer": "Mount Oread"
      },
      {
        "turn_num": 2,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-2",
        "original_question": "when was kansas made the 34th state?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-2",
        "answer": "1861"
      },
      {
        "turn_num": 3,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-3",
        "original_question": "is KU a private university?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-3",
        "answer": "No"
      },
      {
        "turn_num": 4,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-4",
        "original_question": "when was it founded?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-4",
        "answer": "1865"
      },
      {
        "turn_num": 5,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-5",
        "original_question": "how many members are in the Association of American Universities?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-5",
        "answer": "62"
      },
      {
        "turn_num": 6,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-6",
        "original_question": "does that inlcude KU?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-6",
        "answer": "Yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-7",
        "original_question": "how many places do they have branch campuses?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-7",
        "answer": "five"
      },
      {
        "turn_num": 8,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-8",
        "original_question": "where is the main campus?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-8",
        "answer": "Lawrence"
      },
      {
        "turn_num": 9,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-9",
        "original_question": "who granted their charter?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-9",
        "answer": "Kansas State Legislature"
      },
      {
        "turn_num": 10,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-10",
        "original_question": "when?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-10",
        "answer": "in 1864"
      },
      {
        "turn_num": 11,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-11",
        "original_question": "where is the Edwards Campus?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-11",
        "answer": "in Overland Park"
      },
      {
        "turn_num": 12,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-12",
        "original_question": "what sites are in Parsons?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-12",
        "answer": "research sites"
      },
      {
        "turn_num": 13,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-13",
        "original_question": "did Kansas have its own civil war?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-13",
        "answer": "Yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-14",
        "original_question": "what year did that happen?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-14",
        "answer": "during the 1850s"
      },
      {
        "turn_num": 15,
        "turn_id": "3ctoc39k37qip3385rpymau1s9p7jf-15",
        "original_question": "what is it referred to as?",
        "original_question_id": "3ctoc39k37qip3385rpymau1s9p7jf-15",
        "answer": "Bleeding Kansas"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What's the highest spot in Lawrence?",
        "answer": "Mount Oread",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is the highest location in Lawrence?",
        "answer": "Mount Oread",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is the highest location in Lawrence?",
        "answer": "Mount Oread",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "what is the highest location in Lawrence?",
        "answer": "Mount Oread",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "In what year did Kansas become the 34th state?",
        "answer": "1861",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was kansas made the 34th state?",
        "answer": "1861",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was kansas made the 34th state?",
        "answer": "1861",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was Kansas made the 34th state?",
        "answer": "1861",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Is the University of Kansas private?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is KU a private university?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is KU a private university?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is KU a private university?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "When was the University of Kansas founded?",
        "answer": "1865",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was KU founded?",
        "answer": "1865",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was KU founded?",
        "answer": "1865",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was KU founded?",
        "answer": "1865",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "How many members does the Association of American Universities have?",
        "answer": "62",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many members are in the Association of American Universities?",
        "answer": "62",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many members are in the Association of American Universities?",
        "answer": "62",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many members does the Association of American Universities have?",
        "answer": "62",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Is the University of Kansas a member of the Association of American Universities?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "does KU belong to the Association of American Universities?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "does the Association of American Universities include KU?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the Association of American Universities include KU?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How many branches does the University of Kansas have?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many branch campuses does the Association of American Universities have?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many places do KU have branch campuses?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many branches of KU are there?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What's the location of the University of Kansas's main campus?",
        "answer": "Lawrence",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where is the main campus of KU?",
        "answer": "Lawrence",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is the main campus of KU?",
        "answer": "Lawrence",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the location of KU's main campus?",
        "answer": "Lawrence",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who granted the charter of the University of Kansas?",
        "answer": "Kansas State Legislature",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who granted the charter to KU?",
        "answer": "Kansas State Legislature",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who granted the Association of American Universities' charter?",
        "answer": "Kansas State Legislature",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who gave KU its charter?",
        "answer": "Kansas State Legislature",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "When did the University of Kansas get its charter from the Kansas State Legislature?",
        "answer": "in 1864",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was KU granted its charter?",
        "answer": "in 1864",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was the kansas state legislature granted the KU charter?",
        "answer": "in 1864",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Kansas State Legislature grant the university's charter?",
        "answer": "in 1864",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What's the location of the Edwards Campus?",
        "answer": "in Overland Park",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where is the Edwards Campus located?",
        "answer": "in Overland Park",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is the Edwards Campus in Lawrence?",
        "answer": "in Overland Park",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the location of the Edwards Campus?",
        "answer": "in Overland Park",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What does the University of Kansas have in Parsons?",
        "answer": "research sites",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "sites in Parsons",
        "answer": "research sites",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what sites are in Parsons?",
        "answer": "research sites",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the locations of the Parsons campus?",
        "answer": "research sites",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Was there ever a civil war in Kansas?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Kansas have its own civil war?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Kansas have its own civil war?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Kansas have its own civil war?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What was the year of the Kansas Civil War?",
        "answer": "during the 1850s",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Kansas Civil War",
        "answer": "during the 1850s",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what year did the kansas state civil war happen?",
        "answer": "during the 1850s",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "In what year did the civil war occur at KU?",
        "answer": "during the 1850s",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What is the name of the Kansas Civil War?",
        "answer": "Bleeding Kansas",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is it referred to as?",
        "answer": "Bleeding Kansas",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is the kansas state capitol referred to as?",
        "answer": "Bleeding Kansas",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the Civil War that took place in Kansas?",
        "answer": "Bleeding Kansas",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3di28l7yxaew312e2axyokqwkq31en": {
    "number_of_turns": 14,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-1",
        "original_question": "Who was groaning?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-1",
        "answer": "Bathurst"
      },
      {
        "turn_num": 2,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-2",
        "original_question": "Why was he upset?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-2",
        "answer": "He wanted to be killed"
      },
      {
        "turn_num": 3,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-3",
        "original_question": "Who did he think was still alive?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-3",
        "answer": "Isobel"
      },
      {
        "turn_num": 4,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-4",
        "original_question": "Where was she?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-4",
        "answer": "In the boat"
      },
      {
        "turn_num": 5,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-5",
        "original_question": "How many women were on it?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-5",
        "answer": "Three"
      },
      {
        "turn_num": 6,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-6",
        "original_question": "Were they together?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-6",
        "answer": "Yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-7",
        "original_question": "What had they done when they were threatened?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-7",
        "answer": "Screamed"
      },
      {
        "turn_num": 8,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-8",
        "original_question": "Who posed a danger to those people?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-8",
        "answer": "The natives"
      },
      {
        "turn_num": 9,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-9",
        "original_question": "What were they about to do?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-9",
        "answer": "Murder those who were with them"
      },
      {
        "turn_num": 10,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-10",
        "original_question": "With what?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-10",
        "answer": "Muskets"
      },
      {
        "turn_num": 11,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-11",
        "original_question": "Were they pointed at her?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-11",
        "answer": "No"
      },
      {
        "turn_num": 12,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-12",
        "original_question": "Why did he think they weren't?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-12",
        "answer": "She would not have screamed"
      },
      {
        "turn_num": 13,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-13",
        "original_question": "In light of what happened, how was he feeling?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-13",
        "answer": "confident"
      },
      {
        "turn_num": 14,
        "turn_id": "3di28l7yxaew312e2axyokqwkq31en-14",
        "original_question": "What time of day was it?",
        "original_question_id": "3di28l7yxaew312e2axyokqwkq31en-14",
        "answer": "About an hour before the comes up"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who groaned?",
        "answer": "Bathurst",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was groaning?",
        "answer": "Bathurst",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was groaning?",
        "answer": "Bathurst",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was groaning?",
        "answer": "Bathurst",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was the matter with Bathurst?",
        "answer": "He wanted to be killed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why was Bathurst upset?",
        "answer": "He wanted to be killed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why was Bathurst upset?",
        "answer": "He wanted to be killed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Bathurst upset about?",
        "answer": "He wanted to be killed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who did Bathurst believe to still be alive?",
        "answer": "Isobel",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did Bathurst think was still alive?",
        "answer": "Isobel",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who did Bathurst think was still alive?",
        "answer": "Isobel",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Bathurst think was still alive?",
        "answer": "Isobel",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Where was Isobel Hannay?",
        "answer": "In the boat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was Isobel?",
        "answer": "In the boat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was Isobel?",
        "answer": "In the boat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was Isobel?",
        "answer": "In the boat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "How many women did the boat have in it?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many women were on the boat?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many women were on the boat that Bathurst was on?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many women were on the boat?",
        "answer": "Three",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did the three women site together in the boat?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Were the women together on the boat?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Were Isobel and Bathurst together?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Were Isobel and Bathurst together?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did the women do when threatened?",
        "answer": "Screamed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What had they done when they were threatened?",
        "answer": "Screamed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What had Bathurst and Isobel done when they were threatened?",
        "answer": "Screamed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What had Isobel and Bathurst done when they were threatened?",
        "answer": "Screamed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Who posed a threat to the women?",
        "answer": "The natives",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who posed a danger to those people?",
        "answer": "The natives",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who posed a danger to the people on the boat that Bathurst was on?",
        "answer": "The natives",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was a danger to the women?",
        "answer": "The natives",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What were the natives about to do?",
        "answer": "Murder those who were with them",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were the natives about to do?",
        "answer": "Murder those who were with them",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What were the natives about to do when they were threatened?",
        "answer": "Murder those who were with them",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What were Bathurst and Isobel about to do?",
        "answer": "Murder those who were with them",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What were the natives going to murder with?",
        "answer": "Muskets",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "With what were the natives about to murder those who were with them?",
        "answer": "Muskets",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "With what did the natives do?",
        "answer": "Muskets",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What were Bathurst and Isobel about to do?",
        "answer": "Muskets",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Did the natives point their muskets at Isobel?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Were the muskets pointed at her?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Were the muskets pointed at Isobel?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Were the muskets directed at Isobel?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Why did Bathurst think Isobel didn't have a musket on her?",
        "answer": "She would not have screamed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did he think they weren't?",
        "answer": "She would not have screamed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did he think the Musketeers were not?",
        "answer": "She would not have screamed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Bathurst think the muskets were not pointed at Isobel?",
        "answer": "She would not have screamed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "How did Bathurst feel about the situation?",
        "answer": "confident",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In light of what happened, how was he feeling?",
        "answer": "confident",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In light of what happened, how was he feeling?",
        "answer": "confident",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Bathurst's mood in light of what had happened?",
        "answer": "confident",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "When in the day was it?",
        "answer": "About an hour before the comes up",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What time of day was it?",
        "answer": "About an hour before the comes up",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What time of day was it when the muskets were shot?",
        "answer": "About an hour before the comes up",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "At what time of day was Bathurst feeling confident?",
        "answer": "About an hour before the comes up",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3e47sobeyqws69eyeqc9qv7fghdcib": {
    "number_of_turns": 12,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-1",
        "original_question": "who did not want to let down fans ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-1",
        "answer": "Isner"
      },
      {
        "turn_num": 2,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-2",
        "original_question": "was it his 2nd match since Wimbledon ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-2",
        "answer": "no"
      },
      {
        "turn_num": 3,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-3",
        "original_question": "what is the correct answer ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-3",
        "answer": "first tournamentck"
      },
      {
        "turn_num": 4,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-4",
        "original_question": "who has a house hold name ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-4",
        "answer": "Isner"
      },
      {
        "turn_num": 5,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-5",
        "original_question": "what sport is this about ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-5",
        "answer": "competitive tennis"
      },
      {
        "turn_num": 6,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-6",
        "original_question": "who was firing 33 aces ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-6",
        "answer": "Isner"
      },
      {
        "turn_num": 7,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-7",
        "original_question": "how long did it take ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-7",
        "answer": "two-and-a-half hours"
      },
      {
        "turn_num": 8,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-8",
        "original_question": "how many people were there ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-8",
        "answer": "5,000"
      },
      {
        "turn_num": 9,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-9",
        "original_question": "he had a lot of what there ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-9",
        "answer": "friends and family"
      },
      {
        "turn_num": 10,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-10",
        "original_question": "iiiiiiiiiiis that why he did not want to go down ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-10",
        "answer": "yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-11",
        "original_question": "the Tennis games were held where ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-11",
        "answer": "Atlanta"
      },
      {
        "turn_num": 12,
        "turn_id": "3e47sobeyqws69eyeqc9qv7fghdcib-12",
        "original_question": "whos from Luxembourg ?",
        "original_question_id": "3e47sobeyqws69eyeqc9qv7fghdcib-12",
        "answer": "Gilles Muller"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who wanted to make sure not to let fans down?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who did not want to let down fans ?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who did not want to let down fans?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did not want to let down fans?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Was John Isner playing his second match since Wimbledon?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Isner",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was it Isner's 2nd match since Wimbledon?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was it Roger Federer's second match since Wimbledon?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Which of John Isner's matches does the article discuss?",
        "answer": "first tournamentck",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did not want to let down fans?",
        "answer": "first tournamentck",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is the correct answer to the question about Isner's 2nd match since Wimbledon?",
        "answer": "first tournamentck",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the correct answer to the question?",
        "answer": "first tournamentck",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What player is a household name?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who has a house hold name?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who has a house hold name?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the house hold of?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What sport does John Isner play?",
        "answer": "competitive tennis",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who has a house hold name ?",
        "answer": "competitive tennis",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what sport is the Wimbledon tournament about?",
        "answer": "competitive tennis",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What sport is the article about?",
        "answer": "competitive tennis",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Which player fired 33 aces?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who was firing 33 aces?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who was firing 33 aces in the first tournament at the ATP Championships?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was firing 33 aces?",
        "answer": "Isner",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "How long did John Isner's match last?",
        "answer": "two-and-a-half hours",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how long did it take?",
        "answer": "two-and-a-half hours",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how long did it take for Isner to fire 33 aces in his first match?",
        "answer": "two-and-a-half hours",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long did it take for Isner to fire 33 aces?",
        "answer": "two-and-a-half hours",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How many people attended John Isner's winning match?",
        "answer": "5,000",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many people were there?",
        "answer": "5,000",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many people were there at the first tournament for Isner?",
        "answer": "5,000",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many people were there at the tournament?",
        "answer": "5,000",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who was in the crowd for John Isner at his match?",
        "answer": "friends and family",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "he had a lot of what there?",
        "answer": "friends and family",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Isner had a lot of what there?",
        "answer": "friends and family",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the main item that was present at the tournament?",
        "answer": "friends and family",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did seeing friends and family make John Isner want to win?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is that why he did not want to go down?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Isner did not want to go down to the tennis court?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why didn't Steven Isner want to go down?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What was the location of the tennis games?",
        "answer": "Atlanta",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "the Tennis games were held where?",
        "answer": "Atlanta",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "the tennis games were held where?",
        "answer": "Atlanta",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where were the tennis games held?",
        "answer": "Atlanta",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Which player is from Luxembourg?",
        "answer": "Gilles Muller",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who is from Luxembourg?",
        "answer": "Gilles Muller",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "whos from Luxembourg?",
        "answer": "Gilles Muller",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is from Luxembourg?",
        "answer": "Gilles Muller",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3ejjqnku9r5wggsxq5kjfe5mfyrrhe": {
    "number_of_turns": 16,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-1",
        "original_question": "Who was the princess?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-1",
        "answer": "Janet"
      },
      {
        "turn_num": 2,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-2",
        "original_question": "Was she unattractive?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-2",
        "answer": "no"
      },
      {
        "turn_num": 3,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-3",
        "original_question": "What did she want to do?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-3",
        "answer": "wed a prince named Harold"
      },
      {
        "turn_num": 4,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-4",
        "original_question": "Did Harold have siblings?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-4",
        "answer": "three sisters"
      },
      {
        "turn_num": 5,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-5",
        "original_question": "What are their names?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-5",
        "answer": "Tammy, Jenny, and Clarice"
      },
      {
        "turn_num": 6,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-6",
        "original_question": "Did they like the princess?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-6",
        "answer": "Tammy and Jenny liked Janet, but Clarice didn't"
      },
      {
        "turn_num": 7,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-7",
        "original_question": "Was there any violence?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-7",
        "answer": "yes"
      },
      {
        "turn_num": 8,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-8",
        "original_question": "what was the act?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-8",
        "answer": "Clarice threw a shoe at Janet"
      },
      {
        "turn_num": 9,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-9",
        "original_question": "What was the consequence for that?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-9",
        "answer": "the shoe hit Harold instead"
      },
      {
        "turn_num": 10,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-10",
        "original_question": "was there a payback for that incident?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-10",
        "answer": "yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-11",
        "original_question": "What was it?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-11",
        "answer": "She cut off her hair one night"
      },
      {
        "turn_num": 12,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-12",
        "original_question": "Did the couple ever tie the knot?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-12",
        "answer": "yes"
      },
      {
        "turn_num": 13,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-13",
        "original_question": "Where did the couple reside afterward?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-13",
        "answer": "the castle"
      },
      {
        "turn_num": 14,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-14",
        "original_question": "Whose hair was cut off?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-14",
        "answer": "Clarice"
      },
      {
        "turn_num": 15,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-15",
        "original_question": "And who did it?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-15",
        "answer": "Janet"
      },
      {
        "turn_num": 16,
        "turn_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-16",
        "original_question": "Why was she upset about her hair being cut?",
        "original_question_id": "3ejjqnku9r5wggsxq5kjfe5mfyrrhe-16",
        "answer": "she wanted her hair to be long"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What was the name of the princess?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was the princess?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was the princess?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the princess?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Was the princess ugly?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Janet unattractive?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Janet unattractive?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Janet unattractive?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What did Janet want to do?",
        "answer": "wed a prince named Harold",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the princess want to do?",
        "answer": "wed a prince named Harold",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Janet want to do?",
        "answer": "wed a prince named Harold",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Janet want to do?",
        "answer": "wed a prince named Harold",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Did Harold have sisters?",
        "answer": "three sisters",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Harold have siblings?",
        "answer": "three sisters",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Harold have siblings?",
        "answer": "three sisters",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Harold have siblings?",
        "answer": "three sisters",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who are Harold's three sisters?",
        "answer": "Tammy, Jenny, and Clarice",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What are the names of Harold's sisters?",
        "answer": "Tammy, Jenny, and Clarice",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What are the three sisters' names?",
        "answer": "Tammy, Jenny, and Clarice",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the names of the three sisters?",
        "answer": "Tammy, Jenny, and Clarice",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did Harold's sisters like the princess?",
        "answer": "Tammy and Jenny liked Janet, but Clarice didn't",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they like the princess?",
        "answer": "Tammy and Jenny liked Janet, but Clarice didn't",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Tammy, Jenny, and Clarice like the princess?",
        "answer": "Tammy and Jenny liked Janet, but Clarice didn't",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Tammy, Jenny, and Clarice like Janet?",
        "answer": "Tammy and Jenny liked Janet, but Clarice didn't",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Were there any altercations between Janet and Harold's sisters?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was there any violence?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was there any violence between Tammy, Jenny, and Clarice?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was there any violence against Janet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What altercation occurred?",
        "answer": "Clarice threw a shoe at Janet",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what was the act?",
        "answer": "Clarice threw a shoe at Janet",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was the act of violence by Tammy, Jenny, and Clarice?",
        "answer": "Clarice threw a shoe at Janet",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the act of violence?",
        "answer": "Clarice threw a shoe at Janet",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What happened with Clarice threw her shoe?",
        "answer": "the shoe hit Harold instead",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "consequence of Clarice throwing a shoe at Janet",
        "answer": "the shoe hit Harold instead",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the consequence for Clarice throwing a shoe at Janet?",
        "answer": "the shoe hit Harold instead",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the consequence of Clarice throwing a shoe at Janet?",
        "answer": "the shoe hit Harold instead",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Was there payback for Clarice throwing her shoe at Janet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "was there a payback for that incident?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "was there a payback for the incident where Clarice threw a shoe at Harold instead?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was there a payback for Clarice throwing a shoe at Janet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "How did Janet get her revenge on Clarice?",
        "answer": "She cut off her hair one night",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the payback for throwing the shoe at Harold?",
        "answer": "She cut off her hair one night",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the payback for the incident when Clarice threw a shoe at Janet?",
        "answer": "She cut off her hair one night",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the consequence of Clarice throwing a shoe at Janet?",
        "answer": "She cut off her hair one night",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Harold and Janet ever tie the knot?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did the couple ever tie the knot?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the couple ever tie the knot?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Tammy and Jenny ever tie the knot?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Where did Harold and Janet live after getting married?",
        "answer": "the castle",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did the couple reside afterward?",
        "answer": "the castle",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the couple reside after the wedding?",
        "answer": "the castle",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Tammy and Jenny live after the wedding?",
        "answer": "the castle",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Who got her hair cut off?",
        "answer": "Clarice",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Whose hair was cut off?",
        "answer": "Clarice",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Whose hair was cut off at the castle?",
        "answer": "Clarice",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose hair was cut off?",
        "answer": "Clarice",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Who cut off Clarice's hair?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who cut off Clarice's hair?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And who did the hair cutting?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who cut Clarice's hair?",
        "answer": "Janet",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Why was Clarice mad about her hair being cut?",
        "answer": "she wanted her hair to be long",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why was Janet upset about her hair being cut?",
        "answer": "she wanted her hair to be long",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why was Clarice upset about her hair being cut?",
        "answer": "she wanted her hair to be long",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why was Clarice upset about her hair being cut off?",
        "answer": "she wanted her hair to be long",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3eret4btvm9he6xj29nu1llk20nk91": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-1",
        "original_question": "Which port was home to the Titanic?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-1",
        "answer": "Liverpool"
      },
      {
        "turn_num": 2,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-2",
        "original_question": "Was the Queen Mary registered there as well?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-2",
        "answer": "Yes."
      },
      {
        "turn_num": 3,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-3",
        "original_question": "What other famous ships have called it home?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-3",
        "answer": "Lusitania and Olympic"
      },
      {
        "turn_num": 4,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-4",
        "original_question": "What estuary lies to its west?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-4",
        "answer": "Mersey Estuary"
      },
      {
        "turn_num": 5,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-5",
        "original_question": "Before 1889, what county was it a part of?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-5",
        "answer": "Lancashire"
      },
      {
        "turn_num": 6,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-6",
        "original_question": "What ancient hundred does it lie in?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-6",
        "answer": "West Derby"
      },
      {
        "turn_num": 7,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-7",
        "original_question": "Did it become a borough and a city at the same time?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-7",
        "answer": "No."
      },
      {
        "turn_num": 8,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-8",
        "original_question": "Which occurred first?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-8",
        "answer": "It became a borough first."
      },
      {
        "turn_num": 9,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-9",
        "original_question": "When was that?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-9",
        "answer": "1207"
      },
      {
        "turn_num": 10,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-10",
        "original_question": "When did it gain city status?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-10",
        "answer": "1880"
      },
      {
        "turn_num": 11,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-11",
        "original_question": "What happened that sped up the growth of the city?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-11",
        "answer": "Its growth as a major port ."
      },
      {
        "turn_num": 12,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-12",
        "original_question": "What was going on that caused that?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-12",
        "answer": "The Atlantic slave trade."
      },
      {
        "turn_num": 13,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-13",
        "original_question": "Did any raw materials go through the port?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-13",
        "answer": "Yes."
      },
      {
        "turn_num": 14,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-14",
        "original_question": "Name one of the materials.",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-14",
        "answer": "Coal."
      },
      {
        "turn_num": 15,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-15",
        "original_question": "What would be another example?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-15",
        "answer": "Cotton"
      },
      {
        "turn_num": 16,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-16",
        "original_question": "If I were Irish in the 1800s and going to America, what port would I likely leave from?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-16",
        "answer": "Liverpool"
      },
      {
        "turn_num": 17,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-17",
        "original_question": "What part of England is it in?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-17",
        "answer": "North West"
      },
      {
        "turn_num": 18,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-18",
        "original_question": "Does the city itself have over a million inhabitants?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-18",
        "answer": "No."
      },
      {
        "turn_num": 19,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-19",
        "original_question": "What about the metropolitan area?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-19",
        "answer": "Yes."
      },
      {
        "turn_num": 20,
        "turn_id": "3eret4btvm9he6xj29nu1llk20nk91-20",
        "original_question": "How many metropolitan areas in the UK are larger?",
        "original_question_id": "3eret4btvm9he6xj29nu1llk20nk91-20",
        "answer": "Four."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What port did the Titanic come from?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which port was home to the Titanic?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which port was home to the Titanic?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What port was home to the Titanic?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Was Liverpool the port of registry for the Queen Mary?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the Queen Mary registered there as well?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the Queen Mary registered at Liverpool as well?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the Queen Mary registered in Liverpool?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What other famous ships have been based out of Liverpool, like the Queen Mary and the Titanic?",
        "answer": "Lusitania and Olympic",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What other famous ships have called Liverpool home?",
        "answer": "Lusitania and Olympic",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What other famous ships have called the Titanic home?",
        "answer": "Lusitania and Olympic",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What other famous ships have called Liverpool home?",
        "answer": "Lusitania and Olympic",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What estuary is to the west of Liverpool?",
        "answer": "Mersey Estuary",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What estuary lies to its west?",
        "answer": "Mersey Estuary",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What estuary lies to its west of Liverpool?",
        "answer": "Mersey Estuary",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What estuary is to the west of Liverpool?",
        "answer": "Mersey Estuary",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Prior to 1889, what county was Liverpool a part of?",
        "answer": "Lancashire",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Before 1889, what county was it a part of?",
        "answer": "Lancashire",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Before 1889, what county was the Mersey Estuary a part of?",
        "answer": "Lancashire",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What county was Liverpool a part of before 1889?",
        "answer": "Lancashire",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What ancient hundred does Liverpool lie in?",
        "answer": "West Derby",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What ancient hundred does Liverpool lie in?",
        "answer": "West Derby",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What ancient hundred does the Lancashire Estuary lie in?",
        "answer": "West Derby",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What ancient hundred is Liverpool in?",
        "answer": "West Derby",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Did Liverpool become a borough at the same time it became a city?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did it become a borough and a city at the same time?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did West Derby become a borough and a city at the same time?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Liverpool become a borough and a city at the same time?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Was Liverpool made a borough or a city first?",
        "answer": "It became a borough first.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which occurred first, becoming a borough or becoming a city?",
        "answer": "It became a borough first.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which occurred first in the county of Derby?",
        "answer": "It became a borough first.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the first time that Liverpool became a city and borough?",
        "answer": "It became a borough first.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "When was Liverpool made into a borough?",
        "answer": "1207",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Liverpool become a borough?",
        "answer": "1207",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the borough of Derby first formed?",
        "answer": "1207",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Liverpool become a borough?",
        "answer": "1207",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "When was Liverpool made into a city?",
        "answer": "1880",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Liverpool gain city status?",
        "answer": "1880",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Derby gain city status?",
        "answer": "1880",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Liverpool gain city status?",
        "answer": "1880",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What sped up the growth of the city of Liverpool?",
        "answer": "Its growth as a major port .",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened that sped up the growth of the city?",
        "answer": "Its growth as a major port .",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happened that sped up the growth of the city of Derby?",
        "answer": "Its growth as a major port .",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened that sped up the growth of Liverpool?",
        "answer": "Its growth as a major port .",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What was the cause of Liverpool's rapid expansion?",
        "answer": "The Atlantic slave trade.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was going on in Liverpool that caused its growth as a major port?",
        "answer": "The Atlantic slave trade.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was going on that caused Liverpool's growth as a major port?",
        "answer": "The Atlantic slave trade.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was going on that sped up the growth of Liverpool?",
        "answer": "The Atlantic slave trade.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Were there raw materials that went through Liverpool's port?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "raw materials through the port",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did any raw materials go through the Liverpool port?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Liverpool receive raw materials from the Atlantic slave trade?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Name a raw material that passed through Liverpool.",
        "answer": "Coal.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Name one of the materials that went through the port.",
        "answer": "Coal.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Name one of the raw materials that went through the port?",
        "answer": "Coal.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was one of the raw materials that came through Liverpool?",
        "answer": "Coal.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Name a raw material that passed through Liverpool, besides coal.",
        "answer": "Cotton",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What other raw materials went through the port of Liverpool?",
        "answer": "Cotton",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What would be another example of the port of Liverpool being a major port?",
        "answer": "Cotton",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What would be another example of raw materials that came through Liverpool?",
        "answer": "Cotton",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What port would an Irishman pass through to get to America in the 1800s?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "port Irish immigrants leave from in the 1800s",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "If I were Irish in the 1800s and going to America, what port would I likely leave from?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What port would I likely leave Liverpool from if I were Irish in the 1800s?",
        "answer": "Liverpool",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What part of England is Liverpool in?",
        "answer": "North West",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What part of England is Liverpool in?",
        "answer": "North West",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What part of England is Liverpool in?",
        "answer": "North West",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What part of England is Liverpool in?",
        "answer": "North West",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Are there over a million inhabitants in the city of Liverpool?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does the city itself have over a million inhabitants?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does Liverpool itself have over a million inhabitants?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Liverpool have over a million residents?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Are there over a million inhabitants in the  metropolitan area of Liverpool?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the population of the Liverpool metropolitan area?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What about the metropolitan area of Liverpool?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does Liverpool have a metropolitan area of over a million people?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "How many UK metropolitan areas are bigger than Liverpool?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many metropolitan areas in the UK are larger than Liverpool?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many metropolitan areas in the UK are larger than Liverpool?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many metropolitan areas in the UK are larger than Liverpool?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3gdtjdapvubcqpecituwg2id7wk8mj": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-1",
        "original_question": "Who led the Bolsheviks?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-1",
        "answer": "Vladimir Lenin,"
      },
      {
        "turn_num": 2,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-2",
        "original_question": "What did they overthrow?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-2",
        "answer": "Russian Provisional Government"
      },
      {
        "turn_num": 3,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-3",
        "original_question": "Which had replaced the rule of whom?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-3",
        "answer": "Tsar Nicholas II"
      },
      {
        "turn_num": 4,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-4",
        "original_question": "When did the overthrow occur?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-4",
        "answer": "1917,"
      },
      {
        "turn_num": 5,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-5",
        "original_question": "Who succeeded Lenin?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-5",
        "answer": "Joseph Stalin"
      },
      {
        "turn_num": 6,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-6",
        "original_question": "When?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-6",
        "answer": "the mid-1920s."
      },
      {
        "turn_num": 7,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-7",
        "original_question": "When was the Great Purge?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-7",
        "answer": "the mid-1930s"
      },
      {
        "turn_num": 8,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-8",
        "original_question": "True or False: Stalin tolerated political criticism.",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-8",
        "answer": "false"
      },
      {
        "turn_num": 9,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-9",
        "original_question": "What did he cause in 1933?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-9",
        "answer": "a major famine"
      },
      {
        "turn_num": 10,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-10",
        "original_question": "Where?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-10",
        "answer": "Soviet Ukraine,"
      },
      {
        "turn_num": 11,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-11",
        "original_question": "For how many deaths there was he basically responsible?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-11",
        "answer": "f over 7 million people."
      },
      {
        "turn_num": 12,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-12",
        "original_question": "Did the Soviet Union have nuclear weapons?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-12",
        "answer": "yes"
      },
      {
        "turn_num": 13,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-13",
        "original_question": "How many other states had them?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-13",
        "answer": "four others"
      },
      {
        "turn_num": 14,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-14",
        "original_question": "What does USSR stand for?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-14",
        "answer": "Union of Soviet Socialist Republics"
      },
      {
        "turn_num": 15,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-15",
        "original_question": "What was the Russian version of this acronym?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-15",
        "answer": "\u0421\u0421\u0421\u0420"
      },
      {
        "turn_num": 16,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-16",
        "original_question": "True or False: The USSR was a two-party state.",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-16",
        "answer": "false"
      },
      {
        "turn_num": 17,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-17",
        "original_question": "What party did they have?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-17",
        "answer": "governed by the Communist Party"
      },
      {
        "turn_num": 18,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-18",
        "original_question": "What was the capital of the USSR?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-18",
        "answer": "Moscow"
      },
      {
        "turn_num": 19,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-19",
        "original_question": "Name another major city of the USSR.",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-19",
        "answer": "Minsk"
      },
      {
        "turn_num": 20,
        "turn_id": "3gdtjdapvubcqpecituwg2id7wk8mj-20",
        "original_question": "And another?",
        "original_question_id": "3gdtjdapvubcqpecituwg2id7wk8mj-20",
        "answer": "Leningrad"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who was the leader of the Bolsheviks?",
        "answer": "Vladimir Lenin,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who led the Bolsheviks?",
        "answer": "Vladimir Lenin,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who led the Bolsheviks?",
        "answer": "Vladimir Lenin,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the leader of the Bolsheviks?",
        "answer": "Vladimir Lenin,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What did the Bolsheviks overthrow?",
        "answer": "Russian Provisional Government",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the Bolsheviks overthrow?",
        "answer": "Russian Provisional Government",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the Bolsheviks overthrow?",
        "answer": "Russian Provisional Government",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the Bolsheviks overthrow?",
        "answer": "Russian Provisional Government",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What did the Russian Provisional Government replace?",
        "answer": "Tsar Nicholas II",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which had replaced the rule of whom?",
        "answer": "Tsar Nicholas II",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which had replaced the rule of whom?",
        "answer": "Tsar Nicholas II",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who had replaced the rule of the Bolsheviks?",
        "answer": "Tsar Nicholas II",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "When did the Bolsheviks overthrow the Russian Provisional Government?",
        "answer": "1917,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did the overthrow occur?",
        "answer": "1917,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did the overthrow of Tsar Nicholas II occur?",
        "answer": "1917,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Bolsheviks overthrow the Russian Provisional Government?",
        "answer": "1917,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who was successor to Lenin?",
        "answer": "Joseph Stalin",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who succeeded Lenin?",
        "answer": "Joseph Stalin",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who succeeded Vladimir Lenin?",
        "answer": "Joseph Stalin",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who succeeded Vladimir Lenin?",
        "answer": "Joseph Stalin",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "When did Joseph Stalin come into power?",
        "answer": "the mid-1920s.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Joseph Stalin succeed Lenin?",
        "answer": "the mid-1920s.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Joseph Stalin succeed Vladimir Lenin?",
        "answer": "the mid-1920s.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Joseph Stalin succeed Vladimir Lenin?",
        "answer": "the mid-1920s.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "When did the Great Purge happen?",
        "answer": "the mid-1930s",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the Great Purge?",
        "answer": "the mid-1930s",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the Great Purge?",
        "answer": "the mid-1930s",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Great Purge occur?",
        "answer": "the mid-1930s",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Is it true or false that Joseph Stalin tolerated political criticism?",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "True or False: Stalin tolerated political criticism.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "True or False: Stalin tolerated political criticism.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "True or False: Stalin tolerated political criticism.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What did Joseph Stalin cause in 1933?",
        "answer": "a major famine",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Stalin cause in 1933?",
        "answer": "a major famine",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Joseph Stalin cause in 1933?",
        "answer": "a major famine",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Joseph Stalin cause in 1933?",
        "answer": "a major famine",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Where did major famine occur in 1933?",
        "answer": "Soviet Ukraine,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Stalin cause a major famine in 1933?",
        "answer": "Soviet Ukraine,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Joseph Stalin cause a major famine in 1933?",
        "answer": "Soviet Ukraine,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Joseph Stalin cause a major famine in 1933?",
        "answer": "Soviet Ukraine,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "How many famine deaths was Joseph Stalin responsible for?",
        "answer": "f over 7 million people.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "For how many deaths was Stalin basically responsible?",
        "answer": "f over 7 million people.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "For how many deaths was Stalin basically responsible for?",
        "answer": "f over 7 million people.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many deaths did Joseph Stalin basically cause?",
        "answer": "f over 7 million people.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Was the Soviet Union a recognized nuclear weapons state?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did the Soviet Union have nuclear weapons?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the Soviet Union have nuclear weapons?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the Soviet Union have nuclear weapons?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "How many states apart from the USSR had nuclear weapons?",
        "answer": "four others",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many other states had nuclear weapons?",
        "answer": "four others",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many other states had nuclear weapons besides the Soviet Union?",
        "answer": "four others",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many other states had nuclear weapons?",
        "answer": "four others",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is USSR an acronym for?",
        "answer": "Union of Soviet Socialist Republics",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does USSR stand for?",
        "answer": "Union of Soviet Socialist Republics",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does USSR stand for?",
        "answer": "Union of Soviet Socialist Republics",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does USSR stand for?",
        "answer": "Union of Soviet Socialist Republics",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How was the acronym USSR written in Russian?",
        "answer": "\u0421\u0421\u0421\u0420",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the Russian version of the acronym USSR?",
        "answer": "\u0421\u0421\u0421\u0420",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the Russian version of the Union of Soviet Socialist Republics?",
        "answer": "\u0421\u0421\u0421\u0420",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the Russian version of the acronym USSR?",
        "answer": "\u0421\u0421\u0421\u0420",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Is it true or false that the Soviet Union was a two-party state.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the USSR a two-party state?",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "True or False: The USSR was a two-party state",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "True or false: the USSR was a two-party state.",
        "answer": "false",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What was the Political party of the USSR?",
        "answer": "governed by the Communist Party",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What party did the USSR have?",
        "answer": "governed by the Communist Party",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What party did the USSR have?",
        "answer": "governed by the Communist Party",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What party did the USSR have?",
        "answer": "governed by the Communist Party",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What was the capital city of the Soviet Union?",
        "answer": "Moscow",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the capital of the USSR?",
        "answer": "Moscow",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the capital of the USSR?",
        "answer": "Moscow",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the capital of the USSR?",
        "answer": "Moscow",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Besides Moscow, what was another major city of the USSR?",
        "answer": "Minsk",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Name another major city of the USSR.",
        "answer": "Minsk",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Name another major city of the USSR?",
        "answer": "Minsk",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was another major city in the USSR?",
        "answer": "Minsk",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Besides Moscow and Minsk, what was another major city of the USSR?",
        "answer": "Leningrad",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Name another major city of the USSR.",
        "answer": "Leningrad",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And another major city of the USSR besides Minsk?",
        "answer": "Leningrad",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the name of the USSR's capital city, in addition to Minsk?",
        "answer": "Leningrad",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3i2pta7r3tun65e5jbygngb9c04kq0": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-1",
        "original_question": "What does being in the top 10% of your high school class get you?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-1",
        "answer": "automatic admission"
      },
      {
        "turn_num": 2,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-2",
        "original_question": "To where?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-2",
        "answer": "at the university"
      },
      {
        "turn_num": 3,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-3",
        "original_question": "Which one?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-3",
        "answer": "University of Texas"
      },
      {
        "turn_num": 4,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-4",
        "original_question": "Where do whites rank on the SAT?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-4",
        "answer": "89th percentile"
      },
      {
        "turn_num": 5,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-5",
        "original_question": "Asians?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-5",
        "answer": "93rd percentile"
      },
      {
        "turn_num": 6,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-6",
        "original_question": "Hispanics",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-6",
        "answer": "80th percentile"
      },
      {
        "turn_num": 7,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-7",
        "original_question": "Blacks?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-7",
        "answer": "52nd percentile"
      },
      {
        "turn_num": 8,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-8",
        "original_question": "What system is to blame for this difference?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-8",
        "answer": "K-12 school system."
      },
      {
        "turn_num": 9,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-9",
        "original_question": "Who is taking their case to The Supreme Court?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-9",
        "answer": "Abigail Fisher"
      },
      {
        "turn_num": 10,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-10",
        "original_question": "Who is she suing?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-10",
        "answer": "University of Texas"
      },
      {
        "turn_num": 11,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-11",
        "original_question": "What does she feel made her a victim?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-11",
        "answer": "race-conscious admission policies"
      },
      {
        "turn_num": 12,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-12",
        "original_question": "What race is she?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-12",
        "answer": "white"
      },
      {
        "turn_num": 13,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-13",
        "original_question": "How does the university feel about their policy?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-13",
        "answer": "is educationally enriching"
      },
      {
        "turn_num": 14,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-14",
        "original_question": "How many justices will participate in this case?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-14",
        "answer": "eight"
      },
      {
        "turn_num": 15,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-15",
        "original_question": "Who's they ninth judge?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-15",
        "answer": "Elena Kagan"
      },
      {
        "turn_num": 16,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-16",
        "original_question": "What happened to her?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-16",
        "answer": "bowed out"
      },
      {
        "turn_num": 17,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-17",
        "original_question": "Was she previously involved in this case?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-17",
        "answer": "yes"
      },
      {
        "turn_num": 18,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-18",
        "original_question": "During what?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-18",
        "answer": "during Obama administration"
      },
      {
        "turn_num": 19,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-19",
        "original_question": "What was her job?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-19",
        "answer": "solicitor general"
      },
      {
        "turn_num": 20,
        "turn_id": "3i2pta7r3tun65e5jbygngb9c04kq0-20",
        "original_question": "Without her how many will doubt the policy?",
        "original_question_id": "3i2pta7r3tun65e5jbygngb9c04kq0-20",
        "answer": "five"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "In Texas, what does being in the top 10% of one's high school class grant you?",
        "answer": "automatic admission",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does being in the top 10% of your high school class get you?",
        "answer": "automatic admission",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does being in the top 10% of your high school class get you?",
        "answer": "automatic admission",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does being in the top 10% of your high school class get you?",
        "answer": "automatic admission",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Where are high achieving high schoolers automatically admitted?",
        "answer": "at the university",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does being in the top 10% of your high school class get you?",
        "answer": "at the university",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "To where does being in the top 10% of your high school class get you admission?",
        "answer": "at the university",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where does being in the top 10% of your high school class get you admission?",
        "answer": "at the university",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What univiersity gives high achieving high schoolers automatic admission?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does being in the top 10% of your high school class get you?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which university does being in the top 10% of your high school class get you admission to?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which university does automatic admission to get you to?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What is the ranking of white high schoolers on the SAT?",
        "answer": "89th percentile",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where do whites rank on the SAT?",
        "answer": "89th percentile",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where do whites rank on the SAT at the University of Texas?",
        "answer": "89th percentile",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the rank of white students on the SAT?",
        "answer": "89th percentile",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What is the ranking of Asians high schoolers on the SAT?",
        "answer": "93rd percentile",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where do Asians rank on the SAT?",
        "answer": "93rd percentile",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Asians rank on the SAT?",
        "answer": "93rd percentile",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the ranking of Asians on the SAT?",
        "answer": "93rd percentile",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What is the ranking of Hispanic high schoolers on the SAT?",
        "answer": "80th percentile",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where do Hispanics rank on the SAT?",
        "answer": "80th percentile",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Hispanics rank 93rd on the SAT?",
        "answer": "80th percentile",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Hispanics rank in what percentile on the SAT?",
        "answer": "80th percentile",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is the ranking of Black high schoolers on the SAT?",
        "answer": "52nd percentile",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the SAT score percentile for blacks?",
        "answer": "52nd percentile",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where do blacks rank on the SAT?",
        "answer": "52nd percentile",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the rank of blacks on the SAT?",
        "answer": "52nd percentile",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What system could be blamed for racial differences in SAT scores?",
        "answer": "K-12 school system.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What system is to blame for the difference in SAT scores among racial groups?",
        "answer": "K-12 school system.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What system is to blame for the difference between the 52nd percentile and the 93rd percentile?",
        "answer": "K-12 school system.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What system is to blame for the difference in SAT scores between blacks and Asians?",
        "answer": "K-12 school system.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What student is going to the Supreme Court with her case?",
        "answer": "Abigail Fisher",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is taking their case to The Supreme Court?",
        "answer": "Abigail Fisher",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is taking the case of the Hispanics to The Supreme Court?",
        "answer": "Abigail Fisher",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is taking their case to the Supreme Court?",
        "answer": "Abigail Fisher",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who is Abigail Fisher suing?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Abigail Fisher suing?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is Abigail Fisher suing?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is Abigail Fisher suing?",
        "answer": "University of Texas",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What does Abigail Fisher feel victimized by?",
        "answer": "race-conscious admission policies",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Abigail Fisher feel made her a victim?",
        "answer": "race-conscious admission policies",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Abigail Fisher feel made her a victim?",
        "answer": "race-conscious admission policies",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Abigail Fisher feel made her a victim?",
        "answer": "race-conscious admission policies",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What is Abigail Fisher's race?",
        "answer": "white",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What race is Abigail Fisher?",
        "answer": "white",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What race is Abigail Fisher?",
        "answer": "white",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Abigail Fisher's race?",
        "answer": "white",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "How does the University of Texas feel about their race-based admissions policy?",
        "answer": "is educationally enriching",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How does the university feel about their policy?",
        "answer": "is educationally enriching",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How does the University of Texas feel about their race-conscious admission policy?",
        "answer": "is educationally enriching",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the University of Texas's opinion of their admission policy?",
        "answer": "is educationally enriching",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "How many justices will participate in Abigail Fisher's suit?",
        "answer": "eight",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many justices will participate in the Fisher v. University of Texas case?",
        "answer": "eight",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many justices will participate in Abigail Fisher's case?",
        "answer": "eight",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many justices will be involved in Abigail Fisher's case?",
        "answer": "eight",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Who is the ninth judge on the Supreme Court?",
        "answer": "Elena Kagan",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is the ninth judge in the Supreme Court?",
        "answer": "Elena Kagan",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who's the ninth judge at the University of Texas?",
        "answer": "Elena Kagan",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the ninth judge on the Supreme Court?",
        "answer": "Elena Kagan",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What happened to Elena Kagan?",
        "answer": "bowed out",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened to Elena Kagan?",
        "answer": "bowed out",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happened to Abigail Fisher?",
        "answer": "bowed out",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened to Abigail Fisher?",
        "answer": "bowed out",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Was Elena Kagan previously involved in Abigail Fisher's case?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Elena Kagan previously involved in the Fisher v. University of Texas case?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Abigail Fisher previously involved in the case of Elena Kagan?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Abigail Fisher previously involved in the case?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "At what point was Elena Kagan previously involved in Abigail Fisher's case?",
        "answer": "during Obama administration",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "During what was Elena Kagan previously involved in the case?",
        "answer": "during Obama administration",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "During what?",
        "answer": "during Obama administration",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Abigail Fisher's previous involvement in the case?",
        "answer": "during Obama administration",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What was Elena Kagan's role under the Obama administration?",
        "answer": "solicitor general",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Elena Kagan's job during the Obama administration?",
        "answer": "solicitor general",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Elena Kagan's job?",
        "answer": "solicitor general",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Abigail Fisher's job?",
        "answer": "solicitor general",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "How many judges will cast doubt on the race-based admissions policy?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Without her how many will doubt the policy?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Without Elena Kagan how many will doubt the Obama administration's race-conscious admission policy?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many people will doubt the University of Texas's race-conscious admission policy without Abigail Fisher?",
        "answer": "five",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3io1lgzlk9xa1mtkvdnfr6lrhv2682": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-1",
        "original_question": "What's the french version's address?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-1",
        "answer": "(slate.fr)"
      },
      {
        "turn_num": 2,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-2",
        "original_question": "when was it started?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-2",
        "answer": "February 2009"
      },
      {
        "turn_num": 3,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-3",
        "original_question": "how many people started it",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-3",
        "answer": "four"
      },
      {
        "turn_num": 4,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-4",
        "original_question": "what was their jobs?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-4",
        "answer": "journalists,"
      },
      {
        "turn_num": 5,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-5",
        "original_question": "anything else?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-5",
        "answer": "Jacques Attali was an economist"
      },
      {
        "turn_num": 6,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-6",
        "original_question": "how many main topics does the US version cover?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-6",
        "answer": "three"
      },
      {
        "turn_num": 7,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-7",
        "original_question": "name 1",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-7",
        "answer": "politics"
      },
      {
        "turn_num": 8,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-8",
        "original_question": "and?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-8",
        "answer": "culture"
      },
      {
        "turn_num": 9,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-9",
        "original_question": "what's it's point of view?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-9",
        "answer": "a liberal perspective"
      },
      {
        "turn_num": 10,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-10",
        "original_question": "when was it started?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-10",
        "answer": "in 1996"
      },
      {
        "turn_num": 11,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-11",
        "original_question": "is it a hard copy paper?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-11",
        "answer": "no"
      },
      {
        "turn_num": 12,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-12",
        "original_question": "who launched it?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-12",
        "answer": "Michael Kinsley"
      },
      {
        "turn_num": 13,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-13",
        "original_question": "what was his previous job?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-13",
        "answer": "editor of New Republic"
      },
      {
        "turn_num": 14,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-14",
        "original_question": "who was it's first owner?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-14",
        "answer": "Microsoft as part of MSN."
      },
      {
        "turn_num": 15,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-15",
        "original_question": "when was it bought?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-15",
        "answer": "On December 21, 2004"
      },
      {
        "turn_num": 16,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-16",
        "original_question": "who bought it?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-16",
        "answer": "by The Washington Post Company"
      },
      {
        "turn_num": 17,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-17",
        "original_question": "what did that change it's name to?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-17",
        "answer": "the Graham Holdings Compan"
      },
      {
        "turn_num": 18,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-18",
        "original_question": "how many years had it been managed by The Slate Group?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-18",
        "answer": "Since June 4, 2008 so 10"
      },
      {
        "turn_num": 19,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-19",
        "original_question": "where is it based?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-19",
        "answer": "in New York City"
      },
      {
        "turn_num": 20,
        "turn_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-20",
        "original_question": "any other offices?",
        "original_question_id": "3io1lgzlk9xa1mtkvdnfr6lrhv2682-20",
        "answer": "yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is the French version of Slate's address?",
        "answer": "(slate.fr)",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What's the french version's address?",
        "answer": "(slate.fr)",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What's the french version's address?",
        "answer": "(slate.fr)",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the French version's address?",
        "answer": "(slate.fr)",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "When was the French version of Slate founded?",
        "answer": "February 2009",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was the french version started?",
        "answer": "February 2009",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the french version of Slate.fr started?",
        "answer": "February 2009",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was the French version started?",
        "answer": "February 2009",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "How many people started the French version of Slate?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many people started slate.fr",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many people started slate.fr in 2009?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many people started Slate.fr?",
        "answer": "four",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What was the occupation of the people who founded French Slate?",
        "answer": "journalists,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were the jobs of the people who started the French version?",
        "answer": "journalists,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the French version's job?",
        "answer": "journalists,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the founders of Slate do for a living?",
        "answer": "journalists,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Was there anyone who wasn't an economist that helped found French Slate?",
        "answer": "Jacques Attali was an economist",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what was their jobs?",
        "answer": "Jacques Attali was an economist",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Anything else about Slate.fr besides journalists, what was their jobs?",
        "answer": "Jacques Attali was an economist",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the job of the journalists, besides being journalists?",
        "answer": "Jacques Attali was an economist",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What's the number of main topics covered by the US version of Slate?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many main topics does the US version cover?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many main topics does the US version of Slate cover?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the number of main topics covered by the US version of Slate?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is a main topic that the US version of Slate covers?",
        "answer": "politics",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "name 1",
        "answer": "politics",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "name 1 of the french version of slate.fr",
        "answer": "politics",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is one of the main topics covered by the US version of Slate?",
        "answer": "politics",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What is a main topic that the US version of Slate covers besides politics?",
        "answer": "culture",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What are the other main topics covered by the US version of Slate?",
        "answer": "culture",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and how many main topics does the US version of Slate cover?",
        "answer": "culture",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the main topics covered by the US version of the article, besides politics?",
        "answer": "culture",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What kind of point of view does Slate have?",
        "answer": "a liberal perspective",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the point of view of the US version of Slate?",
        "answer": "a liberal perspective",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What's the French version's point of view?",
        "answer": "a liberal perspective",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the US version's point of view?",
        "answer": "a liberal perspective",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "When was Slate founded?",
        "answer": "in 1996",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when was the US version started?",
        "answer": "in 1996",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was the french version of slate.fr started?",
        "answer": "in 1996",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was Slate.fr founded?",
        "answer": "in 1996",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Is Slate a hard copy paper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is the US version of the French newspaper a hard copy paper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is the french version of slate.fr a hard copy paper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is the French version of Slate a hard copy paper?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Who launched Slate?",
        "answer": "Michael Kinsley",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who launched the US version of the French newspaper?",
        "answer": "Michael Kinsley",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who launched the french version of slate?",
        "answer": "Michael Kinsley",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who launched Slate?",
        "answer": "Michael Kinsley",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Where did Michael Kinsley work before founding Slate?",
        "answer": "editor of New Republic",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Michael Kinsley previous job",
        "answer": "editor of New Republic",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what was Michael Kinsley's previous job?",
        "answer": "editor of New Republic",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Michael Kinsley's previous job?",
        "answer": "editor of New Republic",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Who was the first owner of Slate?",
        "answer": "Microsoft as part of MSN.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was the first owner of the US version?",
        "answer": "Microsoft as part of MSN.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who was the New Republic's first owner?",
        "answer": "Microsoft as part of MSN.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the first owner of the New Republic?",
        "answer": "Microsoft as part of MSN.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "When was Slate purchased?",
        "answer": "On December 21, 2004",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was it bought?",
        "answer": "On December 21, 2004",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when was the new republic bought?",
        "answer": "On December 21, 2004",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was the New Republic bought?",
        "answer": "On December 21, 2004",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Who bought Slate from Microsoft?",
        "answer": "by The Washington Post Company",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who bought Slate?",
        "answer": "by The Washington Post Company",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who bought the New Republic?",
        "answer": "by The Washington Post Company",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who bought the New Republic?",
        "answer": "by The Washington Post Company",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What did The Washington Post Company change its name to?",
        "answer": "the Graham Holdings Compan",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did The Washington Post Company change its name to?",
        "answer": "the Graham Holdings Compan",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did the Washington Post change it's name to?",
        "answer": "the Graham Holdings Compan",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the Washington Post Company change the name of the newspaper?",
        "answer": "the Graham Holdings Compan",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "For how many years has the Slate Group managed the magazine?",
        "answer": "Since June 4, 2008 so 10",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many years had it been managed by The Slate Group?",
        "answer": "Since June 4, 2008 so 10",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many years had the slate.fr. been managed by The Slate Group?",
        "answer": "Since June 4, 2008 so 10",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many years had The Slate Group been managing the newspaper?",
        "answer": "Since June 4, 2008 so 10",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Where is Slate based?",
        "answer": "in New York City",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where is the Slate Group based?",
        "answer": "in New York City",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is the french version of the slate.fr based?",
        "answer": "in New York City",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where is the Slate Group based?",
        "answer": "in New York City",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Are there Slate offices outside of New York City?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "any other offices?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Any other offices for the Slate Group besides New York City?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the Slate Group have any other offices in New York City?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3jjvg1ybebxxkgrdt6xkq2xss4y5b1": {
    "number_of_turns": 12,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-1",
        "original_question": "DId the boy have a pet?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-1",
        "answer": "yes"
      },
      {
        "turn_num": 2,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-2",
        "original_question": "What kind?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-2",
        "answer": "a dog"
      },
      {
        "turn_num": 3,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-3",
        "original_question": "What was it called?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-3",
        "answer": "Ruffles"
      },
      {
        "turn_num": 4,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-4",
        "original_question": "What did the mother make him do?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-4",
        "answer": "lick Douglas"
      },
      {
        "turn_num": 5,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-5",
        "original_question": "Why?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-5",
        "answer": "To get Douglas up"
      },
      {
        "turn_num": 6,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-6",
        "original_question": "Did he do the same thing to wake him up another time?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-6",
        "answer": "yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-7",
        "original_question": "Where?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-7",
        "answer": "the face"
      },
      {
        "turn_num": 8,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-8",
        "original_question": "And where were they?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-8",
        "answer": "The game"
      },
      {
        "turn_num": 9,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-9",
        "original_question": "What kind of game?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-9",
        "answer": "soccer"
      },
      {
        "turn_num": 10,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-10",
        "original_question": "Who took them there?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-10",
        "answer": "His Mom"
      },
      {
        "turn_num": 11,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-11",
        "original_question": "And who won it?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-11",
        "answer": "the Dolphins"
      },
      {
        "turn_num": 12,
        "turn_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-12",
        "original_question": "Which player helped them do that?",
        "original_question_id": "3jjvg1ybebxxkgrdt6xkq2xss4y5b1-12",
        "answer": "Douglas"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Did Douglas have a pet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "DId the boy have a pet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "DId the boy have a pet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the boy have a pet?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What kind of pet did Douglas have?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of pet did the boy have?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of pet did the boy have?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of animal did the boy have?",
        "answer": "a dog",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What was the name of Douglas's dog?",
        "answer": "Ruffles",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the name of the boy's pet?",
        "answer": "Ruffles",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the dog called?",
        "answer": "Ruffles",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the dog called?",
        "answer": "Ruffles",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What did Douglas's mother make Ruffles do?",
        "answer": "lick Douglas",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the mother make him do?",
        "answer": "lick Douglas",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the mother make Ruffles do?",
        "answer": "lick Douglas",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the mother make Ruffles do?",
        "answer": "lick Douglas",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Why did Ruffles lick Douglas's face?",
        "answer": "To get Douglas up",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did the mother make him do that?",
        "answer": "To get Douglas up",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did the mother make Douglas lick Douglas?",
        "answer": "To get Douglas up",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did the mother make Douglas lick Douglas?",
        "answer": "To get Douglas up",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did Ruffles lick Douglas's face to wake him up a second time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he do the same thing to wake him up another time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the boy do the same thing to wake Douglas up another time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Douglas get up again by licking Ruffles?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Where did Ruffles lick Douglas?",
        "answer": "the face",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he do the same thing to wake him up another time?",
        "answer": "the face",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the boy do the same thing to wake him up another time?",
        "answer": "the face",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Douglas get licked by Ruffles?",
        "answer": "the face",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Where did Ruffles lick Douglas the second time?",
        "answer": "The game",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "And where were they?",
        "answer": "The game",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And where were the ruffles?",
        "answer": "The game",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Douglas's parents go to get him up?",
        "answer": "The game",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What kind of game was Douglas playing in?",
        "answer": "soccer",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of game were they playing?",
        "answer": "soccer",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of game did the boy play?",
        "answer": "soccer",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of game did Douglas play?",
        "answer": "soccer",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who took Douglas to his soccer game?",
        "answer": "His Mom",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who took them to the soccer game?",
        "answer": "His Mom",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who took the boys to the soccer game?",
        "answer": "His Mom",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who took the boys to soccer?",
        "answer": "His Mom",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who won the soccer match?",
        "answer": "the Dolphins",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who won the soccer game?",
        "answer": "the Dolphins",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And who won the soccer game with Douglas?",
        "answer": "the Dolphins",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who won the soccer game?",
        "answer": "the Dolphins",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Which player helped the Dolphins win the game?",
        "answer": "Douglas",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which player helped the Dolphins win the game?",
        "answer": "Douglas",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which player helped the Dolphins win the game?",
        "answer": "Douglas",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which player helped the Dolphins win?",
        "answer": "Douglas",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3kxir214i4gl0knhw8lzkhoaz9342l": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-1",
        "original_question": "what what age did her husband die ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-1",
        "answer": "75"
      },
      {
        "turn_num": 2,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-2",
        "original_question": "what did the children think ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-2",
        "answer": "suggested that she move"
      },
      {
        "turn_num": 3,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-3",
        "original_question": "to where ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-3",
        "answer": "Senior living community"
      },
      {
        "turn_num": 4,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-4",
        "original_question": "what is her name ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-4",
        "answer": "Thelma"
      },
      {
        "turn_num": 5,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-5",
        "original_question": "how many things best decribes her ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-5",
        "answer": "Four"
      },
      {
        "turn_num": 6,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-6",
        "original_question": "name 2",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-6",
        "answer": "life-loving and easy-going"
      },
      {
        "turn_num": 7,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-7",
        "original_question": "and never felt what ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-7",
        "answer": "sad"
      },
      {
        "turn_num": 8,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-8",
        "original_question": "what happened at 80 ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-8",
        "answer": "surprise birthday party"
      },
      {
        "turn_num": 9,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-9",
        "original_question": "who did this for her ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-9",
        "answer": "her new friends"
      },
      {
        "turn_num": 10,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-10",
        "original_question": "why ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-10",
        "answer": "showed their appreciation"
      },
      {
        "turn_num": 11,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-11",
        "original_question": "the night was filled with how many things ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-11",
        "answer": "Two"
      },
      {
        "turn_num": 12,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-12",
        "original_question": "name one ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-12",
        "answer": "entertainment"
      },
      {
        "turn_num": 13,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-13",
        "original_question": "another ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-13",
        "answer": "laughter"
      },
      {
        "turn_num": 14,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-14",
        "original_question": "how many times did she get married ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-14",
        "answer": "Four"
      },
      {
        "turn_num": 15,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-15",
        "original_question": "who really asked her that question ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-15",
        "answer": "gentleman"
      },
      {
        "turn_num": 16,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-16",
        "original_question": "at the end of the party what happened ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-16",
        "answer": "rose from her seat"
      },
      {
        "turn_num": 17,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-17",
        "original_question": "what did she say ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-17",
        "answer": "Pardon"
      },
      {
        "turn_num": 18,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-18",
        "original_question": "did they get married ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-18",
        "answer": "yes"
      },
      {
        "turn_num": 19,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-19",
        "original_question": "thelma entered what room ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-19",
        "answer": "dining room"
      },
      {
        "turn_num": 20,
        "turn_id": "3kxir214i4gl0knhw8lzkhoaz9342l-20",
        "original_question": "for what ?",
        "original_question_id": "3kxir214i4gl0knhw8lzkhoaz9342l-20",
        "answer": "dinner"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "At what age did Thelma's husband pass away?",
        "answer": "75",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what what age did her husband die ?",
        "answer": "75",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what what age did her husband die?",
        "answer": "75",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the age of the woman's husband?",
        "answer": "75",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What did Thelma's children suggest she do after her husband's death?",
        "answer": "suggested that she move",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what did the children think ?",
        "answer": "suggested that she move",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did the children think of her husband?",
        "answer": "suggested that she move",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the children think of the death of their father?",
        "answer": "suggested that she move",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Where did Thelma's kids suggest she move?",
        "answer": "Senior living community",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "to where?",
        "answer": "Senior living community",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "to where did she move?",
        "answer": "Senior living community",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the children think that the wife should move?",
        "answer": "Senior living community",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What is the name of the woman in the story?",
        "answer": "Thelma",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is her name?",
        "answer": "Thelma",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is her name?",
        "answer": "Thelma",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the woman that moved to the senior living community?",
        "answer": "Thelma",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "How many adjectives best describe Thelma?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many things best decribes her ?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many things best decribe Thelma?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the most accurate description of Thelma?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What are two adjectives that describe Thelma?",
        "answer": "life-loving and easy-going",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "name 2",
        "answer": "life-loving and easy-going",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "name 2 of Thelma",
        "answer": "life-loving and easy-going",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the names of the two things that best describe Thelma?",
        "answer": "life-loving and easy-going",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did Thelma never feel?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "and never felt what?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "and never felt what?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Thelma never feel?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What happened to Thelma at 80?",
        "answer": "surprise birthday party",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what happened at 80",
        "answer": "surprise birthday party",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what happened to Thelma at 80?",
        "answer": "surprise birthday party",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened at 80?",
        "answer": "surprise birthday party",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who threw Thelma a surprise party?",
        "answer": "her new friends",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who organized the surprise birthday party for her?",
        "answer": "her new friends",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who did the surprise birthday party for Thelma at 80?",
        "answer": "her new friends",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Thelma's birthday party hosted by?",
        "answer": "her new friends",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "WHy did Thelma's new friends want to surprise her?",
        "answer": "showed their appreciation",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "why did her new friends throw a surprise birthday party for her?",
        "answer": "showed their appreciation",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "why did she have a surprise birthday party for her new friends?",
        "answer": "showed their appreciation",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Thelma's new friends throw her a surprise birthday party?",
        "answer": "showed their appreciation",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "How many activities filled the night?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "the night was filled with how many things ?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "the night was filled with how many things for Thelma?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the number of things that Thelma's friends showed appreciation for?",
        "answer": "Two",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What was there a lot of the night of Thelma's party?",
        "answer": "entertainment",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "the night was filled with how many things ?",
        "answer": "entertainment",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "name one?",
        "answer": "entertainment",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was one of Thelma's gifts?",
        "answer": "entertainment",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What was there a lot of the night of Thelma's party, besides entertainment?",
        "answer": "laughter",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "the night was filled with how many things ?",
        "answer": "laughter",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "another?",
        "answer": "laughter",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was one of Thelma's entertainments?",
        "answer": "laughter",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "How many times had Thelma already been married?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many times did she get married?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many times did Thelma get married?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many times did Thelma get married?",
        "answer": "Four",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Who asked Thelma about her previous marriages?",
        "answer": "gentleman",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who really asked her that question ?",
        "answer": "gentleman",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who really asked Thelma the cat to get married?",
        "answer": "gentleman",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who really asked Thelma what she had done?",
        "answer": "gentleman",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What happened at the end of the party?",
        "answer": "rose from her seat",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "at the end of the party what happened ?",
        "answer": "rose from her seat",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "at the end of the party what happened to Thelma?",
        "answer": "rose from her seat",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened at the end of Thelma's birthday party?",
        "answer": "rose from her seat",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What did Thelma say to the gentleman?",
        "answer": "Pardon",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what did she say ?",
        "answer": "Pardon",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what did Thelma say at the end of the party?",
        "answer": "Pardon",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Thelma say at the end of the party?",
        "answer": "Pardon",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Did Thelma and the gentleman get married?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "did they get married?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "did Thelma and her husband get married?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Thelma and her husband get married?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What room did Thelma walk into?",
        "answer": "dining room",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "thelma entered what room?",
        "answer": "dining room",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "thelma entered what room at the party?",
        "answer": "dining room",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What room did Thelma enter?",
        "answer": "dining room",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Why did Thelma go into the dining room?",
        "answer": "dinner",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "for what?",
        "answer": "dinner",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "for what?",
        "answer": "dinner",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Thelma's reason for entering the dining room?",
        "answer": "dinner",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3m0nz3jdp1yt2eutzkdnck4vjynz5m": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-1",
        "original_question": "who is Susan Boyle ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-1",
        "answer": "TV talent show star"
      },
      {
        "turn_num": 2,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-2",
        "original_question": "who will she sing for ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-2",
        "answer": "Pope Benedict XVI"
      },
      {
        "turn_num": 3,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-3",
        "original_question": "who said this ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-3",
        "answer": "the Catholic Church"
      },
      {
        "turn_num": 4,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-4",
        "original_question": "where ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-4",
        "answer": "in Scotland"
      },
      {
        "turn_num": 5,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-5",
        "original_question": "is he due to visit china ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-5",
        "answer": "no"
      },
      {
        "turn_num": 6,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-6",
        "original_question": "where will he visit ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-6",
        "answer": "England and Scotland"
      },
      {
        "turn_num": 7,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-7",
        "original_question": "when ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-7",
        "answer": "September 16-19"
      },
      {
        "turn_num": 8,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-8",
        "original_question": "how many times will she perform ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-8",
        "answer": "three"
      },
      {
        "turn_num": 9,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-9",
        "original_question": "at what park ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-9",
        "answer": "Bellahouston Park"
      },
      {
        "turn_num": 10,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-10",
        "original_question": "in chicago ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-10",
        "answer": "no"
      },
      {
        "turn_num": 11,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-11",
        "original_question": "where ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-11",
        "answer": "Glasgow"
      },
      {
        "turn_num": 12,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-12",
        "original_question": "when did Pope John Paul II visit ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-12",
        "answer": "1982"
      },
      {
        "turn_num": 13,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-13",
        "original_question": "where is the pope flying to ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-13",
        "answer": "London"
      },
      {
        "turn_num": 14,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-14",
        "original_question": "what will boyle sing ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-14",
        "answer": "farewell song"
      },
      {
        "turn_num": 15,
        "turn_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-15",
        "original_question": "the 16th of September will stand out in my memory in what way ?",
        "original_question_id": "3m0nz3jdp1yt2eutzkdnck4vjynz5m-15",
        "answer": "as something I've always wanted to do"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "How is Susan Boyle known to people?",
        "answer": "TV talent show star",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who is Susan Boyle ?",
        "answer": "TV talent show star",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who is Susan Boyle?",
        "answer": "TV talent show star",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Susan Boyle's occupation?",
        "answer": "TV talent show star",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who will Susan Boyle sing for?",
        "answer": "Pope Benedict XVI",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who will Susan Boyle sing for?",
        "answer": "Pope Benedict XVI",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who will Susan Boyle sing for on TV talent show?",
        "answer": "Pope Benedict XVI",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who will Susan Boyle sing for?",
        "answer": "Pope Benedict XVI",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who said that Susan Boyle would sing for the Pope?",
        "answer": "the Catholic Church",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who said this?",
        "answer": "the Catholic Church",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who said Susan Boyle was Pope Benedict XVI?",
        "answer": "the Catholic Church",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who said that Susan Boyle would sing for Pope Benedict XVI?",
        "answer": "the Catholic Church",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Where did the Catholic Church make a statement about Susan Boyle?",
        "answer": "in Scotland",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where?",
        "answer": "in Scotland",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where?",
        "answer": "in Scotland",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the Catholic Church say Susan Boyle would sing?",
        "answer": "in Scotland",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Is the Pope about to visit China?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is Pope Benedict XVI due to visit China?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is Pope Benedict XVI due to visit china?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is Pope Benedict XVI going to visit China?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What countries is the Pope visiting?",
        "answer": "England and Scotland",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where will Pope Benedict XVI visit?",
        "answer": "England and Scotland",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where will Pope Benedict XVI visit?",
        "answer": "England and Scotland",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where will Pope Benedict XVI visit?",
        "answer": "England and Scotland",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "When is the Pope visiting England and Scotland?",
        "answer": "September 16-19",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "when will Pope Benedict XVI visit England and Scotland?",
        "answer": "September 16-19",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when is Pope Benedict XVI due to visit England and Scotland?",
        "answer": "September 16-19",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When is Pope Benedict XVI going to visit China?",
        "answer": "September 16-19",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How many times will Susan Boyle perform?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many times will Susan Boyle perform?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many times will Susan Boyle perform on TV talent show?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many times will Susan Boyle perform?",
        "answer": "three",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "At what park will Susan Boyle perform?",
        "answer": "Bellahouston Park",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "at what park?",
        "answer": "Bellahouston Park",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "at what park will Susan Boyle perform?",
        "answer": "Bellahouston Park",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What park will Susan Boyle perform at?",
        "answer": "Bellahouston Park",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Will Susan Boyle's performance take place in Chicago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Susan Boyle will perform at Bellahouston Park in Chicago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "in chicago?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where in Chicago will Susan Boyle perform?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What city is Susan Boyle going to perform in?",
        "answer": "Glasgow",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where will Susan Boyle perform?",
        "answer": "Glasgow",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where?",
        "answer": "Glasgow",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where is Bellahouston Park?",
        "answer": "Glasgow",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "When did Pope John Paul II visit Bellahouston Park?",
        "answer": "1982",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Pope John Paul II visit?",
        "answer": "1982",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "when did Pope John Paul II visit?",
        "answer": "1982",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Pope John Paul II visit?",
        "answer": "1982",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Where is Pope Benedict flying into?",
        "answer": "London",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "where is the pope flying to?",
        "answer": "London",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is pape john paul ii flying to?",
        "answer": "London",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where is Pope John Paul II going to?",
        "answer": "London",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is Susan Boyle going to sing to the Pope?",
        "answer": "farewell song",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what will Susan Boyle sing?",
        "answer": "farewell song",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what will Susan Boyle sing?",
        "answer": "farewell song",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What will Susan Boyle sing?",
        "answer": "farewell song",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What's important about September 16th to Susan Boyle?",
        "answer": "as something I've always wanted to do",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "the 16th of September will stand out in my memory in what way ?",
        "answer": "as something I've always wanted to do",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "the 16th of September will stand out in my memory in what way?",
        "answer": "as something I've always wanted to do",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What will Susan Boyle's performance on September 16 be remembered for?",
        "answer": "as something I've always wanted to do",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3of2m9aatgowkxfw67hte9ndh4tkz0": {
    "number_of_turns": 11,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-1",
        "original_question": "How old is he?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-1",
        "answer": "12-year"
      },
      {
        "turn_num": 2,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-2",
        "original_question": "Is he good at anything?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-2",
        "answer": "yes"
      },
      {
        "turn_num": 3,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-3",
        "original_question": "What is he good at?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-3",
        "answer": "sports"
      },
      {
        "turn_num": 4,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-4",
        "original_question": "Does he like it?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-4",
        "answer": "yes"
      },
      {
        "turn_num": 5,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-5",
        "original_question": "What else does he want to know how to do?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-5",
        "answer": "skating"
      },
      {
        "turn_num": 6,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-6",
        "original_question": "is he allowed?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-6",
        "answer": "no"
      },
      {
        "turn_num": 7,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-7",
        "original_question": "why not?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-7",
        "answer": "to do his homework"
      },
      {
        "turn_num": 8,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-8",
        "original_question": "Who meets him for his excursion?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-8",
        "answer": "his father"
      },
      {
        "turn_num": 9,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-9",
        "original_question": "Does he want him to?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-9",
        "answer": "do his homework"
      },
      {
        "turn_num": 10,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-10",
        "original_question": "What do they do on their way?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-10",
        "answer": "sees some boys skating"
      },
      {
        "turn_num": 11,
        "turn_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-11",
        "original_question": "do they get something to consume?",
        "original_question_id": "3of2m9aatgowkxfw67hte9ndh4tkz0-11",
        "answer": "yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is the boy's age?",
        "answer": "12-year",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How old is he?",
        "answer": "12-year",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How old is he?",
        "answer": "12-year",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How old is the boy?",
        "answer": "12-year",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Is Wang Bing good at anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is he good at anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is he good at anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is the boy good at anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What is Wang Bing good at?",
        "answer": "sports",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is he good at?",
        "answer": "sports",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is he good at?",
        "answer": "sports",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the subject of the story that is good for the boy?",
        "answer": "sports",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Does Wang Bing enjoy sports?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does he like sports?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does he like sports?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the boy like sports?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What, besides sports, does Wang Bing want to know how to do?",
        "answer": "skating",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else does he want to know how to do?",
        "answer": "skating",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else does he want to know how to do besides play sports?",
        "answer": "skating",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does the boy want to know how to do besides play sports?",
        "answer": "skating",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Is Wang Bing allowed to learn how to skate?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is he allowed to skate?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is he allowed to skate?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is skating allowed for the boy?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Why isn't Wang Bing allowed to go skating?",
        "answer": "to do his homework",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "why is he not allowed to skate?",
        "answer": "to do his homework",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "why is skating forbidden?",
        "answer": "to do his homework",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why is skating forbidden to the boy?",
        "answer": "to do his homework",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Who takes Wang Bing to get ice cream?",
        "answer": "his father",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who meets him for his excursion?",
        "answer": "his father",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who meets him for his excursion?",
        "answer": "his father",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the person that meets up with the boy for his excursion?",
        "answer": "his father",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What does Wang Bing's father want him to do?",
        "answer": "do his homework",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does he want him to?",
        "answer": "do his homework",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does his father want him to skate?",
        "answer": "do his homework",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the father want to meet up with the boy?",
        "answer": "do his homework",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What do Wang Bing and his father do on the drive?",
        "answer": "sees some boys skating",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What do they do on their way?",
        "answer": "sees some boys skating",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What do the boys do on their way to the ice rink?",
        "answer": "sees some boys skating",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What do the boys do on their way?",
        "answer": "sees some boys skating",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Do Wang Bing and his dad get a treat?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "do they get something to consume?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "do the boys get something to consume when they go skating?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do the boys have something to eat on their way?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3qjoxow4xjrtzqo3vwgw8cezi27mer": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-1",
        "original_question": "What is Anna's rank?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-1",
        "answer": "83rd"
      },
      {
        "turn_num": 2,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-2",
        "original_question": "Who she beat?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-2",
        "answer": "Ashleigh Barty"
      },
      {
        "turn_num": 3,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-3",
        "original_question": "What was the score?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-3",
        "answer": "6-2 7-6 (7-4)"
      },
      {
        "turn_num": 4,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-4",
        "original_question": "Who she will be playing next?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-4",
        "answer": "Wozniacki"
      },
      {
        "turn_num": 5,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-5",
        "original_question": "What is her rank?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-5",
        "answer": "No. 1"
      },
      {
        "turn_num": 6,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-6",
        "original_question": "Who she played first?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-6",
        "answer": "Anastasia Rodionova"
      },
      {
        "turn_num": 7,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-7",
        "original_question": "When?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-7",
        "answer": "Monday"
      },
      {
        "turn_num": 8,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-8",
        "original_question": "What was the score?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-8",
        "answer": "6-2 6-1"
      },
      {
        "turn_num": 9,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-9",
        "original_question": "What tournament is this?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-9",
        "answer": "The Australian Open"
      },
      {
        "turn_num": 10,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-10",
        "original_question": "Who were finalists the previous year?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-10",
        "answer": "Kim Clijsters and Li Na"
      },
      {
        "turn_num": 11,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-11",
        "original_question": "Who is joining them in the second round?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-11",
        "answer": "Caroline Wozniacki"
      },
      {
        "turn_num": 12,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-12",
        "original_question": "Who was former top ranked player?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-12",
        "answer": "Clijsters"
      },
      {
        "turn_num": 13,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-13",
        "original_question": "Did she have a good start?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-13",
        "answer": "Yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-14",
        "original_question": "Who did she play?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-14",
        "answer": "Maria Joao Koehler"
      },
      {
        "turn_num": 15,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-15",
        "original_question": "What was the score?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-15",
        "answer": "7-5 6-1"
      },
      {
        "turn_num": 16,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-16",
        "original_question": "Who is France's top ranked player?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-16",
        "answer": "unknown"
      },
      {
        "turn_num": 17,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-17",
        "original_question": "Who is the 11th seed?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-17",
        "answer": "The Belgian"
      },
      {
        "turn_num": 18,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-18",
        "original_question": "Will she continue playing?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-18",
        "answer": "Yes"
      },
      {
        "turn_num": 19,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-19",
        "original_question": "Who had to leave the tournament?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-19",
        "answer": "unknown"
      },
      {
        "turn_num": 20,
        "turn_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-20",
        "original_question": "Who talked to WTA?",
        "original_question_id": "3qjoxow4xjrtzqo3vwgw8cezi27mer-20",
        "answer": "Caroline Wozniacki"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is the rank of Anna Tatishvili?",
        "answer": "83rd",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Anna's rank?",
        "answer": "83rd",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Anna's rank?",
        "answer": "83rd",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Anna's rank?",
        "answer": "83rd",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who did Anna Tatishvili beat?",
        "answer": "Ashleigh Barty",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did Anna beat?",
        "answer": "Ashleigh Barty",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who Anna beat?",
        "answer": "Ashleigh Barty",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Anna beat?",
        "answer": "Ashleigh Barty",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What was the score of the Anna Tatishvili-Ashleigh Barty match?",
        "answer": "6-2 7-6 (7-4)",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the score of Anna's match against Ashleigh Barty?",
        "answer": "6-2 7-6 (7-4)",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the score for Ashleigh Barty?",
        "answer": "6-2 7-6 (7-4)",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the score of Ashleigh Barty's match?",
        "answer": "6-2 7-6 (7-4)",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who will Anna Tatishvili play next?",
        "answer": "Wozniacki",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Anna playing next?",
        "answer": "Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who will be playing next after Ashleigh Barty?",
        "answer": "Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who will Anna play next?",
        "answer": "Wozniacki",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What is the rank of Caroline Wozniacki?",
        "answer": "No. 1",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Wozniacki's rank?",
        "answer": "No. 1",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Anna Wozniacki's rank?",
        "answer": "No. 1",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Anna Wozniacki's rank?",
        "answer": "No. 1",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Who did Caroline Wozniacki play first?",
        "answer": "Anastasia Rodionova",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did Wozniacki play first?",
        "answer": "Anastasia Rodionova",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who played first for Anna?",
        "answer": "Anastasia Rodionova",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the first player to play Anna Wozniacki?",
        "answer": "Anastasia Rodionova",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What was the date of Caroline Wozniacki's match against Anastasia Rodionova?",
        "answer": "Monday",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Anna play against Anastasia Rodionova?",
        "answer": "Monday",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Anna Rodionova play?",
        "answer": "Monday",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Anastasia Rodionova play?",
        "answer": "Monday",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was the score of Caroline Wozniacki's match against Anastasia Rodionova?",
        "answer": "6-2 6-1",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the score of Anna's match against Anastasia Rodionova?",
        "answer": "6-2 6-1",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the score for Anastasia Rodionova?",
        "answer": "6-2 6-1",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the score of Anastasia Rodionova's match against Ashleigh Barty?",
        "answer": "6-2 6-1",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What tournament are the women playing in?",
        "answer": "The Australian Open",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What tournament is this?",
        "answer": "The Australian Open",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What tournament is the Granada Open?",
        "answer": "The Australian Open",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What tournament is Anastasia Rodionova playing in?",
        "answer": "The Australian Open",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who were the last year's finalists in the Australian Open?",
        "answer": "Kim Clijsters and Li Na",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who were the finalists of the previous year's Australian Open?",
        "answer": "Kim Clijsters and Li Na",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who were finalists the previous year at the Australian Open?",
        "answer": "Kim Clijsters and Li Na",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the runner-up in the Australian Open the year before?",
        "answer": "Kim Clijsters and Li Na",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who joins Kim Clijsters and Li Na in the second round?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is joining them in the second round?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is joining Kim Clijsters and Li Na in the second round of the Australian Open?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is joining Kim Clijsters and Li Na in the second round?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Which player used to be top ranked?",
        "answer": "Clijsters",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was former top ranked player?",
        "answer": "Clijsters",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was former top ranked player at the Australian Open?",
        "answer": "Clijsters",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was the former top ranked player?",
        "answer": "Clijsters",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did Kim Clijsters have a good start?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Clijsters have a good start?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Anna Clijsters have a good start at the Australian Open?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Kim Clijsters have a good start?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Who did Kim Clijsters play against?",
        "answer": "Maria Joao Koehler",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did Clijsters play?",
        "answer": "Maria Joao Koehler",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who did Anna Clijsters play in the Australian Open?",
        "answer": "Maria Joao Koehler",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Anastasia Rodionova play?",
        "answer": "Maria Joao Koehler",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "In the match between Kim Clijsters and Maria Joao Koehler, what was the score?",
        "answer": "7-5 6-1",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the score of the match between Kim Clijsters and Maria Joao Koehler?",
        "answer": "7-5 6-1",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the score at the Australian Open?",
        "answer": "7-5 6-1",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Maria Joao Koehler's score?",
        "answer": "7-5 6-1",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What is the name of France's top ranked player?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is France's top ranked player?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is France's top ranked player?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is France's top ranked player?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Who is ranked 11th?",
        "answer": "The Belgian",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is the 11th seed?",
        "answer": "The Belgian",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is the 11th seed at the Australian Open?",
        "answer": "The Belgian",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is the 11th seed in the Australian Open?",
        "answer": "The Belgian",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Will Kim Clijsters keep playing?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Will she continue playing?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Will Anna Joao Koehler continue playing at the Australian Open?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Will Maria Joao Koehler continue playing?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Who was forced to leave the Australian Open?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who had to leave the tournament?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who had to leave the Australian Open?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who had to leave the Australian Open?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Who did WTA interview?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who talked to WTA?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who talked to WTA about Anna Joao Koehler?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who spoke to the WTA?",
        "answer": "Caroline Wozniacki",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3quo65dnquoyop61ycae4yp7zexuoo": {
    "number_of_turns": 11,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-1",
        "original_question": "do genes evolve?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-1",
        "answer": "Yes"
      },
      {
        "turn_num": 2,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-2",
        "original_question": "what are polygenes?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-2",
        "answer": "many different genes"
      },
      {
        "turn_num": 3,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-3",
        "original_question": "are all genetic traits visible?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-3",
        "answer": "No"
      },
      {
        "turn_num": 4,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-4",
        "original_question": "can genes mutate?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-4",
        "answer": "Yes"
      },
      {
        "turn_num": 5,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-5",
        "original_question": "what are different variants of genes called?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-5",
        "answer": "alleles"
      },
      {
        "turn_num": 6,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-6",
        "original_question": "how many specific genetic traits are named?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-6",
        "answer": "Five"
      },
      {
        "turn_num": 7,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-7",
        "original_question": "what is a locus also called?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-7",
        "answer": "Region"
      },
      {
        "turn_num": 8,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-8",
        "original_question": "is eye color visible?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-8",
        "answer": "Yes"
      },
      {
        "turn_num": 9,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-9",
        "original_question": "is blood type visible?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-9",
        "answer": "No"
      },
      {
        "turn_num": 10,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-10",
        "original_question": "are genes related to DNA?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-10",
        "answer": "Yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3quo65dnquoyop61ycae4yp7zexuoo-11",
        "original_question": "what is a gene?",
        "original_question_id": "3quo65dnquoyop61ycae4yp7zexuoo-11",
        "answer": "DNA that encodes a functional RNA or protein product"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Do genes undergo evolution?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "do genes evolve?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "do genes evolve?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do genes evolve?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What is the definition of polygenes?",
        "answer": "many different genes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "polygenes",
        "answer": "many different genes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what are polygenes?",
        "answer": "many different genes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are polygenes?",
        "answer": "many different genes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Are genetic traits always visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "are all genetic traits visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "are all genetic traits visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Are all genetic traits visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Is it possible for genes to mutate?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "can genes mutate?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "can genes mutate?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Can a gene mutate?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What's the name for different variants of genes?",
        "answer": "alleles",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what are different variants of genes called?",
        "answer": "alleles",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what are different variants of genes called?",
        "answer": "alleles",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the different variants of genes called?",
        "answer": "alleles",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "How many genetic traits does the article mention?",
        "answer": "Five",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many specific genetic traits are named?",
        "answer": "Five",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many specific genetic traits are named?",
        "answer": "Five",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many specific genetic traits are named?",
        "answer": "Five",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is another name for a locus?",
        "answer": "Region",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is a locus also called?",
        "answer": "Region",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is a locus also called?",
        "answer": "Region",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of a locus?",
        "answer": "Region",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Is eye color a visible genetic trait?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is eye color visible?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is eye color visible?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is eye color visible?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Is blood type a visible genetic trait?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "is blood type visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "is blood type visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is blood type visible?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Is there a relationship between genes and DNA?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "are genes related to DNA?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "are genes related to DNA?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Are genes related to DNA?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What's the definition of a gene?",
        "answer": "DNA that encodes a functional RNA or protein product",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "what is a gene?",
        "answer": "DNA that encodes a functional RNA or protein product",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what is a gene?",
        "answer": "DNA that encodes a functional RNA or protein product",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is a gene?",
        "answer": "DNA that encodes a functional RNA or protein product",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3qxnc7eipivf1gqfygdci16bnvn90b": {
    "number_of_turns": 13,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-1",
        "original_question": "How many children were there?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-1",
        "answer": "Four."
      },
      {
        "turn_num": 2,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-2",
        "original_question": "What was John's favorite color?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-2",
        "answer": "Blue."
      },
      {
        "turn_num": 3,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-3",
        "original_question": "And Jack's?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-3",
        "answer": "Red."
      },
      {
        "turn_num": 4,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-4",
        "original_question": "Jenny's?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-4",
        "answer": "Purple ."
      },
      {
        "turn_num": 5,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-5",
        "original_question": "And Alice's?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-5",
        "answer": "Yellow."
      },
      {
        "turn_num": 6,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-6",
        "original_question": "Where'd they walk?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-6",
        "answer": "Along the road"
      },
      {
        "turn_num": 7,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-7",
        "original_question": "Who had trouble finding their favorite color?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-7",
        "answer": "John."
      },
      {
        "turn_num": 8,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-8",
        "original_question": "Where'd he look?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-8",
        "answer": "Along the road and by the stream."
      },
      {
        "turn_num": 9,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-9",
        "original_question": "Where they all siblings?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-9",
        "answer": "Yes."
      },
      {
        "turn_num": 10,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-10",
        "original_question": "Who'd they give them to?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-10",
        "answer": "Their mother."
      },
      {
        "turn_num": 11,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-11",
        "original_question": "Where'd John find his flowers?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-11",
        "answer": "The fence near their home."
      },
      {
        "turn_num": 12,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-12",
        "original_question": "Was their Mom angry?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-12",
        "answer": "No."
      },
      {
        "turn_num": 13,
        "turn_id": "3qxnc7eipivf1gqfygdci16bnvn90b-13",
        "original_question": "Where'd she place them?",
        "original_question_id": "3qxnc7eipivf1gqfygdci16bnvn90b-13",
        "answer": "On her kitchen table."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "How many children are in the story?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many children were there?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many children were there?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many children were there?",
        "answer": "Four.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What color did John prefer?",
        "answer": "Blue.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was John's favorite color?",
        "answer": "Blue.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was John's favorite color?",
        "answer": "Blue.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was John's favorite color?",
        "answer": "Blue.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What color did Jack prefer?",
        "answer": "Red.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Jack's favorite color?",
        "answer": "Red.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And Jack's?",
        "answer": "Red.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Jack's favorite color?",
        "answer": "Red.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What color did Jenny prefer?",
        "answer": "Purple .",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Jenny's favorite color?",
        "answer": "Purple .",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Jenny's favorite color?",
        "answer": "Purple .",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Jenny's favorite color?",
        "answer": "Purple .",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What color did Alice prefer?",
        "answer": "Yellow.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Alice's favorite color?",
        "answer": "Yellow.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And Alice's?",
        "answer": "Yellow.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Alice's favorite color?",
        "answer": "Yellow.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Where did the kids walk to?",
        "answer": "Along the road",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where'd they walk?",
        "answer": "Along the road",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where'd the children of the church walk?",
        "answer": "Along the road",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the children go?",
        "answer": "Along the road",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Who had a hard time finding flowers in their favorite color?",
        "answer": "John.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who had trouble finding their favorite color?",
        "answer": "John.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who had trouble finding their favorite color?",
        "answer": "John.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who had trouble finding their favorite color?",
        "answer": "John.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Where did John look for blue flowers?",
        "answer": "Along the road and by the stream.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where'd he look?",
        "answer": "Along the road and by the stream.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where'd John look?",
        "answer": "Along the road and by the stream.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did John look?",
        "answer": "Along the road and by the stream.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Were the children siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where they all siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where were they all siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where were John and Alice's siblings?",
        "answer": "Yes.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who did the kids give their flowers to?",
        "answer": "Their mother.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who'd they give them to?",
        "answer": "Their mother.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who'd John give his siblings to?",
        "answer": "Their mother.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did the children give their siblings to?",
        "answer": "Their mother.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Where did John find blue flowers?",
        "answer": "The fence near their home.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where'd John find his flowers?",
        "answer": "The fence near their home.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where'd John find his flowers?",
        "answer": "The fence near their home.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did John find his flowers?",
        "answer": "The fence near their home.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Was the children's mom angry?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was their Mom angry?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was John's Mom angry?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was John's mom angry?",
        "answer": "No.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Where did the kid's mom put the flowers?",
        "answer": "On her kitchen table.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where'd she place them?",
        "answer": "On her kitchen table.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where'd Mom place the children?",
        "answer": "On her kitchen table.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did John's mom place the flowers?",
        "answer": "On her kitchen table.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3suwzrl0mydran3b8g9fjghds2de6u": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-1",
        "original_question": "How many NOCs were there in 2016?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-1",
        "answer": "206"
      },
      {
        "turn_num": 2,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-2",
        "original_question": "What does NOC stand for?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-2",
        "answer": "National Olympic Committee"
      },
      {
        "turn_num": 3,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-3",
        "original_question": "So is there only one NOC per nation?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-3",
        "answer": "unknown"
      },
      {
        "turn_num": 4,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-4",
        "original_question": "Can you name two NOCs?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-4",
        "answer": "United Nations observer state Palestine and the Cook Islands"
      },
      {
        "turn_num": 5,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-5",
        "original_question": "What is one thing an NOC can do?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-5",
        "answer": "organizing their people's participation in the Olympic Games"
      },
      {
        "turn_num": 6,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-6",
        "original_question": "Anything else?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-6",
        "answer": "They nominate cities within their respective areas as candidates for future Olympic Games"
      },
      {
        "turn_num": 7,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-7",
        "original_question": "Is there an entity that controls NOCs?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-7",
        "answer": "Yes"
      },
      {
        "turn_num": 8,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-8",
        "original_question": "What is it called?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-8",
        "answer": "International Olympic Committee"
      },
      {
        "turn_num": 9,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-9",
        "original_question": "How many dependent territories have NOCs?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-9",
        "answer": "nine"
      },
      {
        "turn_num": 10,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-10",
        "original_question": "Was there an amendment to the Olympic Charter?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-10",
        "answer": "Yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-11",
        "original_question": "When?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-11",
        "answer": "1996"
      },
      {
        "turn_num": 12,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-12",
        "original_question": "Was it good retroactively?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-12",
        "answer": "no"
      },
      {
        "turn_num": 13,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-13",
        "original_question": "Did it have to do with recognition as an independent state?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-13",
        "answer": "Yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-14",
        "original_question": "Could those 9 dependent territories keep participating?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-14",
        "answer": "Yes"
      },
      {
        "turn_num": 15,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-15",
        "original_question": "What kind of team does the Faroe Islands send?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-15",
        "answer": "Paralympic"
      },
      {
        "turn_num": 16,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-16",
        "original_question": "What kind of team does Macau send?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-16",
        "answer": "Paralympic"
      },
      {
        "turn_num": 17,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-17",
        "original_question": "How many member states does the Uniter Nations have?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-17",
        "answer": "193"
      },
      {
        "turn_num": 18,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-18",
        "original_question": "Can you name a United Nations observer state?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-18",
        "answer": "Palestine"
      },
      {
        "turn_num": 19,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-19",
        "original_question": "Do the Cook Islands have something to do with New Zealand?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-19",
        "answer": "Yes"
      },
      {
        "turn_num": 20,
        "turn_id": "3suwzrl0mydran3b8g9fjghds2de6u-20",
        "original_question": "What's another name for Taiwan?",
        "original_question_id": "3suwzrl0mydran3b8g9fjghds2de6u-20",
        "answer": "Chinese Taipei"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "As of 2016, how many National Olympic Committeess existed?",
        "answer": "206",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many NOCs were there in 2016?",
        "answer": "206",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many NOCs were there in 2016?",
        "answer": "206",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many NOCs were there in 2016?",
        "answer": "206",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What is NOC short for?",
        "answer": "National Olympic Committee",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does NOC stand for?",
        "answer": "National Olympic Committee",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does NOC stand for?",
        "answer": "National Olympic Committee",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is NOC short for?",
        "answer": "National Olympic Committee",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Does each country have just one National Olympic Committee?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is there only one NOC per nation?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "So is there only one NOC per nation?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is there only one NOC per nation?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What are two National Olympic Committees?",
        "answer": "United Nations observer state Palestine and the Cook Islands",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can you name two NOCs?",
        "answer": "United Nations observer state Palestine and the Cook Islands",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Can you name two NOCs?",
        "answer": "United Nations observer state Palestine and the Cook Islands",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What are the names of two NOCs?",
        "answer": "United Nations observer state Palestine and the Cook Islands",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What is one thing a National Olympic Committee can do?",
        "answer": "organizing their people's participation in the Olympic Games",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What can an NOC do?",
        "answer": "organizing their people's participation in the Olympic Games",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is one thing an NOC can do?",
        "answer": "organizing their people's participation in the Olympic Games",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is one thing that an NOC can do?",
        "answer": "organizing their people's participation in the Olympic Games",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What is one thing a National Olympic Committee can do, besides organizing?",
        "answer": "They nominate cities within their respective areas as candidates for future Olympic Games",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else can an NOC do?",
        "answer": "They nominate cities within their respective areas as candidates for future Olympic Games",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Anything else about the NOCs besides organizing their people's participation in the Olympic Games?",
        "answer": "They nominate cities within their respective areas as candidates for future Olympic Games",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can an NOC do other than organize their people's participation in the Olympic Games?",
        "answer": "They nominate cities within their respective areas as candidates for future Olympic Games",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Is there an organization that controls National Olympic Committee?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is there an entity that controls NOCs?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is there an entity that controls NOCs?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is there a body that controls NOCs?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Who controls National Olympic Committees?",
        "answer": "International Olympic Committee",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the entity that controls NOCs?",
        "answer": "International Olympic Committee",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the NOC called?",
        "answer": "International Olympic Committee",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the entity that controls NOCs?",
        "answer": "International Olympic Committee",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "How many dependent territories have National Olympic Committees?",
        "answer": "nine",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many dependent territories have NOCs?",
        "answer": "nine",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many dependent territories have NOCs?",
        "answer": "nine",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many dependent territories have NOCs?",
        "answer": "nine",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did the Olympic Charter get amended?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was there an amendment to the Olympic Charter?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was there an amendment to the Olympic Charter?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was there an amendment to the Olympic Charter?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "When was the Olympic Charter amended?",
        "answer": "1996",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the amendment made to the Olympic Charter?",
        "answer": "1996",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did the International Olympic Charter become effective?",
        "answer": "1996",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Olympic Charter get amended?",
        "answer": "1996",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did the Olympic Charter's amendment apply retroactively?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the amendment to the Olympic Charter in 1996 applied retroactively?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the 1996 Olympic Charter good retroactively?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the 1996 amendment to the Olympic Charter good retroactively?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did the Olympic Charter's amendment concern recognition of independent states?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did the amendment to the Olympic Charter in 1996 have to do with recognition as an independent state?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the 1996 Olympic Charter have to do with recognition as an independent state?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the amendment to the Olympic Charter have to do with recognition as an independent state?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Did the amendment allow for the 9 dependent territories with National Olympic Committees to keep participating?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Could those 9 dependent territories keep participating?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Could those 9 dependent territories keep participating in the 2016 Olympic Games?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Could the 9 dependent territories keep participating in the 2016 Olympic Games?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What sort of team comes from the Faroe Islands?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of team does the Faroe Islands send?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of team do the Faroe Islands send?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of team does the Faroe Islands send?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What sort of team comes from Macau?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of team does Macau send to the Olympics?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of team does Macau send?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of team does Macau send?",
        "answer": "Paralympic",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "How many member states are there in the United Nations?",
        "answer": "193",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many member states does the United Nations have?",
        "answer": "193",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many member states does the Uniter Nations have?",
        "answer": "193",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many member states does the United Nations have?",
        "answer": "193",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What's one of the United Nations observer states?",
        "answer": "Palestine",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can you name a United Nations observer state?",
        "answer": "Palestine",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Can you name a United Nations observer state?",
        "answer": "Palestine",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of a United Nations observer state?",
        "answer": "Palestine",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Are the Cook Islands related to New Zealand?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Do the Cook Islands have something to do with New Zealand?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Do the Cook Islands have something to do with New Zealand?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do the Cook Islands have something to do with New Zealand?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "How else is Taiwan referred to?",
        "answer": "Chinese Taipei",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What's another name for Taiwan?",
        "answer": "Chinese Taipei",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What's another name for Taiwan?",
        "answer": "Chinese Taipei",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Taiwan's other name?",
        "answer": "Chinese Taipei",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix": {
    "number_of_turns": 14,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-1",
        "original_question": "Who woke up?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-1",
        "answer": "Little Tommy"
      },
      {
        "turn_num": 2,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-2",
        "original_question": "What was happening?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-2",
        "answer": "the sun shining and the birds singing"
      },
      {
        "turn_num": 3,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-3",
        "original_question": "Who sat outside?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-3",
        "answer": "a bird"
      },
      {
        "turn_num": 4,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-4",
        "original_question": "What was it's name?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-4",
        "answer": "Martha"
      },
      {
        "turn_num": 5,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-5",
        "original_question": "Did he give her anything?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-5",
        "answer": "yes"
      },
      {
        "turn_num": 6,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-6",
        "original_question": "What?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-6",
        "answer": "some bread"
      },
      {
        "turn_num": 7,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-7",
        "original_question": "Who did he see next?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-7",
        "answer": "Sammy"
      },
      {
        "turn_num": 8,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-8",
        "original_question": "What was he?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-8",
        "answer": "a big hairy dog"
      },
      {
        "turn_num": 9,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-9",
        "original_question": "Where did they go?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-9",
        "answer": "fishing"
      },
      {
        "turn_num": 10,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-10",
        "original_question": "Where?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-10",
        "answer": "the fishing hole"
      },
      {
        "turn_num": 11,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-11",
        "original_question": "Did he bring anything?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-11",
        "answer": "yes"
      },
      {
        "turn_num": 12,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-12",
        "original_question": "What?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-12",
        "answer": "fishing pole, some worms, and a little lunch"
      },
      {
        "turn_num": 13,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-13",
        "original_question": "Did they find anything?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-13",
        "answer": "yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-14",
        "original_question": "What?",
        "original_question_id": "3tdxmtx3cbu3qs5x4zz64vf5jpz6ix-14",
        "answer": "a huge green toad."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who got up in the morning?",
        "answer": "Little Tommy",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who woke up?",
        "answer": "Little Tommy",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who woke up?",
        "answer": "Little Tommy",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who woke up?",
        "answer": "Little Tommy",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was going on in the morning?",
        "answer": "the sun shining and the birds singing",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was happening?",
        "answer": "the sun shining and the birds singing",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was happening to Little Tommy?",
        "answer": "the sun shining and the birds singing",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was happening to Little Tommy?",
        "answer": "the sun shining and the birds singing",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who was sitting outside?",
        "answer": "a bird",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who sat outside?",
        "answer": "a bird",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who sat outside with Little Tommy?",
        "answer": "a bird",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who sat outside?",
        "answer": "a bird",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What was the bird called?",
        "answer": "Martha",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was it's name?",
        "answer": "Martha",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the bird's name?",
        "answer": "Martha",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the bird called?",
        "answer": "Martha",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did Tommy give anything to the bird?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he give her anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Little Tommy give Martha anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Little Tommy give Martha anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What did Tommy give Martha?",
        "answer": "some bread",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he give her anything?",
        "answer": "some bread",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Little Tommy give her?",
        "answer": "some bread",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Little Tommy give Martha?",
        "answer": "some bread",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Who did Tommy see after Martha?",
        "answer": "Sammy",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who did he see next?",
        "answer": "Sammy",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who did Tommy see next after the birds?",
        "answer": "Sammy",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did Little Tommy see next?",
        "answer": "Sammy",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was Sammy?",
        "answer": "a big hairy dog",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Sammy?",
        "answer": "a big hairy dog",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Sammy?",
        "answer": "a big hairy dog",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Sammy?",
        "answer": "a big hairy dog",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Where did Tommy and Sammy go?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did they go?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the little hairy dog go?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Tommy and Sammy go?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Where did Tommy and Sammy go fishing?",
        "answer": "the fishing hole",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did they go fishing?",
        "answer": "the fishing hole",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Sammy and Martha go fishing?",
        "answer": "the fishing hole",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Tommy and Sammy go fishing?",
        "answer": "the fishing hole",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Did Tommy bring anything fishing with him?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he bring anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Sammy bring anything to the fishing hole?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sammy bring anything to the fishing hole?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What supplies did Tommy bring fishing?",
        "answer": "fishing pole, some worms, and a little lunch",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he bring anything?",
        "answer": "fishing pole, some worms, and a little lunch",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Little Tommy bring to the fishing hole?",
        "answer": "fishing pole, some worms, and a little lunch",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Sammy bring to the fishing hole?",
        "answer": "fishing pole, some worms, and a little lunch",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did Tommy and Sammy find anything while fishing?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they find anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the little hairy dog find anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Sammy and Martha find anything?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What did Sammy and Tommy find at the fishing hole?",
        "answer": "a huge green toad.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they find anything?",
        "answer": "a huge green toad.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Little Tommy find?",
        "answer": "a huge green toad.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Little Tommy find?",
        "answer": "a huge green toad.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3txd01zld4hukwwjfsv5q0j2ixa4ul": {
    "number_of_turns": 13,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-1",
        "original_question": "Who was excommunicated?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-1",
        "answer": "Savonarola"
      },
      {
        "turn_num": 2,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-2",
        "original_question": "Was he a heretic?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-2",
        "answer": "no"
      },
      {
        "turn_num": 3,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-3",
        "original_question": "Was he superstitious?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-3",
        "answer": "no"
      },
      {
        "turn_num": 4,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-4",
        "original_question": "Why was he excommunicated then?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-4",
        "answer": "unknown"
      },
      {
        "turn_num": 5,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-5",
        "original_question": "Who was inspired by this man?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-5",
        "answer": "Romola"
      },
      {
        "turn_num": 6,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-6",
        "original_question": "Where was the Excommunication published?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-6",
        "answer": "in the Duomo"
      },
      {
        "turn_num": 7,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-7",
        "original_question": "When?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-7",
        "answer": "June"
      },
      {
        "turn_num": 8,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-8",
        "original_question": "How long was it published?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-8",
        "answer": "for some weeks"
      },
      {
        "turn_num": 9,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-9",
        "original_question": "What was Romola looking for?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-9",
        "answer": "a sign from Baldassarre"
      },
      {
        "turn_num": 10,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-10",
        "original_question": "What else was she looking for?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-10",
        "answer": "sympathy with Savonarola"
      },
      {
        "turn_num": 11,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-11",
        "original_question": "What did the summer days bring?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-11",
        "answer": "Plague"
      },
      {
        "turn_num": 12,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-12",
        "original_question": "Who was she alienated from?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-12",
        "answer": "the Frate"
      },
      {
        "turn_num": 13,
        "turn_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-13",
        "original_question": "Did Romola fight the plague?",
        "original_question_id": "3txd01zld4hukwwjfsv5q0j2ixa4ul-13",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who got excommunicated from the church?",
        "answer": "Savonarola",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was excommunicated?",
        "answer": "Savonarola",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was excommunicated?",
        "answer": "Savonarola",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was excommunicated?",
        "answer": "Savonarola",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Had Savonarola committed heresy?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Savonarola a heretic?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Savonarola a heretic?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Savonarola a heretic?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Did Savonarola dabble in superstitions?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Savonarola superstitious?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Savonarola superstitious?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Savonarola superstitious?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Why was Savonarola excommunicated from the church?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why was Savonarola excommunicated?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why was Savonarola excommunicated then?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why was Savonarola excommunicated?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who had Savonarola inspired?",
        "answer": "Romola",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was inspired by Savonarola?",
        "answer": "Romola",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was inspired by Savonarola?",
        "answer": "Romola",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was inspired by Savonarola?",
        "answer": "Romola",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "In what publication could the excommunication of Savonarola be found?",
        "answer": "in the Duomo",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was the Excommunication published?",
        "answer": "in the Duomo",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was the Excommunication by Savonarola published?",
        "answer": "in the Duomo",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was the Excommunication of Savonarola published?",
        "answer": "in the Duomo",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "In what month was Savonarola's excommunciation published?",
        "answer": "June",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the Excommunication published?",
        "answer": "June",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the Excommunication published?",
        "answer": "June",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did the Duomo publish the Excommunication?",
        "answer": "June",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "For how long did Savonarola's excommunication remain published?",
        "answer": "for some weeks",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long was the Excommunication published?",
        "answer": "for some weeks",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long was the Excommunication by Savonarola published?",
        "answer": "for some weeks",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long was the Excommunication published?",
        "answer": "for some weeks",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What did Romola want to find?",
        "answer": "a sign from Baldassarre",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Romola looking for?",
        "answer": "a sign from Baldassarre",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was Savonarola looking for in the Excommunication?",
        "answer": "a sign from Baldassarre",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Romola looking for?",
        "answer": "a sign from Baldassarre",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What did Romola think the resistance could invigorate?",
        "answer": "sympathy with Savonarola",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else was Romola looking for?",
        "answer": "sympathy with Savonarola",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else was Savonarola looking for besides a sign from Baldassarre?",
        "answer": "sympathy with Savonarola",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was Romola looking for, besides a sign from Baldassarre?",
        "answer": "sympathy with Savonarola",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What disease arrived in the summer?",
        "answer": "Plague",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the summer days bring?",
        "answer": "Plague",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the summer days bring for Savonarola?",
        "answer": "Plague",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the summer days bring?",
        "answer": "Plague",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "From what did Romola feel alienated?",
        "answer": "the Frate",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was she alienated from?",
        "answer": "the Frate",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was Savonarola alienated from?",
        "answer": "the Frate",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Savonarola alienated from?",
        "answer": "the Frate",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Was Romola fighting against the plague?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Romola fight the plague?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Savonarola fight the plague?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Romola fight the plague?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3v26sbztbder9sei68k31obqk4qzz2": {
    "number_of_turns": 11,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-1",
        "original_question": "Where did Erin live?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-1",
        "answer": "England"
      },
      {
        "turn_num": 2,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-2",
        "original_question": "Who was her friend?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-2",
        "answer": "Kevin"
      },
      {
        "turn_num": 3,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-3",
        "original_question": "What country did she travel to?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-3",
        "answer": "United States"
      },
      {
        "turn_num": 4,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-4",
        "original_question": "Why?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-4",
        "answer": "to see the sights"
      },
      {
        "turn_num": 5,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-5",
        "original_question": "What cities?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-5",
        "answer": "New York City"
      },
      {
        "turn_num": 6,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-6",
        "original_question": "How did they feel to see each other?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-6",
        "answer": "she ran up to him and jumped into his arms, giving him a great big hug"
      },
      {
        "turn_num": 7,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-7",
        "original_question": "What did they end up eating in the morning?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-7",
        "answer": "pancakes"
      },
      {
        "turn_num": 8,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-8",
        "original_question": "Where did they visit in NYC?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-8",
        "answer": "Empire State building and  the Statue of Liberty and Central Park"
      },
      {
        "turn_num": 9,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-9",
        "original_question": "How did she feel towards the end?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-9",
        "answer": "sad"
      },
      {
        "turn_num": 10,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-10",
        "original_question": "Did she feel better?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-10",
        "answer": "Yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3v26sbztbder9sei68k31obqk4qzz2-11",
        "original_question": "What did they come up with?",
        "original_question_id": "3v26sbztbder9sei68k31obqk4qzz2-11",
        "answer": "came up with a plan for the months ahead"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What country did Erin live in?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did Erin live?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Erin live?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Erin live?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who was Erin's friend?",
        "answer": "Kevin",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was Erin's friend?",
        "answer": "Kevin",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was Erin's friend?",
        "answer": "Kevin",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Erin's friend?",
        "answer": "Kevin",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What country did Erin travel to?",
        "answer": "United States",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What country did Erin travel to?",
        "answer": "United States",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What country did Erin travel to?",
        "answer": "United States",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What country did Erin travel to?",
        "answer": "United States",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Why did Erin visit the United States?",
        "answer": "to see the sights",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Why did Erin travel to the United States?",
        "answer": "to see the sights",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Why did Erin travel to the United States?",
        "answer": "to see the sights",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Why did Erin travel to the United States?",
        "answer": "to see the sights",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What American cities did Erin visit?",
        "answer": "New York City",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What cities did Erin travel to?",
        "answer": "New York City",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What cities did Erin travel to?",
        "answer": "New York City",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What cities did Erin travel to?",
        "answer": "New York City",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "How did Erin and Kevin feel about seeing each other?",
        "answer": "she ran up to him and jumped into his arms, giving him a great big hug",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did they feel to see each other?",
        "answer": "she ran up to him and jumped into his arms, giving him a great big hug",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Erin and Kevin feel to see each other in New York City?",
        "answer": "she ran up to him and jumped into his arms, giving him a great big hug",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the feeling of Erin and Kevin's meeting?",
        "answer": "she ran up to him and jumped into his arms, giving him a great big hug",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What did Erin and Kevin eat in the morning?",
        "answer": "pancakes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they end up eating in the morning?",
        "answer": "pancakes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Erin and Kevin end up eating in the morning?",
        "answer": "pancakes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Erin and Kevin end up eating in the morning?",
        "answer": "pancakes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Where in NYC did Erin and Kevin go?",
        "answer": "Empire State building and  the Statue of Liberty and Central Park",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did they visit in NYC?",
        "answer": "Empire State building and  the Statue of Liberty and Central Park",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did Erin and Kevin visit in NYC?",
        "answer": "Empire State building and  the Statue of Liberty and Central Park",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What city did Erin and Kevin visit in New York City?",
        "answer": "Empire State building and  the Statue of Liberty and Central Park",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "How did Erin feel towards the end of her trip?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did she feel towards the end?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Erin feel towards the end of the movie?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did Erin feel towards the end of the trip?",
        "answer": "sad",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did Erin feel better at the end of her trip?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did she feel better?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Erin feel better after visiting the Empire State Building and the Statue of Liberty?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Erin feel better towards the end of the story?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What did Erin and Kevin come up with?",
        "answer": "came up with a plan for the months ahead",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they come up with?",
        "answer": "came up with a plan for the months ahead",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Erin and Kevin come up with?",
        "answer": "came up with a plan for the months ahead",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Erin and Kevin come up with?",
        "answer": "came up with a plan for the months ahead",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3vd82fohkqo22vp1clpeas31spqcoi": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-1",
        "original_question": "What does NATO stand for",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-1",
        "answer": "The North Atlantic Treaty Organization"
      },
      {
        "turn_num": 2,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-2",
        "original_question": "What else can it be called",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-2",
        "answer": "North Atlantic Alliance"
      },
      {
        "turn_num": 3,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-3",
        "original_question": "What is it",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-3",
        "answer": "a military alliance"
      },
      {
        "turn_num": 4,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-4",
        "original_question": "between who",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-4",
        "answer": "North American and European states"
      },
      {
        "turn_num": 5,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-5",
        "original_question": "When was it signed",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-5",
        "answer": "4\u00a0April 1949."
      },
      {
        "turn_num": 6,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-6",
        "original_question": "who are the permanent members of NATO",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-6",
        "answer": "the United States, France and the United Kingdom"
      },
      {
        "turn_num": 7,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-7",
        "original_question": "what power do they have",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-7",
        "answer": "veto"
      },
      {
        "turn_num": 8,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-8",
        "original_question": "do they have nuclear weapons",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-8",
        "answer": "yes"
      },
      {
        "turn_num": 9,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-9",
        "original_question": "Where are the headquarters of NATO",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-9",
        "answer": "Haren, Brussels, Belgium"
      },
      {
        "turn_num": 10,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-10",
        "original_question": "where is the ACOs",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-10",
        "answer": "near Mons, Belgium."
      },
      {
        "turn_num": 11,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-11",
        "original_question": "how many independent countries are in NATO",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-11",
        "answer": "29"
      },
      {
        "turn_num": 12,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-12",
        "original_question": "How many additional for peace program",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-12",
        "answer": "21"
      },
      {
        "turn_num": 13,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-13",
        "original_question": "how many for dialogue",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-13",
        "answer": "15"
      },
      {
        "turn_num": 14,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-14",
        "original_question": "So how many altogether",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-14",
        "answer": "65"
      },
      {
        "turn_num": 15,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-15",
        "original_question": "how much do they spend on military out of all the money",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-15",
        "answer": "over 70%"
      },
      {
        "turn_num": 16,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-16",
        "original_question": "what war led to a rivalry",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-16",
        "answer": "the Cold War"
      },
      {
        "turn_num": 17,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-17",
        "original_question": "with what nations",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-17",
        "answer": "nations of the Warsaw Pac"
      },
      {
        "turn_num": 18,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-18",
        "original_question": "in what year",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-18",
        "answer": "1955"
      },
      {
        "turn_num": 19,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-19",
        "original_question": "Did some of those nations ever join NATO",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-19",
        "answer": "yes"
      },
      {
        "turn_num": 20,
        "turn_id": "3vd82fohkqo22vp1clpeas31spqcoi-20",
        "original_question": "In what years",
        "original_question_id": "3vd82fohkqo22vp1clpeas31spqcoi-20",
        "answer": "1999 and 2004."
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What is NATO short for?",
        "answer": "The North Atlantic Treaty Organization",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does NATO stand for",
        "answer": "The North Atlantic Treaty Organization",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does NATO stand for",
        "answer": "The North Atlantic Treaty Organization",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does NATO stand for?",
        "answer": "The North Atlantic Treaty Organization",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What else can NATO be called?",
        "answer": "North Atlantic Alliance",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else can NATO be called?",
        "answer": "North Atlantic Alliance",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else can NATO be called",
        "answer": "North Atlantic Alliance",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What else can NATO be called?",
        "answer": "North Atlantic Alliance",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What is NATO?",
        "answer": "a military alliance",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is NATO?",
        "answer": "a military alliance",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the North Atlantic Treaty Organization",
        "answer": "a military alliance",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the North Atlantic Alliance?",
        "answer": "a military alliance",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who is a part of the NATO military alliance?",
        "answer": "North American and European states",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is NATO?",
        "answer": "North American and European states",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "between who is the North Atlantic Treaty Organization and the NATO",
        "answer": "North American and European states",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who is NATO a military alliance between?",
        "answer": "North American and European states",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "When was NATO signed?",
        "answer": "4\u00a0April 1949.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was NATO signed?",
        "answer": "4\u00a0April 1949.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the North Atlantic Treaty Organization signed",
        "answer": "4\u00a0April 1949.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was the North Atlantic Treaty Organization signed?",
        "answer": "4\u00a0April 1949.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Who are NATO's permanent members?",
        "answer": "the United States, France and the United Kingdom",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "permanent members of NATO",
        "answer": "the United States, France and the United Kingdom",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who are the permanent members of NATO",
        "answer": "the United States, France and the United Kingdom",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "who are the permanent members of NATO",
        "answer": "the United States, France and the United Kingdom",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What do the permanent members of NATO have the power to do?",
        "answer": "veto",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What power do the permanent members of NATO have?",
        "answer": "veto",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what power do the United States, France and the United Kingdom have",
        "answer": "veto",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What power do the United States, France and the United Kingdom have?",
        "answer": "veto",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Do the permanent members of NATO have nuclear weapons?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "do permanent members of NATO have nuclear weapons",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "do the United States, France and the United Kingdom have nuclear weapons",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Do the United States, France and the United Kingdom have nuclear weapons",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Where is NATO based out of?",
        "answer": "Haren, Brussels, Belgium",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where are the headquarters of NATO",
        "answer": "Haren, Brussels, Belgium",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where are the headquarters of NATO",
        "answer": "Haren, Brussels, Belgium",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where are NATO's headquarters?",
        "answer": "Haren, Brussels, Belgium",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Where is the ACOs based out of?",
        "answer": "near Mons, Belgium.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where is the ACOs?",
        "answer": "near Mons, Belgium.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "where is the northatlantic alliance",
        "answer": "near Mons, Belgium.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where can the ACOs be found?",
        "answer": "near Mons, Belgium.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What is the number of independent countries in NATO?",
        "answer": "29",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many independent countries are in NATO",
        "answer": "29",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many independent countries are in NATO",
        "answer": "29",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many independent countries are in NATO",
        "answer": "29",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "How many additional countries are in the NATO's Partnership for Peace program?",
        "answer": "21",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many additional countries are in the NATO Partnership for Peace program?",
        "answer": "21",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many additional countries are in NATO for peace program",
        "answer": "21",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many countries are in NATO in addition to the United States and France?",
        "answer": "21",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "How many countries are involved in institutionalized dialogue programs?",
        "answer": "15",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "how many countries are in NATO's dialogue program?",
        "answer": "15",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how many for peace program are in the nato",
        "answer": "15",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many countries are in NATO for dialogue",
        "answer": "15",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is the total number of countries that participate in NATO?",
        "answer": "65",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many countries are in NATO altogether?",
        "answer": "65",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "So how many altogether are in the NATO",
        "answer": "65",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many independent countries are in NATO?",
        "answer": "65",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What percent of NATO's spending is military related?",
        "answer": "over 70%",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the total military spending of NATO countries?",
        "answer": "over 70%",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "how much do the United States spend on military out of all the money they have",
        "answer": "over 70%",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Out of all the money that NATO has, how much does it spend on military?",
        "answer": "over 70%",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What war caused a rivarly?",
        "answer": "the Cold War",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What war led to a rivalry?",
        "answer": "the Cold War",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "what war led to a rivalry between the United States and NATO",
        "answer": "the Cold War",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What war led to a rivalry between the North American and European states?",
        "answer": "the Cold War",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Which nations did the Cold War cause a rivalry between?",
        "answer": "nations of the Warsaw Pac",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What war led to a rivalry?",
        "answer": "nations of the Warsaw Pac",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "with what nations is the cold war a rivalry",
        "answer": "nations of the Warsaw Pac",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What nations were involved in the Cold War?",
        "answer": "nations of the Warsaw Pac",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "In what year was the Warsaw Pact created?",
        "answer": "1955",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What year did the Cold War start?",
        "answer": "1955",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "in what year did the u.s. join the nato",
        "answer": "1955",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "In what year did the Cold War occur?",
        "answer": "1955",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Did some of the Warsaw Pact countries join NATO?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did some of those nations ever join NATO",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did some of the countries of the Warsaw Pac ever join NATO",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did some of the countries in the Warsaw Pact ever join NATO",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "When did some nations of the Warsaw Pact join NATO?",
        "answer": "1999 and 2004.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In what years did some of those nations join NATO?",
        "answer": "1999 and 2004.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In what years did some of the NATO nations join",
        "answer": "1999 and 2004.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "In what years did some of the NATO members join the alliance?",
        "answer": "1999 and 2004.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3vhhr074h3hoktr88c1b2p7tv3cl7m": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-1",
        "original_question": "who sailed?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-1",
        "answer": "A man"
      },
      {
        "turn_num": 2,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-2",
        "original_question": "What was he in/",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-2",
        "answer": "a small boat."
      },
      {
        "turn_num": 3,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-3",
        "original_question": "What was the sun doing?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-3",
        "answer": "rising"
      },
      {
        "turn_num": 4,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-4",
        "original_question": "What did he like  to do?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-4",
        "answer": "fishing"
      },
      {
        "turn_num": 5,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-5",
        "original_question": "Where did he stop the boat?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-5",
        "answer": "A long way away from the shore"
      },
      {
        "turn_num": 6,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-6",
        "original_question": "What did he put on the hook?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-6",
        "answer": "some worms"
      },
      {
        "turn_num": 7,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-7",
        "original_question": "Did he get a bite?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-7",
        "answer": "no"
      },
      {
        "turn_num": 8,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-8",
        "original_question": "Did he fish for a long time?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-8",
        "answer": "yes"
      },
      {
        "turn_num": 9,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-9",
        "original_question": "How did the sun move?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-9",
        "answer": "slowly"
      },
      {
        "turn_num": 10,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-10",
        "original_question": "What did he live in?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-10",
        "answer": "log cabin"
      },
      {
        "turn_num": 11,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-11",
        "original_question": "Did he sit by a fire?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-11",
        "answer": "yes"
      },
      {
        "turn_num": 12,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-12",
        "original_question": "How long did he fih for?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-12",
        "answer": "all day"
      },
      {
        "turn_num": 13,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-13",
        "original_question": "Did he stop fishing easily?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-13",
        "answer": "no"
      },
      {
        "turn_num": 14,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-14",
        "original_question": "Was he hungry?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-14",
        "answer": "yes"
      },
      {
        "turn_num": 15,
        "turn_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-15",
        "original_question": "What did he say about the next day?",
        "original_question_id": "3vhhr074h3hoktr88c1b2p7tv3cl7m-15",
        "answer": "would be a better day"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who went out on a boat?",
        "answer": "A man",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who sailed?",
        "answer": "A man",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "who sailed?",
        "answer": "A man",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who sailed?",
        "answer": "A man",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was the man in?",
        "answer": "a small boat.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "who sailed?",
        "answer": "a small boat.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was a man in/ on the sailed?",
        "answer": "a small boat.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the man in?",
        "answer": "a small boat.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "How was the sun?",
        "answer": "rising",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the sun doing?",
        "answer": "rising",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the sun doing on the sand dunes?",
        "answer": "rising",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the sun doing?",
        "answer": "rising",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What did the man like to do?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he like to do?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the sun like to do?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the man like to do?",
        "answer": "fishing",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Where did the man stop his boat?",
        "answer": "A long way away from the shore",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where did he stop the boat?",
        "answer": "A long way away from the shore",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where did the sun stop the boat?",
        "answer": "A long way away from the shore",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did the man stop the boat?",
        "answer": "A long way away from the shore",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What did the man attach to his hook?",
        "answer": "some worms",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he put on the hook?",
        "answer": "some worms",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the man put on the hook?",
        "answer": "some worms",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the man put on the hook?",
        "answer": "some worms",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Did the man get any fish to bite?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he get a bite?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the man get a bite from the worms?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the man bite the worms?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Did the man fish for a long time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he fish for a long time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the man fish for a long time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the man fish for a long time?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "How was the sun move?",
        "answer": "slowly",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did the sun move?",
        "answer": "slowly",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did the sun move?",
        "answer": "slowly",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How did the sun move?",
        "answer": "slowly",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What did the man live in?",
        "answer": "log cabin",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he live in?",
        "answer": "log cabin",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the sun live in?",
        "answer": "log cabin",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the man's home?",
        "answer": "log cabin",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Did the man sit by a fire?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he sit by a fire?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the sun sit by a fire?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the man sit by a fire?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "How much time did the man spend fishing?",
        "answer": "all day",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long did he fish for?",
        "answer": "all day",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long did the sun fly for?",
        "answer": "all day",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long did the man spend by the fire?",
        "answer": "all day",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did the man give up easily at fishing?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he stop fishing easily?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the man stop fishing easily?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did the man stop fishing easily?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Was the man hungry?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was he hungry?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the man hungry?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the man hungry?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "How did the man feel towards the next day?",
        "answer": "would be a better day",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did he say about the next day?",
        "answer": "would be a better day",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did the man say about the next day when he stopped fishing?",
        "answer": "would be a better day",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the man say about the next day?",
        "answer": "would be a better day",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3vp0c6efsgwpmbvopexywomm139m6u": {
    "number_of_turns": 15,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-1",
        "original_question": "Who gaped at Hendry?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-1",
        "answer": "Peter"
      },
      {
        "turn_num": 2,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-2",
        "original_question": "How many daughters were there?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-2",
        "answer": "five daughters"
      },
      {
        "turn_num": 3,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-3",
        "original_question": "Whose man was dead at home?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-3",
        "answer": "Bell Christison's"
      },
      {
        "turn_num": 4,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-4",
        "original_question": "Who were in church?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-4",
        "answer": "the Auld Lichts"
      },
      {
        "turn_num": 5,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-5",
        "original_question": "What were they waiting for?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-5",
        "answer": "for their minister"
      },
      {
        "turn_num": 6,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-6",
        "original_question": "Was everyone there to ask for rain?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-6",
        "answer": "no"
      },
      {
        "turn_num": 7,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-7",
        "original_question": "Who was a simple woman?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-7",
        "answer": "Meggy Rattray"
      },
      {
        "turn_num": 8,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-8",
        "original_question": "What were all the women doing?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-8",
        "answer": "wondering"
      },
      {
        "turn_num": 9,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-9",
        "original_question": "And what were most of the men?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-9",
        "answer": "farmers"
      },
      {
        "turn_num": 10,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-10",
        "original_question": "What did Hendry Munn do?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-10",
        "answer": "signed to Peter Tosh"
      },
      {
        "turn_num": 11,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-11",
        "original_question": "Who dove for his hat?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-11",
        "answer": "Peter"
      },
      {
        "turn_num": 12,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-12",
        "original_question": "Then where did he go?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-12",
        "answer": "the vestry"
      },
      {
        "turn_num": 13,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-13",
        "original_question": "Was the church full when they were waiting?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-13",
        "answer": "yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-14",
        "original_question": "Where was Charles Yuill?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-14",
        "answer": "in his pew"
      },
      {
        "turn_num": 15,
        "turn_id": "3vp0c6efsgwpmbvopexywomm139m6u-15",
        "original_question": "Did Bell Christison care how things went?",
        "original_question_id": "3vp0c6efsgwpmbvopexywomm139m6u-15",
        "answer": "no"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "Who stared at Hendry slack jawed?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who gaped at Hendry?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who gaped at Hendry?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who looked at Hendry?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "How many of Peter's daughters were present?",
        "answer": "five daughters",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many daughters were there?",
        "answer": "five daughters",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many daughters were there at Hendry?",
        "answer": "five daughters",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many daughters were there at Hendry?",
        "answer": "five daughters",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who had a man dead at home?",
        "answer": "Bell Christison's",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Whose man was dead at home?",
        "answer": "Bell Christison's",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Whose man was dead at home at Hendry?",
        "answer": "Bell Christison's",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Whose body was found at home?",
        "answer": "Bell Christison's",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What group was in the church?",
        "answer": "the Auld Lichts",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who were in church?",
        "answer": "the Auld Lichts",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who were in church at Hendry?",
        "answer": "the Auld Lichts",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was in church?",
        "answer": "the Auld Lichts",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What were the Auld Lichts waiting for?",
        "answer": "for their minister",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were they waiting for?",
        "answer": "for their minister",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What were the Auld Lichts waiting for at Hendry?",
        "answer": "for their minister",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the group waiting for?",
        "answer": "for their minister",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did everyone present wish for the rain?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was everyone there to ask for rain?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was everyone there to ask for rain at Hendry?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was everyone in the church to ask for rain?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Who was the simple woman?",
        "answer": "Meggy Rattray",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was a simple woman?",
        "answer": "Meggy Rattray",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was a simple woman at Hendry?",
        "answer": "Meggy Rattray",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was a simple woman?",
        "answer": "Meggy Rattray",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was every woman doing?",
        "answer": "wondering",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were all the women doing?",
        "answer": "wondering",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What were all the women at Hendry doing?",
        "answer": "wondering",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What were all the women doing?",
        "answer": "wondering",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What was the job of most of the men?",
        "answer": "farmers",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What were most of the men doing?",
        "answer": "farmers",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "And what were most of the men at Hendry?",
        "answer": "farmers",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What were most of the men doing?",
        "answer": "farmers",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What action did Hendry Munn do?",
        "answer": "signed to Peter Tosh",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Hendry Munn do?",
        "answer": "signed to Peter Tosh",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Hendry Munn do?",
        "answer": "signed to Peter Tosh",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Hendry Munn do?",
        "answer": "signed to Peter Tosh",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who snatched up his hat?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who dove for his hat?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who didve for Hendry Munn's hat?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who did a dove for his hat?",
        "answer": "Peter",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Where did Peter go?",
        "answer": "the vestry",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Then where did he go?",
        "answer": "the vestry",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Then where did Peter Tosh go?",
        "answer": "the vestry",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Peter go after he had swam for his hat?",
        "answer": "the vestry",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Was it a full meeting in the church?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the church full when they were waiting?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the church full when the men were waiting for the rain?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the church full when the men were waiting?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Where could Charles Yuill be found?",
        "answer": "in his pew",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was Charles Yuill?",
        "answer": "in his pew",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was Charles Yuill?",
        "answer": "in his pew",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was Charles Yuill located?",
        "answer": "in his pew",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Was Bell Christison invested in the outcome?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Bell Christison care how things went?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Bell Christison care how things went in the church?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Bell Christison care how things went?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3ydtzai2wxgebz5ld4llfye57zv14z": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-1",
        "original_question": "What can be recorded and played?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-1",
        "answer": "Multimedia"
      },
      {
        "turn_num": 2,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-2",
        "original_question": "What else can it be",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-2",
        "answer": "interacted with"
      },
      {
        "turn_num": 3,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-3",
        "original_question": "Anything else?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-3",
        "answer": "accessed by information content processing devices"
      },
      {
        "turn_num": 4,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-4",
        "original_question": "Who coined the term?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-4",
        "answer": "Bob Goldstein"
      },
      {
        "turn_num": 5,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-5",
        "original_question": "When?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-5",
        "answer": "July 1966"
      },
      {
        "turn_num": 6,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-6",
        "original_question": "Who is that?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-6",
        "answer": "a singer and artist"
      },
      {
        "turn_num": 7,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-7",
        "original_question": "Where was his show?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-7",
        "answer": "Southampton, Long Island"
      },
      {
        "turn_num": 8,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-8",
        "original_question": "What was the show called?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-8",
        "answer": "\"LightWorks at L'Oursin\""
      },
      {
        "turn_num": 9,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-9",
        "original_question": "What is Multimedia?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-9",
        "answer": "content that uses a combination of different content forms"
      },
      {
        "turn_num": 10,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-10",
        "original_question": "What kinds of forms?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-10",
        "answer": "text, audio, images, animations, video and interactive content."
      },
      {
        "turn_num": 11,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-11",
        "original_question": "What does it contrast with?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-11",
        "answer": "media that use only rudimentary computer displays"
      },
      {
        "turn_num": 12,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-12",
        "original_question": "What is used to store multimedia?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-12",
        "answer": "Multimedia devices"
      },
      {
        "turn_num": 13,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-13",
        "original_question": "What is synonymous with interactive multimedia?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-13",
        "answer": "\"rich media\""
      },
      {
        "turn_num": 14,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-14",
        "original_question": "Who was Goldstein aware of",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-14",
        "answer": "Dick Higgins"
      },
      {
        "turn_num": 15,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-15",
        "original_question": "What did he call his approach?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-15",
        "answer": "\"intermedia\"."
      },
      {
        "turn_num": 16,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-16",
        "original_question": "Was he American?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-16",
        "answer": "yes"
      },
      {
        "turn_num": 17,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-17",
        "original_question": "Who borrowed the term",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-17",
        "answer": "Richard Albarino"
      },
      {
        "turn_num": 18,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-18",
        "original_question": "What magazine did he write for?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-18",
        "answer": "\"Variety\""
      },
      {
        "turn_num": 19,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-19",
        "original_question": "When did he write an article about Bob",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-19",
        "answer": "August 10, 1966,"
      },
      {
        "turn_num": 20,
        "turn_id": "3ydtzai2wxgebz5ld4llfye57zv14z-20",
        "original_question": "Who was one of Bob's producers?",
        "original_question_id": "3ydtzai2wxgebz5ld4llfye57zv14z-20",
        "answer": "Iris Sawyer"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What form of media can be both recorded and played?",
        "answer": "Multimedia",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What can be recorded and played?",
        "answer": "Multimedia",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What can be recorded and played?",
        "answer": "Multimedia",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can be recorded and played?",
        "answer": "Multimedia",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Multimedia can be recorded and played, displayed, and what?",
        "answer": "interacted with",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What can be recorded and played?",
        "answer": "interacted with",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else can Multimedia be besides being recorded and played?",
        "answer": "interacted with",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can Multimedia be used for, in addition to recording and playing?",
        "answer": "interacted with",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Multimedia can be recorded and played, displayed, interacted with, and what else?",
        "answer": "accessed by information content processing devices",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else can it be",
        "answer": "accessed by information content processing devices",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Anything else can Multimedia be besides being recorded and played?",
        "answer": "accessed by information content processing devices",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can Multimedia be interacted with, besides being recorded and played?",
        "answer": "accessed by information content processing devices",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Who is credited with creating the term multimedia?",
        "answer": "Bob Goldstein",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who coined the term?",
        "answer": "Bob Goldstein",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who coined the term Multimedia?",
        "answer": "Bob Goldstein",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who coined the term Multimedia?",
        "answer": "Bob Goldstein",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "When did Bob Goldstein first coin the term multimedia?",
        "answer": "July 1966",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the term 'recorded and played' coined?",
        "answer": "July 1966",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was the term Multimedia coined?",
        "answer": "July 1966",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Bob Goldstein coin the term Multimedia?",
        "answer": "July 1966",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Who is Bob Goldstein?",
        "answer": "a singer and artist",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who is Bob Goldstein?",
        "answer": "a singer and artist",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who is the term Multimedia?",
        "answer": "a singer and artist",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the person that coined the term Multimedia?",
        "answer": "a singer and artist",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Where would Bob Goldstein put on shows?",
        "answer": "Southampton, Long Island",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was Bob Goldstein's show?",
        "answer": "Southampton, Long Island",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was Bob Goldstein's show?",
        "answer": "Southampton, Long Island",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where did Bob Goldstein perform?",
        "answer": "Southampton, Long Island",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What was the name of Bob Goldstein's show?",
        "answer": "\"LightWorks at L'Oursin\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was Bob Goldstein's show called?",
        "answer": "\"LightWorks at L'Oursin\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the show called?",
        "answer": "\"LightWorks at L'Oursin\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the name of Bob Goldstein's show?",
        "answer": "\"LightWorks at L'Oursin\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "What is the definition of multimedia?",
        "answer": "content that uses a combination of different content forms",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Multimedia?",
        "answer": "content that uses a combination of different content forms",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Multimedia?",
        "answer": "content that uses a combination of different content forms",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Multimedia?",
        "answer": "content that uses a combination of different content forms",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What forms of content can multimedia contain?",
        "answer": "text, audio, images, animations, video and interactive content.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kinds of forms can multimedia include?",
        "answer": "text, audio, images, animations, video and interactive content.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kinds of content forms did Bob Goldstein use?",
        "answer": "text, audio, images, animations, video and interactive content.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kinds of content forms are used in Multimedia?",
        "answer": "text, audio, images, animations, video and interactive content.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What forms of media can multimedia be contrasted with?",
        "answer": "media that use only rudimentary computer displays",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does multimedia contrast with?",
        "answer": "media that use only rudimentary computer displays",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Multimedia contrast with?",
        "answer": "media that use only rudimentary computer displays",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What does Multimedia contrast with?",
        "answer": "media that use only rudimentary computer displays",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What devices are used to store multimedia?",
        "answer": "Multimedia devices",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is used to store multimedia?",
        "answer": "Multimedia devices",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is used to store multimedia?",
        "answer": "Multimedia devices",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is used to store multimedia?",
        "answer": "Multimedia devices",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What is interactive multimedia synonymous with?",
        "answer": "\"rich media\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is synonymous with interactive multimedia?",
        "answer": "\"rich media\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is synonymous with interactive multimedia?",
        "answer": "\"rich media\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the definition of interactive multimedia?",
        "answer": "\"rich media\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "Who was Bob Goldstein aware of?",
        "answer": "Dick Higgins",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was Bob Goldstein aware of?",
        "answer": "Dick Higgins",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was Bob Goldstein aware of?",
        "answer": "Dick Higgins",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Bob Goldstein aware of?",
        "answer": "Dick Higgins",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What was the media term that Dick Higgens coined?",
        "answer": "\"intermedia\".",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Bob Goldstein call his approach?",
        "answer": "\"intermedia\".",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Bob Goldstein call his approach to multimedia?",
        "answer": "\"intermedia\".",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Bob Goldstein call his approach to multimedia?",
        "answer": "\"intermedia\".",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Was Dick Higgens an American?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Bob Goldstein American?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Bob Goldstein American?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Dick Higgins American?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "Who borrowed the term multimedia?",
        "answer": "Richard Albarino",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who borrowed the term",
        "answer": "Richard Albarino",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who borrowed the term \"multimedia\" from Bob Goldstein?",
        "answer": "Richard Albarino",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who borrowed the term \"intermedia\" from Bob Goldstein?",
        "answer": "Richard Albarino",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What magazine did Richard Albarino write for?",
        "answer": "\"Variety\"",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What magazine did Richard Albarino write for?",
        "answer": "\"Variety\"",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What magazine did Bob Goldstein write for?",
        "answer": "\"Variety\"",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What magazine did Richard Albarino write for?",
        "answer": "\"Variety\"",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "When did Richard Albarino's article about Bob Goldstein come out?",
        "answer": "August 10, 1966,",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Richard Albarino write an article about Bob?",
        "answer": "August 10, 1966,",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Bob Goldstein write an article about Bob?",
        "answer": "August 10, 1966,",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Richard Albarino write an article about Bob Goldstein?",
        "answer": "August 10, 1966,",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "What was the name of one of Bob Goldstein's producers?",
        "answer": "Iris Sawyer",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Bob Goldstein",
        "answer": "Iris Sawyer",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who was one of Bob Goldstein's producers?",
        "answer": "Iris Sawyer",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was one of Bob Goldstein's producers?",
        "answer": "Iris Sawyer",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3z3zlgnnsiuha76yy56h6uu712v3qx": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-1",
        "original_question": "What grew as a commercial port during the Industrial Revolution?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-1",
        "answer": "Plymouth"
      },
      {
        "turn_num": 2,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-2",
        "original_question": "What is it located on the south coast of?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-2",
        "answer": "Devon"
      },
      {
        "turn_num": 3,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-3",
        "original_question": "In what country?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-3",
        "answer": "England"
      },
      {
        "turn_num": 4,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-4",
        "original_question": "What does Plymouth's early history extend to?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-4",
        "answer": "the Bronze Age"
      },
      {
        "turn_num": 5,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-5",
        "original_question": "Where was a first settlement?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-5",
        "answer": "at Mount Batten"
      },
      {
        "turn_num": 6,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-6",
        "original_question": "What happened in 1620?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-6",
        "answer": "the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony"
      },
      {
        "turn_num": 7,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-7",
        "original_question": "Was it the first English settlement?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-7",
        "answer": "no"
      },
      {
        "turn_num": 8,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-8",
        "original_question": "Which number was it?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-8",
        "answer": "the second"
      },
      {
        "turn_num": 9,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-9",
        "original_question": "Who held it during the English Civil War?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-9",
        "answer": "the Parliamentarians"
      },
      {
        "turn_num": 10,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-10",
        "original_question": "When was it besieged?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-10",
        "answer": "between 1642 and 1646"
      },
      {
        "turn_num": 11,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-11",
        "original_question": "What did the city's naval importance lead to?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-11",
        "answer": "its targeting and partial destruction during World War II"
      },
      {
        "turn_num": 12,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-12",
        "original_question": "What happened to the city centre after the war?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-12",
        "answer": "it was completely rebuilt"
      },
      {
        "turn_num": 13,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-13",
        "original_question": "What kind of a trading post was the early Plymouth?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-13",
        "answer": "it was a trading post for the Roman Empire"
      },
      {
        "turn_num": 14,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-14",
        "original_question": "What village did it get surpassed by?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-14",
        "answer": "Sutton"
      },
      {
        "turn_num": 15,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-15",
        "original_question": "What is Sutton called today?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-15",
        "answer": "Plymouth"
      },
      {
        "turn_num": 16,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-16",
        "original_question": "Is Plymouth in England, 38 miles south-west of Exeter?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-16",
        "answer": "no"
      },
      {
        "turn_num": 17,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-17",
        "original_question": "Which river mouths is it in between?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-17",
        "answer": "Plym and Tamar"
      },
      {
        "turn_num": 18,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-18",
        "original_question": "Which boundary do the form?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-18",
        "answer": "Cornwall"
      },
      {
        "turn_num": 19,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-19",
        "original_question": "How many miles is it west-south-west of London?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-19",
        "answer": "190"
      },
      {
        "turn_num": 20,
        "turn_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-20",
        "original_question": "What kind of exports came from Plymouth?",
        "original_question_id": "3z3zlgnnsiuha76yy56h6uu712v3qx-20",
        "answer": "local minerals"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "During the industrial revolution, whose commercial port grew in size?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What grew as a commercial port during the Industrial Revolution?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What grew as a commercial port during the Industrial Revolution?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was a commercial port during the Industrial Revolution?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What county is Plymouth on the south coast of?",
        "answer": "Devon",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is it located on the south coast of?",
        "answer": "Devon",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Plymouth located on the south coast of?",
        "answer": "Devon",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Plymouth on the south coast of?",
        "answer": "Devon",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "What country is Plymouth located in?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "In what country is Plymouth located on the south coast of?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "In what country did Plymouth grew as a commercial port during the Industrial Revolution?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What country is Plymouth located on the south coast of?",
        "answer": "England",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "How far back does the history of Plymouth extend?",
        "answer": "the Bronze Age",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What does Plymouth's early history extend to?",
        "answer": "the Bronze Age",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What does Plymouth's early history extend to?",
        "answer": "the Bronze Age",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the history of Plymouth?",
        "answer": "the Bronze Age",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Where was a first settlement in Plymouth?",
        "answer": "at Mount Batten",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was the first settlement in Plymouth?",
        "answer": "at Mount Batten",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was a first settlement for Plymouth?",
        "answer": "at Mount Batten",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the first settlement of Plymouth?",
        "answer": "at Mount Batten",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "What took place in 1620?",
        "answer": "the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened in 1620?",
        "answer": "the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happened to Plymouth in 1620?",
        "answer": "the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened in 1620?",
        "answer": "the Pilgrim Fathers departed Plymouth for the New World and established Plymouth Colony",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Was the Pilgrim settlement in the new world the English's first?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Plymouth the first English settlement?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Plymouth the first English settlement?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Plymouth the first English settlement?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What number of English settlement was Plymouth Colony?",
        "answer": "the second",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which number was Plymouth Colony?",
        "answer": "the second",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which number was Plymouth's first English settlement?",
        "answer": "the second",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What number was Plymouth the first English settlement?",
        "answer": "the second",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who held Plymouth during the English Civil War?",
        "answer": "the Parliamentarians",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who held Plymouth during the English Civil War?",
        "answer": "the Parliamentarians",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who held Plymouth during the English Civil War?",
        "answer": "the Parliamentarians",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Plymouth held by during the English Civil War?",
        "answer": "the Parliamentarians",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "When did the siege on Plymouth occur?",
        "answer": "between 1642 and 1646",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was Plymouth besieged?",
        "answer": "between 1642 and 1646",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When was Plymouth besieged?",
        "answer": "between 1642 and 1646",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When was Plymouth besieged?",
        "answer": "between 1642 and 1646",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What was caused by Plymouth's naval importance?",
        "answer": "its targeting and partial destruction during World War II",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did the city's naval importance lead to?",
        "answer": "its targeting and partial destruction during World War II",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Plymouth's naval importance lead to?",
        "answer": "its targeting and partial destruction during World War II",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Plymouth's naval importance lead to?",
        "answer": "its targeting and partial destruction during World War II",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What happened to Plymouth's city center after the war?",
        "answer": "it was completely rebuilt",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happened to the city centre after the war?",
        "answer": "it was completely rebuilt",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happened to Plymouth's centre after the war?",
        "answer": "it was completely rebuilt",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happened to Plymouth's centre after the war?",
        "answer": "it was completely rebuilt",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "What sort of trading post did Early Plymouth serve as?",
        "answer": "it was a trading post for the Roman Empire",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of a trading post was the early Plymouth?",
        "answer": "it was a trading post for the Roman Empire",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of a trading post was the early Plymouth?",
        "answer": "it was a trading post for the Roman Empire",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of trading post was Plymouth in the early days?",
        "answer": "it was a trading post for the Roman Empire",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What village surpassed Plymouth?",
        "answer": "Sutton",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What village surpassed Plymouth?",
        "answer": "Sutton",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What village did Plymouth get surpassed by?",
        "answer": "Sutton",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What village did Plymouth get surpassed by?",
        "answer": "Sutton",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "What is Sutton known as today?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is Sutton called today?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is Sutton called today?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is Sutton called today?",
        "answer": "Plymouth",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "Does Plymouth lie in England, 38 miles south-west of Exeter?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is Plymouth in England, 38 miles south-west of Exeter?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is Plymouth in England, 38 miles south-west of Exeter?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is Plymouth located 38 miles south-west of Exeter?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What rivers' mouth is Plymouth in between?",
        "answer": "Plym and Tamar",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which river mouths is Plymouth in between?",
        "answer": "Plym and Tamar",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which river mouths is Plymouth in between?",
        "answer": "Plym and Tamar",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Which river mouths is Plymouth in between?",
        "answer": "Plym and Tamar",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "What do the rivers Plym and Tamar make a boundary with?",
        "answer": "Cornwall",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which boundary do the form?",
        "answer": "Cornwall",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which boundary do the rivers Plym and Tamar form?",
        "answer": "Cornwall",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What boundary does Plymouth have?",
        "answer": "Cornwall",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "Plymouth is located how many miles west-south-west of London?",
        "answer": "190",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many miles is Plymouth west-south-west of London?",
        "answer": "190",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many miles is Plymouth west-south-west of London?",
        "answer": "190",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many miles is Plymouth west-south-west of London?",
        "answer": "190",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "What does Plymouth export?",
        "answer": "local minerals",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of exports came from Plymouth?",
        "answer": "local minerals",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of exports came from Plymouth?",
        "answer": "local minerals",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of exports came from Plymouth?",
        "answer": "local minerals",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3zpbjo59kp12f69s84pzapoi14bhdo": {
    "number_of_turns": 14,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-1",
        "original_question": "What was posted online?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-1",
        "answer": "an audio message purportedly recorded by Osama bin Laden"
      },
      {
        "turn_num": 2,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-2",
        "original_question": "When did he record the message?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-2",
        "answer": "shortly before he was killed"
      },
      {
        "turn_num": 3,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-3",
        "original_question": "Who posted it?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-3",
        "answer": "Radical Islamic websites"
      },
      {
        "turn_num": 4,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-4",
        "original_question": "When?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-4",
        "answer": "Wednesday"
      },
      {
        "turn_num": 5,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-5",
        "original_question": "Is the recording in English?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-5",
        "answer": "no"
      },
      {
        "turn_num": 6,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-6",
        "original_question": "Did anyone try translating it?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-6",
        "answer": "CNN did"
      },
      {
        "turn_num": 7,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-7",
        "original_question": "How does the recording start?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-7",
        "answer": "with a prayer"
      },
      {
        "turn_num": 8,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-8",
        "original_question": "How long is it?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-8",
        "answer": "more than 12 minutes"
      },
      {
        "turn_num": 9,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-9",
        "original_question": "Is there a marker on the recording indicating the date?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-9",
        "answer": "yes"
      },
      {
        "turn_num": 10,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-10",
        "original_question": "What date was it found to be?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-10",
        "answer": "between April 4 and May 3"
      },
      {
        "turn_num": 11,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-11",
        "original_question": "What else does the message speak of?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-11",
        "answer": "remarks about anti-government protests and uprisings"
      },
      {
        "turn_num": 12,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-12",
        "original_question": "Was bin Laden the only speaker?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-12",
        "answer": "yes"
      },
      {
        "turn_num": 13,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-13",
        "original_question": "What group released the recording?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-13",
        "answer": "As-Sahab"
      },
      {
        "turn_num": 14,
        "turn_id": "3zpbjo59kp12f69s84pzapoi14bhdo-14",
        "original_question": "What is that?",
        "original_question_id": "3zpbjo59kp12f69s84pzapoi14bhdo-14",
        "answer": "al Qaeda's media arm"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What was posted to the internet?",
        "answer": "an audio message purportedly recorded by Osama bin Laden",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was posted online?",
        "answer": "an audio message purportedly recorded by Osama bin Laden",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was posted online?",
        "answer": "an audio message purportedly recorded by Osama bin Laden",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was posted online?",
        "answer": "an audio message purportedly recorded by Osama bin Laden",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "When did Osama bin Laden record the message?",
        "answer": "shortly before he was killed",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When did Osama bin Laden record the message?",
        "answer": "shortly before he was killed",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Osama bin Laden record the message?",
        "answer": "shortly before he was killed",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did Osama bin Laden record the message?",
        "answer": "shortly before he was killed",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who posted Osama bin Laden's message?",
        "answer": "Radical Islamic websites",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who posted the audio message purportedly recorded by Osama bin Laden?",
        "answer": "Radical Islamic websites",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who posted the audio message purportedly recorded by Osama bin Laden online?",
        "answer": "Radical Islamic websites",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who posted the message?",
        "answer": "Radical Islamic websites",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "When did Osama bin Laden's get uploaded to radical Islamic sites?",
        "answer": "Wednesday",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "When was the audio message posted online?",
        "answer": "Wednesday",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "When did Osama bin Laden post the audio message purportedly recorded by Osama bin Laden?",
        "answer": "Wednesday",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "When did radical Islamic websites post the audio message?",
        "answer": "Wednesday",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Did Osama bin Laden record his message in English?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is the recording in English?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is the recording of the message by Osama bin Laden in English?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is the recording of Osama bin Laden's message in English?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Did anyone attempt to translate Osama bin Laden's message?",
        "answer": "CNN did",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did anyone try translating the recording?",
        "answer": "CNN did",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did anyone try translating the recording of Osama bin Laden's message?",
        "answer": "CNN did",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did anyone try translating the message?",
        "answer": "CNN did",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What is at the beginning of Osama bin Laden's recording?",
        "answer": "with a prayer",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How does the recording start?",
        "answer": "with a prayer",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How does the recording of Osama bin Laden's message start?",
        "answer": "with a prayer",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the start of the recording of Osama bin Laden's message?",
        "answer": "with a prayer",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "How long is Osama bin Laden's message?",
        "answer": "more than 12 minutes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How long is the recording?",
        "answer": "more than 12 minutes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How long is the recording of Osama bin Laden's message?",
        "answer": "more than 12 minutes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How long is the recording of Osama bin Laden's message?",
        "answer": "more than 12 minutes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Does the recording have a marker that indicates the date?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Is there a marker on the recording indicating the date?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Is there a marker on the recording indicating the date?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is there a marker on the recording that indicates the date?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What date does the message say it is?",
        "answer": "between April 4 and May 3",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What date was the recording found to be?",
        "answer": "between April 4 and May 3",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What date was the recording of Osama bin Laden's message found to be?",
        "answer": "between April 4 and May 3",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What date was the recording of Osama bin Laden's message found to be?",
        "answer": "between April 4 and May 3",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What does the message say after the prayer?",
        "answer": "remarks about anti-government protests and uprisings",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What else does the message speak of?",
        "answer": "remarks about anti-government protests and uprisings",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What else does the message purportedly recorded by Osama bin Laden speak of?",
        "answer": "remarks about anti-government protests and uprisings",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What else does the message mention besides April 4?",
        "answer": "remarks about anti-government protests and uprisings",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Was Osama bin Laden the only person to speak in his message?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was bin Laden the only speaker in the message?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Osama bin Laden the only speaker in the message?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Osama bin Laden the only person to speak in the message?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Who released the recording of Osama bin Laden?",
        "answer": "As-Sahab",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What group released the recording?",
        "answer": "As-Sahab",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What group released the recording of Osama bin Laden's message?",
        "answer": "As-Sahab",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What group released the recording of Osama bin Laden's message?",
        "answer": "As-Sahab",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What is As-Sahab?",
        "answer": "al Qaeda's media arm",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is As-Sahab?",
        "answer": "al Qaeda's media arm",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the As-Sahab recording?",
        "answer": "al Qaeda's media arm",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the group that released the recording of Osama bin Laden's message?",
        "answer": "al Qaeda's media arm",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3zr9aiqjub9e4ak3hlhl1tvv1xy04j": {
    "number_of_turns": 13,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-1",
        "original_question": "What is so special about the pine-tree?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-1",
        "answer": "It contained a possible bird's nest."
      },
      {
        "turn_num": 2,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-2",
        "original_question": "What was the bird's nest?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-2",
        "answer": "Just a little tuft of twigs growing out together."
      },
      {
        "turn_num": 3,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-3",
        "original_question": "Who found the nest?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-3",
        "answer": "Phonny"
      },
      {
        "turn_num": 4,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-4",
        "original_question": "Was anyone with Phonny?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-4",
        "answer": "Yes"
      },
      {
        "turn_num": 5,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-5",
        "original_question": "Who?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-5",
        "answer": "Malleville"
      },
      {
        "turn_num": 6,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-6",
        "original_question": "Are they related?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-6",
        "answer": "unknown"
      },
      {
        "turn_num": 7,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-7",
        "original_question": "Did they get the nest?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-7",
        "answer": "No"
      },
      {
        "turn_num": 8,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-8",
        "original_question": "Did one of them try?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-8",
        "answer": "Yes"
      },
      {
        "turn_num": 9,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-9",
        "original_question": "Who tried?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-9",
        "answer": "Phonny"
      },
      {
        "turn_num": 10,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-10",
        "original_question": "Did he climb the pine-tree?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-10",
        "answer": "Yes"
      },
      {
        "turn_num": 11,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-11",
        "original_question": "How far did he get?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-11",
        "answer": "To the limb with twigs."
      },
      {
        "turn_num": 12,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-12",
        "original_question": "Did he fall out of the tree?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-12",
        "answer": "No"
      },
      {
        "turn_num": 13,
        "turn_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-13",
        "original_question": "Did Malleville help?",
        "original_question_id": "3zr9aiqjub9e4ak3hlhl1tvv1xy04j-13",
        "answer": "No"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What makes the pine-tree so special?",
        "answer": "It contained a possible bird's nest.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is so special about the pine-tree?",
        "answer": "It contained a possible bird's nest.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is so special about the pine-tree?",
        "answer": "It contained a possible bird's nest.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is so special about the pine-tree?",
        "answer": "It contained a possible bird's nest.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What was the thing everyone thought was a bird's nest?",
        "answer": "Just a little tuft of twigs growing out together.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What was the bird's nest?",
        "answer": "Just a little tuft of twigs growing out together.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What was the bird's nest in the pine-tree?",
        "answer": "Just a little tuft of twigs growing out together.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the bird's nest?",
        "answer": "Just a little tuft of twigs growing out together.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who found the supposed bird's nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who found the nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who found the bird's nest in the pine-tree?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who found the bird's nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Did anyone accompany Phonny?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was anyone with Phonny?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was anyone with Phonny?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was anyone with Phonny?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Who was Phonny's companion?",
        "answer": "Malleville",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who was with Phonny when he found the bird's nest?",
        "answer": "Malleville",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who found the pine-tree with Phonny?",
        "answer": "Malleville",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who was Phonny with?",
        "answer": "Malleville",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Are Malleville and Phonny related to each other?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Are Phonny and Malleville related?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Are Malleville and Phonny related?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Are Malleville and Phonny related?",
        "answer": "unknown",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "Did Malleville and Phonny retreive the bird's nest?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they get the nest?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Malleville and Phonny get the nest?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Phonny and Malleville get the bird's nest?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Did Malleville or Phonny try to get the birds nest?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did one of them try to get the nest?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did one of the birds try to get the nest?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did one of the birds try and find the nest?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Who tried to reach the bird's nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who tried to get the nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who tried to get the nest from the pine-tree?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who tried to find the bird's nest?",
        "answer": "Phonny",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Did Phonny climb the pine tree?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did he climb the pine-tree?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Phonny climb the pine-tree?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Phonny climb the pine-tree?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "How far did Phonny get in the pine tree?",
        "answer": "To the limb with twigs.",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How far did Phonny get when he climbed the pine-tree?",
        "answer": "To the limb with twigs.",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How far did Phonny get from the pine-tree?",
        "answer": "To the limb with twigs.",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How far did Phonny climb the pine-tree?",
        "answer": "To the limb with twigs.",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Phonny fall out of the pine tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Phonny fall out of the tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Phonny fall out of the pine-tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Phonny fall out of the pine-tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did Malleville help Phonny up the tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Malleville help?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Malleville help Phonny?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Malleville help Phonny climb the pine-tree?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3zv9h2yqqd7mu42kae5nyjctpgp3w4": {
    "number_of_turns": 17,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-1",
        "original_question": "What did they land on",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-1",
        "answer": "the logs"
      },
      {
        "turn_num": 2,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-2",
        "original_question": "Who wanted to walk around",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-2",
        "answer": "Marco"
      },
      {
        "turn_num": 3,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-3",
        "original_question": "Who wanted to eat",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-3",
        "answer": "Forester"
      },
      {
        "turn_num": 4,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-4",
        "original_question": "Eat what?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-4",
        "answer": "breakfast"
      },
      {
        "turn_num": 5,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-5",
        "original_question": "What kind of ship was it",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-5",
        "answer": "A raft"
      },
      {
        "turn_num": 6,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-6",
        "original_question": "Where was the tavern",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-6",
        "answer": "about a quarter of a mile away"
      },
      {
        "turn_num": 7,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-7",
        "original_question": "From what?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-7",
        "answer": "the mill"
      },
      {
        "turn_num": 8,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-8",
        "original_question": "Was it good?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-8",
        "answer": "Yes"
      },
      {
        "turn_num": 9,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-9",
        "original_question": "Did they go there",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-9",
        "answer": "Yes"
      },
      {
        "turn_num": 10,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-10",
        "original_question": "Who wanted to go there first",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-10",
        "answer": "Forester"
      },
      {
        "turn_num": 11,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-11",
        "original_question": "Who told them about the tavern",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-11",
        "answer": "a fellow passenger"
      },
      {
        "turn_num": 12,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-12",
        "original_question": "Did Forester eat a big breakfast",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-12",
        "answer": "No"
      },
      {
        "turn_num": 13,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-13",
        "original_question": "Did Marco eat?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-13",
        "answer": "Yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-14",
        "original_question": "How did he talk?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-14",
        "answer": "fast"
      },
      {
        "turn_num": 15,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-15",
        "original_question": "Was Forester feeling ill",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-15",
        "answer": "Yes"
      },
      {
        "turn_num": 16,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-16",
        "original_question": "What did he want to do because of that",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-16",
        "answer": "lie down"
      },
      {
        "turn_num": 17,
        "turn_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-17",
        "original_question": "on what?",
        "original_question_id": "3zv9h2yqqd7mu42kae5nyjctpgp3w4-17",
        "answer": "a sofa"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What did the men land on?",
        "answer": "the logs",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did they land on",
        "answer": "the logs",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did they land on",
        "answer": "the logs",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did the group land on?",
        "answer": "the logs",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "Who wanted to walk the shoreline?",
        "answer": "Marco",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who wanted to walk around",
        "answer": "Marco",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who wanted to walk around the logs",
        "answer": "Marco",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who wanted to walk around the logs?",
        "answer": "Marco",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Who was in the mood to eat?",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who wanted to eat",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who wanted to eat the logs",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who wanted to eat?",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "What meal did Forester want to eat?",
        "answer": "breakfast",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Eat what?",
        "answer": "breakfast",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Eat what?",
        "answer": "breakfast",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Forester want to eat?",
        "answer": "breakfast",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "What kind of ship were the men on?",
        "answer": "A raft",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of ship was it?",
        "answer": "A raft",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of ship was the Forester on?",
        "answer": "A raft",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of ship was Forester on?",
        "answer": "A raft",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Where could the tavern be found?",
        "answer": "about a quarter of a mile away",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Where was the tavern",
        "answer": "about a quarter of a mile away",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Where was the tavern?",
        "answer": "about a quarter of a mile away",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Where was the tavern?",
        "answer": "about a quarter of a mile away",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What was the tavern a quarter of a mile from?",
        "answer": "the mill",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "From what?",
        "answer": "the mill",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "From what?",
        "answer": "the mill",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What was the tavern from?",
        "answer": "the mill",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "Was the tavern high-quality?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was the tavern good?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was the breakfast at the tavern good?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was the breakfast at the mill good?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Did Marco and Forester go to the tavern?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did they go to the tavern?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did the rafters go to the tavern?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Marco and Forester go to the mill?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "Who wanted to go to the tavern first?",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who wanted to go there first",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who wanted to go to the tavern first?",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who wanted to go to the mill first?",
        "answer": "Forester",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "Who told Marco and Forester about the tavern?",
        "answer": "a fellow passenger",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Who told them about the tavern",
        "answer": "a fellow passenger",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Who told them about the tavern?",
        "answer": "a fellow passenger",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Who told Forester about the tavern?",
        "answer": "a fellow passenger",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "Did Forester have a large appetite for breakfast?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Forester eat a big breakfast?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Forester eat a big breakfast at the tavern?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Forester have a big breakfast?",
        "answer": "No",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Did Marco have anything to eat?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Did Marco eat?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Did Marco eat at the tavern?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Did Marco eat?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "How was Marco talking?",
        "answer": "fast",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How did Marco talk?",
        "answer": "fast",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How did Forester talk to Marco?",
        "answer": "fast",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Marco say?",
        "answer": "fast",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Did Forester feel under the weather?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Was Forester feeling ill?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Was Forester feeling ill?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Was Forester feeling ill?",
        "answer": "Yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "What did Forester want to do since he was sick?",
        "answer": "lie down",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What did Forester want to do because he was feeling ill?",
        "answer": "lie down",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What did Forester want to do because of the tavern?",
        "answer": "lie down",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Forester want to do because of his illness?",
        "answer": "lie down",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "What did Forester want to lie down on?",
        "answer": "a sofa",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "on what did Forester want to lie down",
        "answer": "a sofa",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "On what did Forester want to do because of the lie down?",
        "answer": "a sofa",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What did Forester want to do because he was feeling ill?",
        "answer": "a sofa",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  },
  "3zwfc4w1uu7c2k1rvfwjctt90e7rf0": {
    "number_of_turns": 20,
    "annotator_id": null,
    "dialog": [
      {
        "turn_num": 1,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-1",
        "original_question": "What can a heart attack symptom feel like?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-1",
        "answer": "heartburn"
      },
      {
        "turn_num": 2,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-2",
        "original_question": "Which symptom?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-2",
        "answer": "chest pain"
      },
      {
        "turn_num": 3,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-3",
        "original_question": "Can this pain travel?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-3",
        "answer": "yes"
      },
      {
        "turn_num": 4,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-4",
        "original_question": "Does it go to the lower part of the body?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-4",
        "answer": "no"
      },
      {
        "turn_num": 5,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-5",
        "original_question": "What about the jaw?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-5",
        "answer": "yes"
      },
      {
        "turn_num": 6,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-6",
        "original_question": "The shoulder and arm?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-6",
        "answer": "yes"
      },
      {
        "turn_num": 7,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-7",
        "original_question": "What is the real name of a heart attack?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-7",
        "answer": "Myocardial infarction (MI) or acute myocardial infarction (AMI)"
      },
      {
        "turn_num": 8,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-8",
        "original_question": "What disease usually causes this?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-8",
        "answer": "coronary artery disease"
      },
      {
        "turn_num": 9,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-9",
        "original_question": "Does this involve the blocking of something?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-9",
        "answer": "yes"
      },
      {
        "turn_num": 10,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-10",
        "original_question": "What?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-10",
        "answer": "a coronary artery"
      },
      {
        "turn_num": 11,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-11",
        "original_question": "What happens to plaque?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-11",
        "answer": "it ruptures"
      },
      {
        "turn_num": 12,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-12",
        "original_question": "Which kind of plaque?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-12",
        "answer": "atherosclerotic plaque"
      },
      {
        "turn_num": 13,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-13",
        "original_question": "Can smoking increase this possibility?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-13",
        "answer": "yes"
      },
      {
        "turn_num": 14,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-14",
        "original_question": "What can be caused by drug use?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-14",
        "answer": "MI's are less commonly caused by them"
      },
      {
        "turn_num": 15,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-15",
        "original_question": "Are there tests around for diagnosis?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-15",
        "answer": "yes"
      },
      {
        "turn_num": 16,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-16",
        "original_question": "How many?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-16",
        "answer": "a number of them"
      },
      {
        "turn_num": 17,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-17",
        "original_question": "What kind of test is creatine kinase MB",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-17",
        "answer": "a blood test"
      },
      {
        "turn_num": 18,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-18",
        "original_question": "Are women or men more likely to have atypical signs?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-18",
        "answer": "women"
      },
      {
        "turn_num": 19,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-19",
        "original_question": "How many people have atypical signs?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-19",
        "answer": "about 30% of people"
      },
      {
        "turn_num": 20,
        "turn_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-20",
        "original_question": "Can being sick to your stomach be one?",
        "original_question_id": "3zwfc4w1uu7c2k1rvfwjctt90e7rf0-20",
        "answer": "yes"
      }
    ],
    "1": {
      "ground_truth_rewrite": {
        "text": "What do heart attack symptoms sometimes mimic?",
        "answer": "heartburn",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What can a heart attack symptom feel like?",
        "answer": "heartburn",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What can a heart attack symptom feel like?",
        "answer": "heartburn",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can a heart attack symptom feel like?",
        "answer": "heartburn",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "2": {
      "ground_truth_rewrite": {
        "text": "What heart attack symptom sometimes feels like heartburn?",
        "answer": "chest pain",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "heart attack symptom",
        "answer": "chest pain",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which symptom of heart attack is most common?",
        "answer": "chest pain",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the name of the symptom that causes heartburn?",
        "answer": "chest pain",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "3": {
      "ground_truth_rewrite": {
        "text": "Can chest pain travel?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can chest pain from a heart attack travel?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Can chest pain travel?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is it possible for the heartburn to travel?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "4": {
      "ground_truth_rewrite": {
        "text": "Does chest pain travel to the lower part of the body?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can chest pain from a heart attack travel to the lower part of the body?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does the heartburn go to the lower part of the body?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the heartburn go to the lower part of the body?",
        "answer": "no",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "5": {
      "ground_truth_rewrite": {
        "text": "Does chest pain travel to the jaw?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can heart attack pain radiate to the jaw?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What about the jaw?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does the pain in the jaw go to the lower part of the body?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "6": {
      "ground_truth_rewrite": {
        "text": "Does chest pain travel to the shoulder and arm?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can a heart attack symptom cause pain in the shoulder and arm?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "The shoulder and arm?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What part of the body is affected by heartburn?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "7": {
      "ground_truth_rewrite": {
        "text": "What are heart attacks really called?",
        "answer": "Myocardial infarction (MI) or acute myocardial infarction (AMI)",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What is the real name of a heart attack?",
        "answer": "Myocardial infarction (MI) or acute myocardial infarction (AMI)",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What is the real name of a heart attack?",
        "answer": "Myocardial infarction (MI) or acute myocardial infarction (AMI)",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the real name of a heart attack?",
        "answer": "Myocardial infarction (MI) or acute myocardial infarction (AMI)",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "8": {
      "ground_truth_rewrite": {
        "text": "What disease commonly causes heart attacks?",
        "answer": "coronary artery disease",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What disease usually causes a heart attack?",
        "answer": "coronary artery disease",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What disease usually causes a heart attack?",
        "answer": "coronary artery disease",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What disease usually causes heartburn?",
        "answer": "coronary artery disease",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "9": {
      "ground_truth_rewrite": {
        "text": "Are there blockages associated with coronary artery disease?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Does coronary artery disease involve the blocking of something?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Does heart attack involve the blocking of something?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Does a heart attack involve the blocking of something?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "10": {
      "ground_truth_rewrite": {
        "text": "What does coronary artery disease block?",
        "answer": "a coronary artery",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What causes the blocking in a heart attack?",
        "answer": "a coronary artery",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What causes a heart attack?",
        "answer": "a coronary artery",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What is the cause of a heart attack?",
        "answer": "a coronary artery",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "11": {
      "ground_truth_rewrite": {
        "text": "What happens to plaque when a coronary artery ruptures?",
        "answer": "it ruptures",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What happens to plaque during a heart attack?",
        "answer": "it ruptures",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What happens to plaque in the coronary artery?",
        "answer": "it ruptures",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What happens to plaque?",
        "answer": "it ruptures",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "12": {
      "ground_truth_rewrite": {
        "text": "What kind of plaque ruptures in the artery?",
        "answer": "atherosclerotic plaque",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Which kind of plaque can rupture?",
        "answer": "atherosclerotic plaque",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Which kind of plaque is the most common cause of heart attack?",
        "answer": "atherosclerotic plaque",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of plaque is ruptured in a coronary artery?",
        "answer": "atherosclerotic plaque",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "13": {
      "ground_truth_rewrite": {
        "text": "Can smoking increase the possibility of coronary artery disease?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can smoking increase the possibility of a heart attack?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Can smoking increase the possibility of atherosclerotic plaque?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is smoking a cause for concern?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "14": {
      "ground_truth_rewrite": {
        "text": "What may result from drug use?",
        "answer": "MI's are less commonly caused by them",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What can be caused by drug use?",
        "answer": "MI's are less commonly caused by them",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What can be caused by drug use?",
        "answer": "MI's are less commonly caused by them",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What can be caused by drug use?",
        "answer": "MI's are less commonly caused by them",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "15": {
      "ground_truth_rewrite": {
        "text": "Are there tests that can diagnose coronary artery disease?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "tests for heart attack diagnosis",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Are there tests around for diagnosis of myocardial infarction?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Are there tests available for diagnosing a heart attack?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "16": {
      "ground_truth_rewrite": {
        "text": "How many diagnostic tests exist for coronary artery disease?",
        "answer": "a number of them",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many tests are there for diagnosing a heart attack?",
        "answer": "a number of them",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many heart attacks are there?",
        "answer": "a number of them",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many tests are available for diagnosing a heart attack?",
        "answer": "a number of them",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "17": {
      "ground_truth_rewrite": {
        "text": "The creatine kinase MB tests for what?",
        "answer": "a blood test",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "What kind of test is creatine kinase MB",
        "answer": "a blood test",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "What kind of test is creatine kinase MB?",
        "answer": "a blood test",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "What kind of test is creatine kinase MB?",
        "answer": "a blood test",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "18": {
      "ground_truth_rewrite": {
        "text": "Do men or women tend to have atypical heart attack signs?",
        "answer": "women",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Are women or men more likely to have atypical signs?",
        "answer": "women",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Are women or men more likely to have atypical signs of heart attack?",
        "answer": "women",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Are women or men more likely to have atypical signs of heart attack?",
        "answer": "women",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "19": {
      "ground_truth_rewrite": {
        "text": "What percentage of the population produces atypical signs of heart attack?",
        "answer": "about 30% of people",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "How many people have atypical signs of a heart attack?",
        "answer": "about 30% of people",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "How many people have atypical signs of heart attack?",
        "answer": "about 30% of people",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "How many people have atypical signs?",
        "answer": "about 30% of people",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    },
    "20": {
      "ground_truth_rewrite": {
        "text": "Is being sick to your stomach a possible sign of a heart attack?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "openai_rewrite": {
        "text": "Can being sick to your stomach be a symptom of a heart attack?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_rewrite": {
        "text": "Can being sick to your stomach be one of the most common atypical signs of heart attack?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "gqr_qrecc_coqar_4ep_rewrite": {
        "text": "Is it possible to have a heart attack if you are sick to the stomach?",
        "answer": "yes",
        "score": null,
        "optimal": null
      },
      "annotator rewrite": null,
      "requires rewrite": null
    }
  }
}