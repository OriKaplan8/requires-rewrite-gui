{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import importlib.util\n",
    "# Define the path to the module\n",
    "module_path = r\"C:\\Users\\User\\Projects\\requires-rewrite-gui\\db.py\"\n",
    "\n",
    "# Load the module\n",
    "spec = importlib.util.spec_from_file_location(\"db\", module_path)\n",
    "db = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(db)\n",
    "\n",
    "\n",
    "def find_keywords_in_json(json_data, keyword = []):\n",
    "    \"\"\"\n",
    "    Find the specified keyword in the given JSON data.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The JSON data to be searched.\n",
    "        keyword (list): The list of keywords to be searched in the JSON data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the dialogs that contain the specified keyword.\n",
    "    \"\"\"\n",
    "    keyword_data = {}\n",
    "    for dialog_key, dialog_value in json_data.items():\n",
    "        for turn_key, turn_value in dialog_value['dialog'].items():\n",
    "            for kw in keyword:\n",
    "                if kw.lower() in turn_value['original_question'].lower().split(' ') or kw in turn_value['answer'].lower().split(' ') :\n",
    "                    print(\"---------------------------------------------------\")\n",
    "                    print(f\"first utterance: {dialog_value['dialog'][0]['original_question']}\")\n",
    "                    print(f\"keyword: {kw} in dialog: {dialog_key}\")\n",
    "                    print(f\"original question: {turn_value['original_question']}\")\n",
    "                    print(f\"answer: {turn_value['answer']}\")\n",
    "    return keyword_data\n",
    "\n",
    "def check_if_ordered_stayed_the_same(original_yael_data, processed_annotation_data, not_enough_dialog=None, processed=False):\n",
    "    if not processed:\n",
    "        original_order = list(dict.fromkeys(original_yael_data['chat_id'].tolist()))\n",
    "    else:\n",
    "        original_order = list(dict.fromkeys(original_yael_data.keys()))\n",
    "        \n",
    "    if not_enough_dialog:\n",
    "        original_order = [dialog_id for dialog_id in original_order if dialog_id not in not_enough_dialog.keys()]\n",
    "    processed_order = list(processed_annotation_data.keys())\n",
    "\n",
    "    if original_order == processed_order:\n",
    "        print(\"The order stayed the same\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"The order changed\")\n",
    "        print(f\"Length: {len(original_order)} | Original order: {original_order}\")\n",
    "        print(f\"Length: {len(processed_order)} | Processed order: {processed_order}\")\n",
    "        changed_order_details(original_order, processed_order)\n",
    "        return False\n",
    "\n",
    "def changed_order_details(original_order, processed_order):\n",
    "    original_set = set(original_order)\n",
    "    processed_set = set(processed_order)\n",
    "\n",
    "    if original_set == processed_set:\n",
    "        print(\"Both orders have the same items when considered as sets.\")\n",
    "    else:\n",
    "        print(\"The sets of items are different.\")\n",
    "        print(f\"Items in original but not in processed: {original_set - processed_set}\")\n",
    "        print(f\"Items in processed but not in original: {processed_set - original_set}\")\n",
    "\n",
    "    # Find where the orders differ\n",
    "    min_length = min(len(original_order), len(processed_order))\n",
    "    differences = []\n",
    "    for i in range(min_length):\n",
    "        if original_order[i] != processed_order[i]:\n",
    "            differences.append((i, original_order[i], processed_order[i]))\n",
    "\n",
    "    if differences:\n",
    "        print(\"Differences in order at positions:\")\n",
    "        for index, original, processed in differences:\n",
    "            print(f\"Position {index}: Original: {original}, Processed: {processed}\")\n",
    "    else:\n",
    "        if len(original_order) != len(processed_order):\n",
    "            print(f\"The lengths are different. Original length: {len(original_order)}, Processed length: {len(processed_order)}\")\n",
    "        else:\n",
    "            print(\"The items are the same but the order is different.\")\n",
    "    \n",
    "def read_json_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and returns the data as a Python object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        dict: The data read from the JSON file.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "        JSONDecodeError: If the file is not a valid JSON file.\n",
    "\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def save_json_file(output_path, data):\n",
    "    \"\"\"\n",
    "    Save data as JSON to the specified output path.\n",
    "\n",
    "    Parameters:\n",
    "    - output_path (str): The path to save the JSON file.\n",
    "    - data (dict): The data to be saved as JSON.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def split_json_file(json_data):\n",
    "    \"\"\"\n",
    "    Splits a JSON file into two separate JSON objects.\n",
    "\n",
    "    Args:\n",
    "        json_data (dict): The JSON data to be split.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two JSON objects. The first object contains the first half of the original JSON data,\n",
    "               and the second object contains the second half of the original JSON data.\n",
    "    \"\"\"\n",
    "    json_data1, json_data2 = {}, {}\n",
    "    mid_index = len(json_data) // 2\n",
    "    \n",
    "    for key, value in json_data.items():\n",
    "        if len(json_data1) < mid_index:\n",
    "            json_data1[key] = value\n",
    "        else:\n",
    "            json_data2[key] = value\n",
    "\n",
    "    return json_data1, json_data2\n",
    "\n",
    "def process_yael_data(df, remove_no_welcome=True):\n",
    "    \"\"\"\n",
    "    Process the Yael data and convert it into a specific format.\n",
    "\n",
    "    Args:\n",
    "        json_data (DataFrame): The input JSON data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two dictionaries. The first dictionary contains the processed data\n",
    "        with enough turns, and the second dictionary contains the data with not enough turns.\n",
    "    \"\"\"\n",
    "    df_grouped = df.groupby('chat_id')\n",
    "\n",
    "    #order = df.drop_duplicates(subset='chat_id').reset_index(drop=True)['chat_id']\n",
    "    \n",
    "    order = list(dict.fromkeys((df['chat_id'].tolist())))\n",
    "    json_data = {}\n",
    "    not_fit_for_annotation = {}\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for group_name in order:  # #your data\n",
    "        if counter == 0:\n",
    "            print(f\"first group name: {group_name}\")\n",
    "        group_df = df_grouped.get_group(group_name)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "        if counter % 500 == 0:\n",
    "            pass\n",
    "            #print(counter)\n",
    "\n",
    "        welcome_message = 0\n",
    "        group_df = group_df.reset_index(drop=True)\n",
    "        dialog_dict = {\"number_of_turns\": 0, \"annotator_id\": None, \"dialog\": {}}\n",
    "        welcome_message = True\n",
    "        if remove_no_welcome:\n",
    "            if str(group_df.iloc[0]['welcome_message']) != \"nan\":\n",
    "                dialog_dict[\"number_of_turns\"] += 1\n",
    "                welcome_message = 1\n",
    "                dialog_dict[\"dialog\"][0] = {\n",
    "                    \"turn_num\": 0,\n",
    "                    \"original_question\": group_df.iloc[0]['welcome_message'],\n",
    "                    \"answer\": \"\",\n",
    "                }\n",
    "            else:\n",
    "                welcome_message = False\n",
    "        \n",
    "\n",
    "        turns_counter = 0\n",
    "\n",
    "        for index in range(welcome_message, len(group_df), 2):\n",
    "            dialog_dict[\"number_of_turns\"] += 1\n",
    "            row1 = group_df.iloc[index]\n",
    "\n",
    "            if index + 1 < len(group_df):\n",
    "                row2 = group_df.iloc[index + 1]\n",
    "            else:\n",
    "                row2 = None\n",
    "\n",
    "            turn_num = int(index / 2) + welcome_message\n",
    "\n",
    "            dialog_dict[\"dialog\"][turn_num] = {\n",
    "                \"turn_num\": int(index / 2) + welcome_message,\n",
    "                \"original_question\": str(row1['message']),\n",
    "                \"answer\": str(row2['message']) if row2 is not None else \"None\",\n",
    "            }\n",
    "\n",
    "            if index + welcome_message > 0:\n",
    "                dialog_dict[\"dialog\"][turn_num][\"requires_rewrite\"] = None\n",
    "                dialog_dict[\"dialog\"][turn_num][\"enough_context\"] = None\n",
    "                dialog_dict[\"dialog\"][turn_num][\"needs_clarification\"] = None\n",
    "                turns_counter += 1\n",
    "            \n",
    "        if \"is_english\" in group_df.keys(): # if the column is in the dialog\n",
    "            if not group_df.iloc[0]['is_english']:\n",
    "                not_fit_for_annotation[group_name] = dialog_dict\n",
    "                \n",
    "        elif remove_no_welcome and not welcome_message: # if the no welcome message (intro) in the dialog\n",
    "            not_fit_for_annotation[group_name] = dialog_dict\n",
    "            \n",
    "        elif turns_counter > 0: # if the column is not in the data\n",
    "           \n",
    "            json_data[group_name] = dialog_dict\n",
    "            \n",
    "        else:\n",
    "            not_fit_for_annotation[group_name] = dialog_dict\n",
    "\n",
    "    return json_data, not_fit_for_annotation\n",
    "\n",
    "def count_dialogs_folder(input_path):\n",
    "    \"\"\"\n",
    "    Count the number of dialogs in each CSV file in the specified folder.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the folder containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    def count(name, data):\n",
    "        return f\"file {name} has {len(data)} dialogs\\n\", len(data)\n",
    "    counter = 0\n",
    "    output = f\"working on folder {input_path}, combined number of dialogs is\"\n",
    "    temp_output = f\"\"\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            data = process_yael_data(pd.read_csv(file_path))[0]\n",
    "            str, num = count(file_name, data)\n",
    "            temp_output += str\n",
    "            counter += num\n",
    "\n",
    "    output = output + f\" {counter}\\n\" + temp_output\n",
    "    print(output)\n",
    "\n",
    "def convert_yael_files_in_folder(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Convert Yael files in a folder.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the folder containing the Yael files.\n",
    "        output_path (str): The path to the folder where the converted files will be saved.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    batch_num = 1\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            data = read_json_file(file_path)\n",
    "            new_data = process_yael_data(data)\n",
    "            output_file_path = os.path.join(output_path, (f'batch_{batch_num}_src.json'))\n",
    "            # Write the modified data to the output file\n",
    "            save_json_file(output_file_path, new_data)\n",
    "            batch_num+=1\n",
    "\n",
    "def combine_csv_files(input_path):\n",
    "    \"\"\"\n",
    "    Combines the data from multiple CSV files in the given input path and converts and combine all of them to the new format.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the combined data from all the CSV files.\n",
    "    \"\"\"\n",
    "    converted_data_all_files = {}\n",
    "    not_fit_all_files = {}\n",
    "    combined_original_data = pd.DataFrame()\n",
    "\n",
    "    for file_name in os.listdir(input_path):\n",
    "   \n",
    "\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            origina_data = pd.read_csv(file_path)\n",
    "            combined_original_data = pd.concat([combined_original_data, origina_data], ignore_index=True)\n",
    "            converted_data_dialog, not_fit_dialog = process_yael_data(origina_data)\n",
    "            check_if_ordered_stayed_the_same(origina_data, converted_data_dialog, not_fit_dialog)\n",
    "            converted_data_all_files.update(converted_data_dialog)\n",
    "            not_fit_all_files.update(not_fit_dialog)\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "    print(f\"check order for all dialogs in all files:\")\n",
    "    check_if_ordered_stayed_the_same(combined_original_data, converted_data_all_files, not_fit_all_files)\n",
    "    return converted_data_all_files,  not_fit_all_files \n",
    "\n",
    "def new_combine_csv_files(input_path, dialogs_to_remove=None):\n",
    "    \"\"\"\n",
    "    Combines the data from multiple CSV files in the given input path and converts and combines all of them to the new format.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): The path to the directory containing the CSV files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the combined data from all the CSV files.\n",
    "    \"\"\"\n",
    "    combined_original_data = pd.DataFrame()\n",
    "\n",
    "    # Combine all CSV files into one DataFrame\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(input_path, file_name)\n",
    "            original_data = pd.read_csv(file_path)\n",
    "            combined_original_data = pd.concat([combined_original_data, original_data], ignore_index=True)\n",
    "\n",
    "    if dialogs_to_remove:\n",
    "        combined_original_data = combined_original_data[~combined_original_data['chat_id'].isin(dialogs_to_remove)]\n",
    "\n",
    "    # Process the combined data\n",
    "    converted_data_all_files, not_fit_all_files = process_yael_data(combined_original_data)\n",
    "\n",
    "    # Check if the order stayed the same for the combined data\n",
    "    check_if_ordered_stayed_the_same(combined_original_data, converted_data_all_files, not_fit_all_files)\n",
    "\n",
    "\n",
    "    return converted_data_all_files, not_fit_all_files, combined_original_data\n",
    "\n",
    "def create_test_file(new_data):\n",
    "    \"\"\"\n",
    "    Create a test file from the given data.\n",
    "\n",
    "    Args:\n",
    "        new_data (dict): A dictionary containing the data to be used for creating the test file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the test file (dict) and the list of dialog keys used in the test file.\n",
    "    \"\"\"\n",
    "    items = list(new_data.items())\n",
    "    random.shuffle(items)\n",
    "    shuffled_data = dict(items)\n",
    "\n",
    "    test_file = {}\n",
    "    dialog_test_key = []\n",
    "    turn_counter = 0\n",
    "\n",
    "    for key, value in shuffled_data.items():\n",
    "        number_of_turns = value['number_of_turns']\n",
    "        turn_counter += number_of_turns\n",
    "        dialog_test_key.append(key)\n",
    "        test_file[key] = value\n",
    "        if turn_counter > 60:\n",
    "            break\n",
    "\n",
    "    for dialog_key, dialog_value in test_file.items():\n",
    "        for turn_key, turn_value in dialog_value['dialog'].items():\n",
    "            if turn_key != '0':\n",
    "                turn_value['needs_clarification'] = None\n",
    "\n",
    "    return test_file, dialog_test_key\n",
    "\n",
    "def expand_test_file(turns_to_expand):\n",
    "\n",
    "    def count_files_with_keyword(folder_path=\"datasets_3_6\\\\results\", keyword=\"test\"):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(folder_path):\n",
    "            for file in files:\n",
    "                if keyword in file:\n",
    "                    count += 1\n",
    "        return count\n",
    "\n",
    "    test_file = read_json_file('datasets_3_6\\\\results\\\\agent_conv_test.json')\n",
    "    json_data, not_enough, combind = new_combine_csv_files(\"datasets_3_6/date_sorted_filtered_double\")\n",
    "    new_test_file, test_dialog_to_add = expand_test_file_f(test_file, json_data, turns_to_expand)\n",
    "    save_json_file(f\"datasets_3_6\\\\results\\\\agent_conv_test_{count_files_with_keyword()}.json\", new_test_file)\n",
    "\n",
    "def expand_test_file_f(test_file, json_data, turns_to_expand=60):\n",
    "    \n",
    "    test_file_keys = list(test_file.keys())\n",
    "  \n",
    "    items = list(json_data.items())\n",
    "    random.shuffle(items)\n",
    "    shuffled_data = dict(items)\n",
    "    \n",
    "    #remove existing keys\n",
    "    for key in test_file_keys:\n",
    "        if key in shuffled_data.keys():\n",
    "            del shuffled_data[key]\n",
    "\n",
    "    new_test_file = {}\n",
    "    turn_counter = 0\n",
    "    number = 5\n",
    "    for key, value in shuffled_data.items():\n",
    "        number_of_turns = value['number_of_turns']\n",
    "        if number_of_turns < 5:\n",
    "            continue\n",
    "        else:\n",
    "            number = random.randint(3, 15)\n",
    "        turn_counter += number_of_turns\n",
    "        new_test_file[key] = value\n",
    "        if turn_counter > turns_to_expand:\n",
    "            break\n",
    "\n",
    "    for dialog_key, dialog_value in new_test_file.items():\n",
    "        for turn_key, turn_value in dialog_value['dialog'].items():\n",
    "            if turn_key != '0':\n",
    "                turn_value['needs_clarification'] = None\n",
    "                \n",
    "    test_file.update(new_test_file)\n",
    "\n",
    "    return test_file, new_test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first group name: chat_9812e6bb-a78c-475b-ad5a-dacf1d7e4f7e\n",
      "The order stayed the same\n"
     ]
    }
   ],
   "source": [
    "test_data = read_json_file('datasets_3_6\\\\results\\\\agent_conv_test_1.json')\n",
    "annotated_dialogs = list(test_data.keys())\n",
    "json_file_proccesed, not_enough, combined_csv = new_combine_csv_files(\"datasets_3_6/date_sorted_filtered_double\", annotated_dialogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The order stayed the same\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_if_ordered_stayed_the_same(combined_csv, json_file_proccesed, not_enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_file('datasets_3_6\\\\results\\\\agent_conv_1.json', json_file_proccesed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
